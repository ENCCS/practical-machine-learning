

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Practical Machine Learning documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
      <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx_lesson.css" />
      <link rel="stylesheet" type="text/css" href="_static/term_role_formatting.css" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx_rtd_theme_ext_color_contrast.css" />
      <link rel="stylesheet" type="text/css" href="_static/tabs.css" />
      <link rel="stylesheet" type="text/css" href="_static/overrides.css" />

  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js"></script>
      <script src="_static/doctools.js"></script>
      <script src="_static/sphinx_highlight.js"></script>
      <script src="_static/clipboard.min.js"></script>
      <script src="_static/copybutton.js"></script>
      <script src="_static/minipres.js"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="_static/togglebutton.js"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="_static/tabs.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            Practical Machine Learning
              <img src="_static/ENCCS.jpg" class="logo" alt="Logo"/>
          </a>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Software setup</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-00-software-setup">Setting Up Programming Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Lesson episodes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-01-intro-to-ML">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-02-fundamentals-of-ML">Fundamentals of Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-03-scientific-data-for-ML">Scientific Data for Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-04-data-preparation-for-ML">Data Preparation for Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-05-supervised-ML-classification">Supervised Learning (I): Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-06-supervised-ML-regression">Supervised Learning (II): Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-07-unsupervised-ML-clustering">Unsupervised Learning (I): Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-08-unsupervised-ML-dimensionality-reduction">Unsupervised Learning (II): Dimensionality Reduction</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-quick-reference">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-guide">Instructor’s guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://enccs.se/lessons/">All lessons</a></li>
<li class="toctree-l1"><a class="reference external" href="https://enccs.se/">ENCCS</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Practical Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Practical Machine Learning  documentation</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/practical-machine-learning/blob/main/content/index" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="practical-machine-learning">
<h1>Practical Machine Learning<a class="headerlink" href="#practical-machine-learning" title="Link to this heading"></a></h1>
<p>Machine learning (ML) is a rapidly growing field within artificial intelligence (AI) that focuses on building systems capable of learning from data. Instead of being explicitly programmed with detailed rules, the ML models identify patterns and make predictions or decisions based on historical data. This approach has revolutionized many industries, including healthcare, finance, marketing, and technology, enabling applications like personalized recommendations, fraud detection, and speech recognition. As the volume of data continues to grow, understanding ML concepts and techniques becomes increasingly important for anyone interested in working with data or building intelligent systems.</p>
<p>This course provides a comprehensive introduction to the fundamental principles and techniques of ML. We will cover essential topics such as supervised learning, unsupervised learning, and model evaluation, as well as the basics of data preprocessing and feature selection. The participants will learn how to design and implement basic ML models using tools and frameworks commonly used in the industry. Through a combination of theory, practical exercises, and real-world examples, the participants will gain a solid foundation in ML, preparing them for further study or practical application in various domains.</p>
<p>By the end of the course, the participants will understand how the ML models are built, how to select the appropriate algorithms for specific problems, and how to assess model performance. In addition, the participants will be equipped with the skills to work on ML projects, from data collection and preparation to model training and evaluation. This knowledge will serve as a valuable stepping stone for those wishing to explore more advanced topics or specialize in areas such as deep learning (DL), natural language processing, or computer vision.</p>
<div class="admonition-prerequisites prerequisites admonition" id="prerequisites-0">
<p class="admonition-title">Prerequisites</p>
<ul class="simple">
<li><p>Familiarity with Python basics (loops, functions, lists, dictionaries) and libraries like NumPy, Pandas, and Matplotlib/Seaborn.</p></li>
<li><p>Understanding vectors, matrices, matrix operations (multiplication, transpose, inverse), and eigenvalues.</p></li>
<li><p>(Optional but useful): Derivatives, gradients, and optimization (especially for understanding gradient descent).</p></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<span id="document-00-software-setup"></span><section id="setting-up-programming-environment">
<h2>Setting Up Programming Environment<a class="headerlink" href="#setting-up-programming-environment" title="Link to this heading"></a></h2>
<section id="using-personal-computer">
<h3>Using Personal Computer<a class="headerlink" href="#using-personal-computer" title="Link to this heading"></a></h3>
<p>This section provides instructions for installing the required packages and their dependencies on a local computer or server.</p>
<section id="install-miniforge">
<h4>Install miniforge<a class="headerlink" href="#install-miniforge" title="Link to this heading"></a></h4>
<p>If you already have a preferred way to manage Python versions and libraries, you can stick to that. Otherwise, we recommend you to install Python3 and all required libraries using <a class="reference external" href="https://conda-forge.org/download/">Miniforge</a>, a free minimal installer for the package, dependency, and environment manager <a class="reference external" href="https://docs.conda.io/en/latest/index.html">Conda</a>.</p>
<ul class="simple">
<li><p>Please follow the <a class="reference external" href="https://conda-forge.org/download/">installation instructions</a> to install Miniforge.</p></li>
<li><p>After installation, verify that Conda is installed correctly by running:</p></li>
</ul>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>conda<span class="w"> </span>--version

<span class="gp"># </span>Example<span class="w"> </span>output:<span class="w"> </span>conda<span class="w"> </span><span class="m">24</span>.11.2
</pre></div>
</div>
</section>
<section id="configure-programming-environment">
<h4>Configure programming environment<a class="headerlink" href="#configure-programming-environment" title="Link to this heading"></a></h4>
<p>With Conda installed, open the <strong>Anaconda Prompt terminal</strong>, and run the command below to install required packages and depenencies (except PyTorch):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>conda<span class="w"> </span>env<span class="w"> </span>create<span class="w"> </span>-y<span class="w"> </span>--file<span class="o">=</span>https://raw.githubusercontent.com/ENCCS/practical-machine-learning/main/content/env/environment.yml
</pre></div>
</div>
<p>This creates a new environment called <code class="docutils literal notranslate"><span class="pre">practical_machine_learning</span></code>.
We activate it and then install PyTorch library:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>conda<span class="w"> </span>activate<span class="w"> </span>practical_machine_learning

<span class="gp">$ </span>conda<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>pytorch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>torchtext<span class="w"> </span>cpuonly<span class="w"> </span>-c<span class="w"> </span>pytorch
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Remember to activate your programming environment each time before running code examples. This ensures that the correct Python version and all required dependencies are available. If you forget to activate it, you may encounter errors or missing packages.</p>
</div>
</section>
<section id="validate-programming-environment">
<h4>Validate programming environment<a class="headerlink" href="#validate-programming-environment" title="Link to this heading"></a></h4>
<p>Once the programming environment is fully set up, open a new <strong>Anaconda Prompt terminal</strong> (just as you should do each time before running code examples), activate the programming environment, and launch JupyterLab by running the command below:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>conda<span class="w"> </span>activate<span class="w"> </span>practical_machine_learning

<span class="gp">$ </span>jupyter<span class="w"> </span>lab
</pre></div>
</div>
<p>This will start a Jupyter server and automatically open the JupyterLab interface in your web browser.</p>
<p>To verify that all required packages are properly installed, follow these steps:</p>
<ul class="simple">
<li><p>Open JupyterLab (see instructions above).</p></li>
<li><p>Create a new Jupyter Notebook by selecting <strong>File</strong> → <strong>New</strong> → <strong>Notebook</strong>.</p></li>
<li><p>Copy the code examples listed below into a cell of the notebook.</p></li>
<li><p>Run the cell (press <strong>Shift + Enter</strong> or click the <strong>Run</strong> button).</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="p">;</span>      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Numpy version: &#39;</span><span class="p">,</span>            <span class="n">numpy</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="p">;</span>     <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Pandas version: &#39;</span><span class="p">,</span>           <span class="n">pandas</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy</span><span class="p">;</span>      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Scipy version: &#39;</span><span class="p">,</span>            <span class="n">scipy</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib</span><span class="p">;</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Matplotlib version: &#39;</span><span class="p">,</span>       <span class="n">matplotlib</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="p">;</span>    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Seaborn version: &#39;</span><span class="p">,</span>          <span class="n">seaborn</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn</span><span class="p">;</span>    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Scikit-learn version: &#39;</span><span class="p">,</span>     <span class="n">sklearn</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">keras</span><span class="p">;</span>      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Keras version: &#39;</span><span class="p">,</span>            <span class="n">keras</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="p">;</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Tensorflow version: &#39;</span><span class="p">,</span>       <span class="n">tensorflow</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span><span class="p">;</span>      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Pytorch version: &#39;</span><span class="p">,</span>          <span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">umap</span><span class="p">;</span>       <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Umap-learn version: &#39;</span><span class="p">,</span>       <span class="n">umap</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">notebook</span><span class="p">;</span>   <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Jupyter Notebook version: &#39;</span><span class="p">,</span> <span class="n">notebook</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
<p>You should see output similar to the figure below. The exact package versions may vary depending on when you installed them.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/0-verification-programming-environment.png"><img alt="_images/0-verification-programming-environment.png" src="_images/0-verification-programming-environment.png" style="width: 80%;" />
</a>
</figure>
<p>If the code runs without errors, it means that all packages are correctly installed and your programming environment is ready to use.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For Windows OS users, you might encounter an error
(<code class="docutils literal notranslate"><span class="pre">ImportError:</span> <span class="pre">DLL</span> <span class="pre">load</span> <span class="pre">failed</span> <span class="pre">while</span> <span class="pre">importing</span> <span class="pre">_C:</span> <span class="pre">The</span> <span class="pre">specified</span> <span class="pre">procedure</span> <span class="pre">could</span> <span class="pre">not</span> <span class="pre">be</span> <span class="pre">found</span></code>) as described below.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/pytorch_error.png"><img alt="_images/pytorch_error.png" src="_images/pytorch_error.png" style="width: 80%;" />
</a>
</figure>
<p>It is a very common Windows-specific PyTorch issue, and it means that the underlying C++/CUDA DLLs that torch depends on could not be loaded correctly.</p>
<p>You should reinstall the correct matching build via the command below.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>conda<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>pytorch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>torchtext<span class="w"> </span>cpuonly<span class="w"> </span>-c<span class="w"> </span>pytorch
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are using VS Code, you can select the installed <code class="docutils literal notranslate"><span class="pre">practical_machine_learning</span></code> programming environment as follows:</p>
<ul class="simple">
<li><p>Open your project folder in VS Code.</p></li>
<li><p>In the upper-right corner of the editor window (when working with a Python file or Jupyter Notebook), click on <strong>Select Kernel</strong>.</p></li>
<li><p>From the list of <strong>Python Environments</strong>, locate and choose the <code class="docutils literal notranslate"><span class="pre">practical_machine_learning</span></code> environment (which you have installed earlier).</p></li>
<li><p>Once selected, VS Code will use this environment for running Python code and Jupyter Notebooks, ensuring that all required packages are available.</p></li>
</ul>
</div>
</section>
<section id="optional-setting-up-pytorch-with-gpu-support">
<h4>(Optional) Setting Up PyTorch with GPU Support<a class="headerlink" href="#optional-setting-up-pytorch-with-gpu-support" title="Link to this heading"></a></h4>
<p><strong>For Windows OS users</strong>, if your computer has a GPU card, you can install PyTorch with GPU support.
Below are step-by-step instructions to update the <code class="docutils literal notranslate"><span class="pre">practical_machine_learning</span></code> programming environment.</p>
<p>First check your CUDA version.
Open a terminal (Linux/macOS) or PowerShell (Windows) and run:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>nvcc<span class="w"> </span>--version
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">nvcc</span></code> is not in your PATH, you can instead run <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>nvidia-smi
</pre></div>
</div>
<p>Here is the output from my Windows machine:</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/test-cuda-compiler-driver.png"><img alt="_images/test-cuda-compiler-driver.png" src="_images/test-cuda-compiler-driver.png" style="width: 80%;" />
</a>
</figure>
<p>Second, remove any CPU-only versions of PyTorch that may have been installed (for example, those coming from Conda’s defaults or conda-forge channels), and hten install an older, CUDA-compatible version of PyTorch directly using <code class="docutils literal notranslate"><span class="pre">pip</span></code>.
Here <code class="docutils literal notranslate"><span class="pre">cu121</span></code> indicates the CUDA version (12.1) that the PyTorch build was compiled with.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>conda<span class="w"> </span>activate<span class="w"> </span>practical_machine_learning

<span class="gp">$ </span>conda<span class="w"> </span>remove<span class="w"> </span>pytorch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>torchtext

<span class="gp">$ </span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">torch</span><span class="o">==</span><span class="m">2</span>.4.0<span class="w"> </span><span class="nv">torchvision</span><span class="o">==</span><span class="m">0</span>.19.0<span class="w"> </span><span class="nv">torchaudio</span><span class="o">==</span><span class="m">2</span>.4.0<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/cu121
</pre></div>
</div>
<p>Third, verify your installation in a Jupyter Notebook.
Run the following command and ensure it returns <code class="docutils literal notranslate"><span class="pre">True</span></code> for <code class="docutils literal notranslate"><span class="pre">torch.cuda.is_available()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>               <span class="c1"># 2.4.0+cu121</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">())</span>       <span class="c1"># True</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>   <span class="c1"># NVIDIA GeForce GT 1030</span>
</pre></div>
</div>
</section>
</section>
<section id="using-google-colab">
<h3>Using Google Colab<a class="headerlink" href="#using-google-colab" title="Link to this heading"></a></h3>
<p>You can also run all the code examples in tutorials using <a class="reference external" href="https://colab.research.google.com/">Google Colab</a>, a free cloud-based platform that provides Jupyter Notebook environments with preinstalled ML libraries.</p>
<section id="download-jupyter-notebooks">
<h4>Download Jupyter Notebooks<a class="headerlink" href="#download-jupyter-notebooks" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>You can open each Jupyter Notebook (usually with the <code class="docutils literal notranslate"><span class="pre">.ipynb</span></code> extension) from <a class="reference external" href="https://github.com/ENCCS/practical-machine-learning/tree/main/content/jupyter-notebooks">HERE</a>, and then select <strong>Download raw file</strong> to save it locally.</p></li>
<li><p>Alternatively, you can download the entire repository at <a class="reference external" href="https://github.com/ENCCS/practical-machine-learning/tree/main">HERE</a> by clicking the green <code class="docutils literal notranslate"><span class="pre">&lt;&gt;</span> <span class="pre">Code</span></code> button and choosing <strong>Download ZIP</strong> file. After unzipping the downloaded ZIP file, you will find all Jupyter Notebooks in the directory <strong>practical-machine-learning-main/content/jupyter-notebooks</strong>.</p></li>
</ul>
</section>
<section id="upload-jupyter-notebooks-to-google-drive">
<h4>Upload Jupyter Notebooks to Google Drive<a class="headerlink" href="#upload-jupyter-notebooks-to-google-drive" title="Link to this heading"></a></h4>
<p>Sign in to your <a class="reference external" href="https://workspace.google.com/intl/en-US/products/drive/">Google Drive</a>, then upload the downloaded Jupyter Notebooks to a convenient folder.
You can simply drag and drop the files directly into Google Drive or use the option <strong>New → File upload</strong>.</p>
</section>
<section id="open-jupyter-notebooks-in-google-colab">
<h4>Open Jupyter Notebooks in Google Colab<a class="headerlink" href="#open-jupyter-notebooks-in-google-colab" title="Link to this heading"></a></h4>
<p>Once uploaded, right-click the Jupyter Notebooks file in Google Drive and select <strong>Open with → Google Colaboratory</strong>.
This will launch the notebook in Google Colab, where you can view, edit, and run the code cells interactively.</p>
</section>
<section id="connect-to-a-hosted-runtime">
<h4>Connect to a Hosted Runtime<a class="headerlink" href="#connect-to-a-hosted-runtime" title="Link to this heading"></a></h4>
<p>In Google Colab, go to the top-right corner and click <strong>Connect</strong> to link your notebook to a Google-hosted runtime environment.
If you need GPU or TPU acceleration, select <strong>Runtime → Change runtime type</strong>, then choose the desired hardware accelerator.</p>
</section>
<section id="run-the-code">
<h4>Run the Code<a class="headerlink" href="#run-the-code" title="Link to this heading"></a></h4>
<p>After connecting, follow the instructions inside the Jupyter Notebooks.
You can run each cell individually by pressing <strong>Shift</strong> + <strong>Enter</strong>, or execute the entire notebook using <strong>Runtime → Run all</strong>.</p>
</section>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-01-intro-to-ML"></span><section id="introduction-to-machine-learning">
<h2>Introduction to Machine Learning<a class="headerlink" href="#introduction-to-machine-learning" title="Link to this heading"></a></h2>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Provide a general overview of machine learning.</p></li>
<li><p>Explain the relationship between artificial intelligence, machine learning, and deep learning.</p></li>
<li><p>Explore representative real-world applications of machine learning.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>15 min teaching</p></li>
<li><p>0 min exercising</p></li>
</ul>
</div>
<section id="what-is-machine-learning">
<h3>What is Machine Learning<a class="headerlink" href="#what-is-machine-learning" title="Link to this heading"></a></h3>
<p>Machine learning (ML) is a field of computer science that studies algorithms and techniques for automating solutions to complex problems that are hard to program using conventional programing methods.</p>
<p>In conventional programming, the programmer explicitly codes the logic (rules) to transform inputs (data) into outputs (answers), making it suitable for well-defined, rule-based tasks. In ML, the system learns the logic (rules) from data and answers, making it ideal for complex, pattern-based tasks where explicit rules are hard to define. The choice between them depends on the problem, data availability, and complexity.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/1-classic-programming-vs-ML.jpg"><img alt="_images/1-classic-programming-vs-ML.jpg" src="_images/1-classic-programming-vs-ML.jpg" style="width: 75%;" />
</a>
<figcaption>
<p><span class="caption-text"><em>Classic programming vs. machine learning.</em> <a class="reference external" href="https://twimlai.com/resources/kubernetes-for-mlops/">Source</a></span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="relation-with-artificial-intelligence-and-deep-learning">
<h3>Relation with Artificial Intelligence and Deep Learning<a class="headerlink" href="#relation-with-artificial-intelligence-and-deep-learning" title="Link to this heading"></a></h3>
<p>Artificial intelligence (AI) is the broadest field, encompassing any technique that enables computers to mimic human intelligence, such as reasoning, problem-solving, perception, and decision-making. AI includes a wide range of approaches, from rule-based systems (like expert systems) to modern data-driven methods. It aims to create systems that can perform tasks that typically require human intelligence, such as playing chess, recognizing images, or understanding language.</p>
<p>ML is a subset of AI that focuses on algorithms and models that learn patterns from data to make predictions or decisions <strong>without being explicitly programmed</strong>. ML is one of the primary ways to achieve AI. It enables systems to improve performance over time by learning from experience (data) rather than relying solely on hardcoded rules. ML includes various techniques like supervised learning (<em>e.g.</em>, regression, classification), unsupervised learning (<em>e.g.</em>, clustering, dimensionality reduction), and reinforcement learning.</p>
<p>Deep learning (DL) is a specialized subset of ML, and it leverages artificial neural networks inspired by the human brain to tackle tasks like image recognition, speech processing, and natural language understanding. DL excels in handling unstructured data (<em>e.g.</em>, images, audio, text) and requires significant computational power and large datasets for training.</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="_images/1-relationship-AI-ML-DL.png"><img alt="_images/1-relationship-AI-ML-DL.png" src="_images/1-relationship-AI-ML-DL.png" style="width: 50%;" />
</a>
<figcaption>
<p><span class="caption-text">The relationship between artificial intelligence, machine learning, and deep learning. <a class="reference external" href="https://carpentries-lab.github.io/deep-learning-intro/">Source</a></span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="why-machine-learning">
<h3>Why Machine Learning?<a class="headerlink" href="#why-machine-learning" title="Link to this heading"></a></h3>
<p>ML is transforming how we solve complex problems in the real world by enabling systems to learn directly from data, rather than relying on explicitly programmed rules. In many real-world scenarios, such as medical diagnosis, stock market prediction, or natural language processing, the relationships between inputs and outputs are too complex or dynamic to define manually. ML models can uncover hidden patterns and make accurate predictions or decisions, making them essential tools in fields like healthcare, finance, transportation, and cybersecurity.</p>
<p>Another crucial advantage of ML is its ability to adapt and improve over time as more data becomes available. Unlike traditional rule-based systems that require constant manual updates, ML models can retrain and adjust themselves to new data, trends, or anomalies, ensuring that the system stays relevant and effective. For example, in fraud detection, ML algorithms can evolve as fraud tactics change, providing a stronger defense compared to static rules that may become outdated. This adaptability makes ML particularly powerful in dynamic, real-time environments where traditional programming methods fall short.</p>
<p>In addition, ML empowers the automation of complex tasks that were previously dependent on human expertise and intuition. From voice recognition in virtual assistants to autonomous driving, ML algorithms can process vast amounts of unstructured data such as text, images, and audio, which are traditionally challenging for computers to handle. By enabling machines to “learn” from experience and improve their performance over time, ML not only enhances productivity but also opens new frontiers for innovation across industries, creating smarter systems that can make meaningful contributions to society.</p>
</section>
<section id="machine-learning-applications">
<h3>Machine Learning Applications<a class="headerlink" href="#machine-learning-applications" title="Link to this heading"></a></h3>
<section id="problems-can-be-solve-with-ml">
<h4>Problems can be solve with ML<a class="headerlink" href="#problems-can-be-solve-with-ml" title="Link to this heading"></a></h4>
<p>ML is used across a wide range of industries and real-world problems in healthcare, finance, natural language processing, computer vision, transportation, manufacturing industry, retail, and cybersecurity.</p>
<p>Below are key categories of problems that can be applied using ML.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Application area</p></th>
<th class="head text-center"><p>Example use Cases</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Healthcare</p></td>
<td class="text-center"><p>Disease prediction &amp; diagnosis, <br>medical image analysis, drug discovery</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Finance</p></td>
<td class="text-center"><p>Fraud detection, credit scoring, algorithmic trading</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Retail &amp; e-commerce</p></td>
<td class="text-center"><p>Product recommendations, customer segmentation, <br>demand forecasting</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Transportation &amp; autonomous systems</p></td>
<td class="text-center"><p>Self-driving cars, traffic prediction, route optimization</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Natural language processing (NLP)</p></td>
<td class="text-center"><p>Chatbots and virtual assistants, sentiment analysis, <br>language translation</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Manufacturing &amp; industry</p></td>
<td class="text-center"><p>Predictive maintenance, quality control, <br>supply chain optimization</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Computer Vision</p></td>
<td class="text-center"><p>Facial recognition, object detection, image classification</p></td>
</tr>
</tbody>
</table>
</section>
<section id="problems-can-t-be-solve-with-ml">
<h4>Problems can’t be solve with ML<a class="headerlink" href="#problems-can-t-be-solve-with-ml" title="Link to this heading"></a></h4>
<p>ML is powerful, but it’s not magic. It’s a tool for finding patterns in data but has no idea what the patterns mean. Therefore it is not a substitute for human reasoning, creativity, or ethical judgment.</p>
<p>Below are key categories of problems that cannot be solved with ML due to inherent limitations, regardless of data or computational advancements.</p>
<ul class="simple">
<li><p>Problems with insufficient or poor-quality data: ML relies heavily on data. If data is scarce, noisy, biased, or unrepresentative, models fail to generalize. For example, predicting rare events with limited historical data (<em>e.g.</em>, catastrophic asteroid impacts, spread of pandemic) is unreliable.</p></li>
<li><p>Problems requiring reasoning, understanding, or deep logic. ML models approximate patterns but don’t understand them. They lack reasoning and common sense unless explicitly designed (<em>e.g.</em>, symbolic AI).</p></li>
<li><p>Problems that involve subjective judgments or value-based decisions. ML models don’t “know” what’s right or wrong – they reflect patterns in the data, including biases.</p></li>
<li><p>Problems outside of distribution generalization. A model trained on photos of cats can’t accurately classify dogs if it never saw dogs. ML models interpolate between known data. They struggle with novel scenarios far outside the training set.</p></li>
</ul>
</section>
<section id="problems-can-be-but-shouldn-t-be-solved-with-ml">
<h4>Problems can be, but shouldn’t be solved with ML<a class="headerlink" href="#problems-can-be-but-shouldn-t-be-solved-with-ml" title="Link to this heading"></a></h4>
<p>There are many problems where ML (DL) could technically be applied, but shouldn’t be – either because of the simplicity of the problem or due to ethical, practical, or societal concerns.</p>
<ul class="simple">
<li><p>Tasks for modelling well defined systems, where the equations governing them are known and understood.</p></li>
<li><p>Problems at high-stakes domains with unacceptable error rates: ML can predict outcomes in fields like medical diagnosis or aviation safety, but even small errors can lead to catastrophic consequences. Over-reliance on ML without human oversight risks lives when models fail in edge cases.</p></li>
<li><p>Privacy-sensitive applications: ML can analyze personal data (<em>e.g.</em>, health records, browsing habits) to predict behaviors, but using it for invasive profiling, surveillance, or targeted manipulation (<em>e.g.</em>, hyper-personalized propaganda) raises serious privacy and autonomy concerns.</p></li>
<li><p>Reinforcing harmful social norms: ML can optimize systems like targeted advertising or content recommendation, but doing so can amplify harmful behaviors (<em>e.g.</em>, echo chambers, misinformation, or addiction to social media) if not carefully regulated.</p></li>
</ul>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Machine learning focuses on building systems that can learn patterns from data <strong>without being explicitly programmed</strong>.</p></li>
<li><p>The relationship between artificial intelligence, machine learning, and deep learning.</p></li>
<li><p>The importance of machine learning in daily work and lives.</p></li>
<li><p>Examples of real-world problems that can, cannot, or can but shouldn’t, be solved using machine learning.</p></li>
</ul>
</div>
</section>
</section>
</section>
<span id="document-02-fundamentals-of-ML"></span><section id="fundamentals-of-machine-learning">
<h2>Fundamentals of Machine Learning<a class="headerlink" href="#fundamentals-of-machine-learning" title="Link to this heading"></a></h2>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Describe the representative types of machine learning (supervised, unsupervised, semi-supervised, reinforcement learning).</p></li>
<li><p>Explain the general workflow of a machine learning project.</p></li>
<li><p>Introduce representative machine learning libraries and discuss their pros and cons.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>25 min teaching</p></li>
<li><p>0 min exercising</p></li>
</ul>
</div>
<section id="types-of-machine-learning">
<h3>Types of Machine Learning<a class="headerlink" href="#types-of-machine-learning" title="Link to this heading"></a></h3>
<p>Machine learning (ML) can be broadly categorized into three main types depending on how the models learn from input data and the nature of the input data they process.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/2-ML-three-types.png"><img alt="_images/2-ML-three-types.png" src="_images/2-ML-three-types.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-text">Three main types of machine learning. Main approaches include classification and regression under the supervised learning and clustering under the unsupervised learning. Reinforcement learning enhance the model performance by interacting with environment. Coloured dots and triangles represent the training data. Yellow stars represent the new data which can be predicted by the trained model. This figure was taken from the paper <a class="reference external" href="https://www.frontiersin.org/journals/pharmacology/articles/10.3389/fphar.2021.720694/full">Machine Learning Techniques for Personalised Medicine Approaches in Immune-Mediated Chronic Inflammatory Diseases: Applications and Challenges </a>.</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="supervised-learning">
<h4>Supervised learning<a class="headerlink" href="#supervised-learning" title="Link to this heading"></a></h4>
<p>In supervised learning, the model is trained on a labeled dataset, where each input is paired with a corresponding output (label). The goal is to learn a mapping from inputs to outputs to make predictions on new, unseen data.</p>
<p>Supervised learning has two subtypes: <strong>Classification</strong> (predicting discrete categories) and <strong>Regression</strong> (predicting continuous values).</p>
<p>Here are representative examples of these two subtypes in real-word problems:</p>
<ul class="simple">
<li><p><strong>Classification</strong>: email spam detection (spam/ham), image recognition (cat/dog), medical diagnosis (disease/no disease).</p></li>
<li><p><strong>Regression</strong>: house price prediction, weather forecasting.</p></li>
</ul>
</section>
<section id="unsupervised-learning">
<h4>Unsupervised learning<a class="headerlink" href="#unsupervised-learning" title="Link to this heading"></a></h4>
<p>In unsupervised learning, the model works with unlabeled data, identifying patterns, structures, or relationships within the data without explicit guidance on what to predict.</p>
<p>Unsupervised learning also has two subtypes: <strong>Clustering</strong> (grouping similar data points together) and <strong>Dimensionality Reduction</strong> (simplifying data by reducing features while preserving important information)</p>
<p>Representative examples of these two subtypes in real-word problems:</p>
<ul class="simple">
<li><p><strong>Clustering</strong>: customer segmentation in marketing (grouping users by behavior), image segmentation (grouping similar pixels).</p></li>
<li><p><strong>Dimensionality Reduction</strong>: compressing high-dimensional data (<em>e.g.</em>, reducing image features for faster processing), anomaly detection.</p></li>
</ul>
</section>
<section id="reinforcement-learning">
<h4>Reinforcement learning<a class="headerlink" href="#reinforcement-learning" title="Link to this heading"></a></h4>
<p>The model (agent) learns by interacting with an environment. It takes actions, receives feedback (rewards or penalties), and learns a strategy (policy) to maximize long-term rewards.</p>
<p>Representative examples of reinforcement learning in real-word problems: game-playing AI (<em>e.g.</em>, AlphaGo), robot navigation, autonomous driving.</p>
</section>
<section id="other-subtypes">
<h4>Other subtypes<a class="headerlink" href="#other-subtypes" title="Link to this heading"></a></h4>
<p>In addition to supervised and unsupervised learning, there are other important paradigms in ML.</p>
<ul class="simple">
<li><p><strong>Semi-supervised learning</strong> bridges the gap between supervised and unsupervised learning by using a small amount of labeled data together with a large amount of unlabeled data, helping models learn more effectively when labeling is expensive or time-consuming (<em>e.g.</em>, medical image analysis).</p></li>
<li><p><strong>Self-supervised learning</strong> is a form of unsupervised learning where the model generates its own labels from the data – typically for pretraining models on tasks like image or language understanding, enabling them to learn robust representations without explicit labels (<em>e.g.</em>, predicting the next word in a sentence, and filling in missing image patches)</p></li>
<li><p><strong>Transfer learning</strong> involves applying knowledge from a pretrained model, trained on a large, general dataset, to a new, related task, significantly reducing training time and data requirements (<em>e.g.</em>, fine-tuning a speech recognition model for a new dialect).</p></li>
</ul>
<p>These techniques expand the capabilities and versatility of ML across data-limited or computationally constrained environments.</p>
</section>
</section>
<section id="machine-learning-workflow">
<h3>Machine Learning Workflow<a class="headerlink" href="#machine-learning-workflow" title="Link to this heading"></a></h3>
<section id="what-is-a-workflow-for-ml">
<h4>What is a workflow for ML?<a class="headerlink" href="#what-is-a-workflow-for-ml" title="Link to this heading"></a></h4>
<p>A ML workflow is a structured approach for developing, training, evaluating, and deploying ML models. It typically involves several key phases, including data collection, preprocessing, model training and evaluation, and finally, deployment to production.</p>
<p>Here is a graphical representation of ML workflow, and a concise overview of the key steps are described below.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/2-ML-workflow.png"><img alt="_images/2-ML-workflow.png" src="_images/2-ML-workflow.png" style="width: 100%;" />
</a>
</figure>
</section>
<section id="problem-definition-and-project-setup">
<h4>Problem definition and project setup<a class="headerlink" href="#problem-definition-and-project-setup" title="Link to this heading"></a></h4>
<p><strong>Problem Definition</strong> is the first and most critical phase of any ML project. It sets the direction, scope, and goals for the entire project.</p>
<ul class="simple">
<li><p>We should understand the problem domain: what is the real-world problem we are trying to solve? are we predicting, classifying, or grouping data? (<em>e.g.</em>, predict house prices, detect spam emails, cluster customers).</p></li>
<li><p>We should determine if ML is the appropriate solution for the problem.</p></li>
<li><p>We then should identify the expected outputs: what will the ML model produce? (<em>e.g.</em>, a number, a label, or a probability).</p></li>
<li><p>We define the type of ML task (<em>e.g.</em>, classification and regression tasks for supervised learning, clustering, dimensionality reduction for unsupervised learning, and decision-making tasks for reinforcement learning).</p></li>
</ul>
<p><strong>Project Setup</strong> is to set up the programming/development environment for the project.</p>
<ul class="simple">
<li><p>Hardware requirements (CPU, SSD, GPU, cloud platforms, <em>etc.</em>).</p></li>
<li><p>Software requirements (programming languages and libraries, ML (DL) frameworks, and development tools, IDEs, Git/Docker, *etc.).</p></li>
<li><p>Project structure: organize the project for clarity and scalability.</p></li>
</ul>
<p>A typical ML project structure looks like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  ML_Project/
  ├── data/                 # raw and processed data
  │   ├── raw/              # original, unprocessed data
  │   ├── processed/        # cleaned, preprocessed data
  ├── notebooks/            # jupyter notebooks for EDA &amp; modeling
  ├── src/                  # source code
  │   ├── utils/            # utility functions (*e.g.*, metrics, logging)
  │   ├── preprocessing.py  # data cleaning script  
  │   └── train.py          # model training script
  ├── models/               # trained model files (*e.g.*, .pkl, .h5)
  ├── tests/                # unit and integration tests
  ├── README.md             # project overview and setup instructions
  ├── requirements.txt      # project dependencies
  ├── config.yaml           # configuration file for hyperparameters and paths
</pre></div>
</div>
</section>
<section id="data-collection-and-preprocessing">
<h4>Data collection and preprocessing<a class="headerlink" href="#data-collection-and-preprocessing" title="Link to this heading"></a></h4>
<p>In ML, data collection and preprocessing are crucial steps that significantly affect the performance of a model. High-quality, well-processed data leads to better predictions, while poor data can result in unreliable models.</p>
<ul class="simple">
<li><p><strong>Data collection</strong>: Gather the necessary data from various sources (<em>e.g.</em>, databases, APIs (twitter, linkedin, <em>etc.</em>), or manual collection), and ensure that data is representative and sufficient for the problem.</p></li>
<li><p><strong>Data preprocessing</strong>: Clean and prepare data by handling missing values (drop, impute, or predict), removing duplicates or irrelevant data, fixing inconsistencies (<em>e.g.</em>, “USA” vs. “United States”), normalizing/scaling features, encoding categorical variables, and addressing outliers, and other data quality issues.</p></li>
<li><p><strong>Exploratory data analysis</strong> (EDA): Analyze data to uncover distributions, correlations, patterns, anomalies, and insights using visualizations and statistical methods. This helps in feature selection and understanding data distribution.</p></li>
<li><p><strong>Feature engineering</strong>: Create or select relevant features to improve model performance. This may involve dimensionality reduction (<em>e.g.</em>, PCA (principal component analysis)) or creating new features based on domain knowledge.</p></li>
<li><p><strong>Data splitting</strong>: Divide the dataset into training, validation, and test sets (<em>e.g.</em>, 70-15-15 split) to evaluate model performance and prevent overfitting.</p></li>
</ul>
</section>
<section id="model-selection-and-training">
<h4>Model selection and training<a class="headerlink" href="#model-selection-and-training" title="Link to this heading"></a></h4>
<p>Model Selection and Training refer to the process of choosing an appropriate model architecture and training it to learn patterns from data to solve a specific task. It involves selecting the appropriate algorithms (<em>e.g.</em>, linear/logistic regression, decision trees, neural networks, Gradient Boosting) based on the problem type, configuring its hyperparameters, and optimizing its parameters using training data to minimize error or maximize performance metrics.</p>
</section>
<section id="model-evaluation-and-assessment">
<h4>Model evaluation and assessment<a class="headerlink" href="#model-evaluation-and-assessment" title="Link to this heading"></a></h4>
<p>Model evaluation and assessment in ML refers to the process of measuring and analyzing a model’s performance to determine its effectiveness in solving a specific task. It involves using metrics and techniques to quantify how well the model generalizes to unseen data, identifies patterns, and meets desired objectives, typically using a test dataset separate from the training data.</p>
<p>Below are common evaluation metrics by task types:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Task types</p></th>
<th class="head text-center"><p>Evaluation metrics</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Classification</p></td>
<td class="text-center"><p>Accuracy, precision, recall, F1-score, ROC-AUC, <em>etc.</em></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Regression</p></td>
<td class="text-center"><p>Mean Squared Error (MSE), Mean Absolute Error (MAE), <br>Root Mean Squared Error (RMSE), R-squared, <em>etc.</em></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Clustering</p></td>
<td class="text-center"><p>Silhouette score, Davies-Bouldin index, Calinski-Harabasz index</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Ranking</p></td>
<td class="text-center"><p>Mean Reciprocal Rank (MRR), <br>Normalized Discounted Cumulative Gain (NDCG)</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>NLP or generative tasks</p></td>
<td class="text-center"><p>BLEU, ROUGE, perplexity (often overlaps with deep learning)</p></td>
</tr>
</tbody>
</table>
<p>Here are representative techniques and processes for the assessment:</p>
<ul class="simple">
<li><p><strong>Train-validation-test split</strong>: Divide data into training (model learning), validation (hyperparameter tuning), and test (final evaluation) sets to prevent overfitting.</p></li>
<li><p><strong>Cross-validation</strong>: Use k-fold cross-validation to assess model stability across multiple data subsets.</p></li>
<li><p><strong>Confusion matrix</strong>: For classification, visualize true positives, false negatives, <em>etc.</em></p></li>
<li><p><strong>Learning curves</strong>: Plot training <em>vs.</em> validation performance to diagnose underfitting or overfitting.</p></li>
<li><p><strong>Comparison with baselines</strong>: Comparing model performance against simple baselines (<em>e.g.</em>, random guessing, linear models) to ensure meaningful improvement.</p></li>
<li><p><strong>Robustness testing</strong>: Evaluate performance under noisy, adversarial, or out-of-distribution data.</p></li>
<li><p><strong>Fairness and bias analysis</strong>: Assess model predictions for fairness across groups (<em>e.g.</em>, demographics).</p></li>
</ul>
</section>
<section id="hyperparameter-tuning">
<h4>Hyperparameter tuning<a class="headerlink" href="#hyperparameter-tuning" title="Link to this heading"></a></h4>
<p>Hyperparameter tuning is the process of optimizing the settings (hyperparameters) of a model that are not learned during training but significantly affect its performance. These include parameters like learning rate, number of hidden layers, or batch size, which control the model’s behavior and training process.</p>
<p>The goal of this process is to find the best combination of hyperparameters that maximizes performance metrics (<em>e.g.</em>, accuracy, precision) on a validation set.</p>
</section>
<section id="model-deployment-monitoring-and-improvement">
<h4>Model deployment, monitoring, and improvement<a class="headerlink" href="#model-deployment-monitoring-and-improvement" title="Link to this heading"></a></h4>
<p>Model deployment, monitoring, and improvement refer to the processes involved in taking a trained ML model from development to production, ensuring it performs effectively in real-world applications, and continuously enhancing its performance.</p>
<ul class="simple">
<li><p><strong>Model deployment</strong> indicates an integration of a trained model into a production environment (APIs or cloud platforms) where it can make predictions or decisions on new, unseen data.</p></li>
<li><p>Once deployed, the model’s performance must be continuously tracked to ensure it remains accurate and reliable over time, which is termed as <strong>model monitoring</strong>.</p></li>
<li><p>As the models degrade over time, so continuous improvement is necessary. <strong>Model improvement</strong> involves updating or retraining the model to maintain or enhance its performance based on monitoring insights or new data.</p></li>
</ul>
</section>
</section>
<section id="machine-learning-libraries">
<h3>Machine Learning Libraries<a class="headerlink" href="#machine-learning-libraries" title="Link to this heading"></a></h3>
<section id="scikit-learn">
<h4>Scikit-learn<a class="headerlink" href="#scikit-learn" title="Link to this heading"></a></h4>
<p><strong>Scikit-learn</strong> is a widely-used, open-source Python library designed for <strong>classical machine learning</strong>, offering a variety of algorithms and tools for for tasks, such classification, regression, clustering, and dimensionality reduction. It supports supervised learning (<em>e.g.</em>, SVM (support vector machine), decision trees, random forests), unsupervised learning (<em>e.g.</em>, k-means, PCA (principal component analysis)), and semi-supervised learning, with robust tools for data preprocessing, model evaluation, and hyperparameter tuning via <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code>. Built on NumPy, SciPy, and Matplotlib, it is designed for ease of use, making it ideal for beginners and rapid prototyping. Scikit-Learn excels in handling small to medium-sized datasets and includes utilities for data preprocessing, model evaluation, hyperparameter tuning, and pipeline construction. However, it lacks support for DL and GPU acceleration, limiting its scalability for large datasets or complex neural network tasks.</p>
</section>
<section id="keras">
<h4>Keras<a class="headerlink" href="#keras" title="Link to this heading"></a></h4>
<p><strong>Keras</strong> is a high-level neural networks API that simplifies the process of building and training DL models. Originally an independent library, Keras is now tightly integrated with TensorFlow as its official high-level interface (but also usable standalone), offering an accessible way to experiment with DL without sacrificing performance. Keras provides user-friendly abstractions for layers, models, loss functions, and optimizers, allowing users for quick prototyping of neural networks for tasks like image classification, text generation, and time series forecasting with minimal code. Keras abstracts away much of the complexity of TensorFlow while retaining flexibility, making it ideal for beginners and those who need fast experimentation.</p>
</section>
<section id="tensorflow">
<h4>TensorFlow<a class="headerlink" href="#tensorflow" title="Link to this heading"></a></h4>
<p>Developed by Google, <strong>TensorFlow</strong> is a powerful open-source library primarily for DL but versatile enough for a broad range of ML tasks. It provides a flexible ecosystem for building complex models, including neural networks for computer vision, natural language processing, and time series analysis. TensorFlow supports distributed computing across CPUs, GPUs, and TPUs, making it suitable for both research and production at scale. Its robust features, such as TensorBoard for visualization, TensorFlow Serving for model deployment, and TensorFlow Lite for mobile inference, make it a comprehensive framework for end-to-end ML development. TensorFlow’s high-level Keras API simplifies model building, while its low-level operations provide flexibility for advanced research. TensorFlow is well-suited for tasks like image recognition, natural language processing (NLP), and reinforcement learning, though its complexity can pose a steeper learning curve for beginners compared to alternatives like PyTorch.</p>
</section>
<section id="pytorch">
<h4>PyTorch<a class="headerlink" href="#pytorch" title="Link to this heading"></a></h4>
<p>Developed by Facebook’s AI Research Lab (FAIR), PyTorch is auser-friendly and open-source DL library that has gained significant popularity in academia and industry. Known for its intuitive design and “define-by-run” (eager execution) approach, PyTorch allows developers to build, train, and debug models in a flexible and interactive manner. Its strong support for GPU acceleration and extensive ecosystem-ranging from computer vision (TorchVision) to NLP (TorchText) and audio (TorchAudio) – make it an excellent choice for cutting-edge DL research and production. Popular in academia and increasingly in industry, PyTorch excels in rapid prototyping and experimentation but is less optimized for production deployment compared to TensorFlow. Its active community and support for GPU acceleration make it a favorite for cutting-edge ML and DL research.</p>
</section>
<section id="xgboost-lightgbm">
<h4>XGBoost &amp; LightGBM<a class="headerlink" href="#xgboost-lightgbm" title="Link to this heading"></a></h4>
<p><strong>XGBoost</strong> (Extreme Gradient Boosting) and <strong>LightGBM</strong> (Light Gradient Boosting Machine) are high-performance gradient boosting libraries that have become go-to solutions for structured data problems, such as tabular datasets. Both libraries implement optimized gradient boosting algorithms that deliver fast training speeds, high accuracy, and scalability to large datasets. XGBoost is known for its robustness and versatility, while LightGBM offers further speed and memory efficiency through histogram-based algorithms and leaf-wise growth strategies. These libraries have become essential tools for data scientists working with structured data, outperforming traditional models in many real-world scenarios.</p>
</section>
<section id="hugging-face-transformers">
<h4>Hugging Face Transformers<a class="headerlink" href="#hugging-face-transformers" title="Link to this heading"></a></h4>
<p><strong>Hugging Face Transformers</strong> is a cutting-edge library that provides access to state-of-the-art pre-trained models for NLP tasks and computer vision, including text classification, translation, summarization, and question answering. The library’s pre-trained models and tokenizers simplify NLP workflows by enabling rapid experimentation with large language models, and in addition, this library supports both TensorFlow and PyTorch backends, integrating with datasets via Hugging Face’s datasets library, and has a vibrant community contributing to its continuous development.</p>
</section>
<section id="fastai">
<h4>FastAI<a class="headerlink" href="#fastai" title="Link to this heading"></a></h4>
<p><strong>FastAI</strong> is a high-level DL library built on PyTorch, designed to make AI accessible to a wider audience by simplifying complex tasks. It provides high-level abstractions and best practices out-of-the-box, allowing users to train powerful models with minimal code and optimal defaults. FastAI is particularly well-known for its transfer learning capabilities, enabling quick adaptation of pre-trained models for tasks like image classification and text generation. With its focus on practical usage, education, and strong community support, FastAI is ideal for beginners and practitioners who want to quickly deploy models without deep theoretical expertise.</p>
</section>
<section id="jax">
<h4>JAX<a class="headerlink" href="#jax" title="Link to this heading"></a></h4>
<p>JAX, developed by Google, combines NumPy-like syntax with automatic differentiation and GPU/TPU acceleration, making it ideal for high-performance ML research. It enables composable function transformations (gradients, JIT compilation) and scales efficiently across hardware. While not as high-level as TensorFlow or PyTorch, JAX is favored for cutting-edge numerical computing, physics simulations, and advanced neural network research where speed and flexibility are crucial.</p>
<p>These libraries cater to different needs: Scikit-learn for classical ML, TensorFlow and PyTorch for DL and scalability, Keras for simplicity, XGBoost for high-performance tabular data tasks, and Hugging Face for transformer-based applications. The choice of these libraries depends on the task, data type, scalability needs, user expertise, and whether the focus is research, prototyping, or production deployment.</p>
<p>A summary of best features and key strengths of these libraries are summarized below.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Library</p></th>
<th class="head text-center"><p>Best Feature</p></th>
<th class="head text-center"><p>Key Strength</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Scikit-Learn</p></td>
<td class="text-center"><p>Simple and consistent API <br>for classical machine learning tasks  <br>(classification, regression, clustering) <br>and small/medium datasets</p></td>
<td class="text-center"><p>Seamless integration with NumPy/Pandas <br>and extensive documentation for ease-of-use with <br>wide algorithm support</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>PyTorch</p></td>
<td class="text-center"><p>Dynamic computation graph (define-by-run) <br>for flexible model building and debugging</p></td>
<td class="text-center"><p>Flexible, intuitive framework with strong adoption <br>for academic research in DL tasks</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>TensorFlow</p></td>
<td class="text-center"><p>Scalability with GPU/TPU acceleration <br>for complex deep learning models</p></td>
<td class="text-center"><p>Excellent ecosystem (Keras, TF Hub, TF-Agents) <br>for production-scale applications</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Keras</p></td>
<td class="text-center"><p>High-level, user-friendly API <br>for rapid prototyping</p></td>
<td class="text-center"><p>Simplifies construction of DL models, making it beginner-friendly <br>and efficient with TensorFlow compatibility <br>for quick model development</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>XGBoost &amp; <br>LightGBM</p></td>
<td class="text-center"><p>Optimized gradient boosting algorithms</p></td>
<td class="text-center"><p>Extremely effective for high-performance <br>supervised learning with tabular/structured data</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Hugging Face <br>Transformers</p></td>
<td class="text-center"><p>Extensive pretrained transformer models <br>for easy fine-tuning</p></td>
<td class="text-center"><p>Community-driven ecosystem with user-friendly pipelines <br>for NLP and vision tasks</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>FastAI</p></td>
<td class="text-center"><p>Transfer learning made easy <br>for NLP &amp; vision tasks</p></td>
<td class="text-center"><p>Fast prototyping with minimal code and strong performance <br>for applied deep learning</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>JAX</p></td>
<td class="text-center"><p>NumPy + autodiff + GPU/TPU acceleration</p></td>
<td class="text-center"><p>Cutting-edge numerical computing, works with PyTorch/TensorFlow <br>via interoperability libraries, but offers lower-level control</p></td>
</tr>
</tbody>
</table>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Representative types of machine learning include supervised learning, unsupervised learning, semi-supervised, reinforcement learning, and the other subtypes.
Supervised and unsupervised learnings with specific tasks will be covered in this workshop.</p></li>
<li><p>The general workflow of a machine learning project include identification of problems, data collection, data preprocessing and processing, training and evaluating model performance, and fine-tuning model hyperparameters, and finally depolyment.</p></li>
<li><p>Representative machine learning libraries include Scikit-learn, Keras, TensorFlow, PyTorch, <em>etc.</em>.</p></li>
</ul>
</div>
</section>
</section>
</section>
<span id="document-03-scientific-data-for-ML"></span><section id="scientific-data-for-machine-learning">
<h2>Scientific Data for Machine Learning<a class="headerlink" href="#scientific-data-for-machine-learning" title="Link to this heading"></a></h2>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Gain an overview of different formats for scientific data.</p></li>
<li><p>Understand common performance pitfalls when working with big data.</p></li>
<li><p>Describe representative data storage formats and their pros and cons.</p></li>
<li><p>Understand data structures for machine learning.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>30 min teaching</p></li>
<li><p>20 min exercising</p></li>
</ul>
</div>
<section id="big-data">
<h3>Big Data<a class="headerlink" href="#big-data" title="Link to this heading"></a></h3>
<div class="admonition-discussion discussion important admonition" id="discussion-0">
<p class="admonition-title">Discussion</p>
<ul class="simple">
<li><p>How large is the data you are working with?</p></li>
<li><p>Are you experiencing performance bottlenecks when you try to analyse it?</p></li>
</ul>
</div>
<p>Big Data refers to datasets that are so large, complex, or fast-changing that traditional data processing tools cannot handle them efficiently. It encompasses not only the sheer volume of data but also its variety, velocity, and veracity — often summarized as the <strong>4 Vs</strong> of big data. These datasets can come from numerous sources, including social media, sensor networks, scientific experiments, and transactional systems. The ability to collect and analyze such massive amounts of information allows organizations and researchers to uncover trends, correlations, and insights that would be impossible to detect with smaller datasets.</p>
<p>The emergence of big data has transformed multiple domains, from business analytics and healthcare to climate science and genomics. Advanced computational methods, distributed storage systems, and parallel processing frameworks such as Hadoop and Spark have become essential for managing and analyzing these vast datasets. Efficient handling of big data enables organizations to make data-driven decisions, optimize operations, and identify opportunities for innovation.</p>
<p>In the context of ML, big data provides the raw material that fuels the learning process. Large and diverse datasets allow ML models to capture complex patterns, generalize well to unseen data, and improve predictive performance. Without sufficient and high-quality data, even the most sophisticated algorithms cannot perform effectively. From image recognition to natural language processing, every ML application depends on properly curated datasets for training, validation, and testing.
Therefore, data is the backbone of ML as it serves as the foundation for training models to recognize patterns, make predictions, and generate insights. In addition, data determines the applicability and scalability of ML solutions across domains, from scientific research to real-world applications.</p>
<p>In this episode, we will dive into the world of scientific data, examining the various types and forms it can take and understanding how it is organized and stored. We will explore different data storage formats, highlighting representative formats along with their respective advantages and limitations. This will provide a solid foundation for making informed decisions about how to handle and manipulate data effectively.
More importantly, we will focus on the data structures that are commonly used in ML and DL projects. Understanding these structures is essential for efficiently preparing, processing, and feeding data into models, ultimately enabling accurate predictions and insights. By the end of this session, you will have a clear understanding of how scientific data is organized and how it can be structured to support ML and DL workflows.</p>
</section>
<section id="understanding-scientific-data">
<h3>Understanding Scientific Data<a class="headerlink" href="#understanding-scientific-data" title="Link to this heading"></a></h3>
<p>Scientific data refers to any form of data that is collected, observed, measured, or generated as part of scientific research or experimentation. This data is used to support scientific analysis, develop theories, and validate hypotheses. It can come from a wide range of sources, including experiments, simulations, observations, or surveys across various scientific fields.</p>
<p>In general, scientific data can be described ty two terms: <strong>types of data</strong> and <strong>forms of data</strong>. They are related but distinct — types describe the nature of the data, while forms describe the how the data is structured and formatted (and stored, which will be discussed below).</p>
<section id="types-of-scientific-data">
<h4>Types of scientific data<a class="headerlink" href="#types-of-scientific-data" title="Link to this heading"></a></h4>
<p>Types of scientific data refer to what the data represents. It focuses on the nature or category of the data content.</p>
<ul class="simple">
<li><p><strong>Bit and byte</strong>: The smallest unit of storage in a computer is a <strong>bit</strong>, which holds either a 0 or a 1. Typically, eight bits are grouped together to form a <strong>byte</strong>. A single byte (8 bits) can represent up to 256 distinct values. By organizing bytes in various ways, computers can interpret and store different types of data.</p></li>
<li><p><strong>Numerical data</strong>: Different numerical data types (<em>e.g.</em>, integer and floating-point numbers) require different binary representation. Using more bytes for each value increases the range or precision, but it consumes more memory.</p>
<ul>
<li><p>For example, integers stored with 1 byte (8 bits) have a range from [-128, 127], while with 2 bytes (16 bits) the range becomes [-32768, 32767]. Integers are whole numbers and can be represented exactly given enough bytes.</p></li>
<li><p>In contrast, floating-point numbers (used for decimals) often suffer from representation errors, since most fractional values cannot be precisely expressed in binary. These errors can accumulate during arithmetic operations. Therefore, in scientific computing, numerical algorithms must be carefully designed to minimize error accumulation. To ensure stability, floating-point numbers are typically allocated 8 bytes (64 bits), keeping approximation errors small enough to avoid unreliable results.</p></li>
<li><p>In ML/DL, half, single, and double precision refer to different formats for representing floating-point numbers, typically using 16, 32, and 64 bits, respectively.</p>
<ul>
<li><p><strong>Single precision</strong> (32-bit) is commonly used as a balance between computational efficiency and numerical accuracy.</p></li>
<li><p><strong>Half precision</strong> (16-bit) offers faster computation and reduced memory usage, making it popular for training large models on GPUs, though it may suffer from lower numerical stability.</p></li>
<li><p><strong>Double precision</strong> (64-bit) provides higher accuracy but is slower and more memory-intensive, so it’s mainly used when high numerical precision is critical.</p></li>
<li><p>Many modern frameworks, like TensorFlow and PyTorch, support mixed precision training, combining half and single precision to optimize performance while maintaining stability.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Text data</strong>: When it comes to text data, the simplest character encoding is ASCII (American Standard Code for Information Interchange), which was the most widely used encoding until 2008 when UTF-8 took over. The original ASCII uses only 7 bits for representing each character and therefore can encode 128 specified characters. Later, it became common to use an 8-bit byte to store each character, resulting in extended ASCII with support for up to 256 characters. As computers became more powerful and the need for including more characters from other alphabets, UTF-8 became the most common encoding. UTF-8 uses a minimum of one byte and up to four bytes per character. This flexibility makes UTF-8 ideal for modern applications requiring global character support.</p></li>
<li><p><strong>Metadata</strong>: Metadata encompasses diverse information about data, including units, timestamps, identifiers, and other descriptive attributes. While most scientific data is either numerical or textual, the associated metadata is usually domain-specific, and different types of data may have different metadata conventions. In scientific applications, such as simulations and experimental results, metadata is typically integrated with the corresponding dataset to ensure proper interpretation and reproducibility.</p></li>
</ul>
</section>
<section id="forms-of-scientific-data">
<h4>Forms of scientific data<a class="headerlink" href="#forms-of-scientific-data" title="Link to this heading"></a></h4>
<p>Forms of scientific data refer to how the data is structured or formatted. It focuses on the presentation or shape of the data.</p>
<ul class="simple">
<li><p><strong>Tabular data structure</strong> (numerical arrays) is a collection of numbers arranged in a specific structure that one can perform mathematical operations on. Examples of numerical arrays are scalar (0D), row or column vector (1D), matrix (2D), and tensor (3D), <em>etc.</em></p></li>
<li><p><strong>Textual data structure</strong> is a format for storing and organizing text-based data. It represents unstructured or semi-structured information as sequences of characters (letters, numbers, symbols, punctuation) arranged in strings.</p></li>
<li><p><strong>Images, videos, and audio</strong> are forms of scientific data that represent information through visual and auditory formats. Images capture static visual information as pixel arrays, videos combine sequential frames to show temporal changes, and audio encodes sound signals as time-series data for analysis.</p></li>
<li><p><strong>Graphs and networks</strong> are forms of scientific data that represent relationships between entities as nodes and connections as edges. They are used to model complex systems such as social networks, molecular interactions, and ecological food webs, capturing the structure and connectivity of scientific phenomena.</p></li>
</ul>
</section>
</section>
<section id="data-storage-format">
<h3>Data Storage Format<a class="headerlink" href="#data-storage-format" title="Link to this heading"></a></h3>
<section id="representative-data-storage-format">
<h4>Representative data storage format<a class="headerlink" href="#representative-data-storage-format" title="Link to this heading"></a></h4>
<p>When it comes to data storage, there are many types of storage formats used in scientific computing and data analysis. There isn’t one data storage format that works in all cases, so choose a file format that best suits your data.</p>
<p>For tabular data, each column usually has a name and a specific data type while each row is a distinct sample which provides data according to each column (including missing values). The simplest way to save tabular data is using the so-called CSV (comma-separated values) file, which is human-readable and easily shareable. However, it is not the best format to use when working with big (numerical) data.</p>
<p>Gridded data is another very common data type in which numerical data is normally saved in a multi-dimensional grid (array). Common field-agnostic array formats include:</p>
<ul class="simple">
<li><p><strong>Hierarchical Data Format</strong> (HDF5) is a high performance storage format for storing large amounts of data in multiple datasets in a single file. It is especially popular in fields where you need to store big multidimensional arrays such as physical sciences.</p></li>
<li><p><strong>Network Common Data Form version 4</strong> (NetCDF4) is a data format built on top of HDF5, but exposes a simpler API with a more standardised structure. NetCDF4 is one of the most used formats for storing large data from big simulations in physical sciences.</p></li>
<li><p><strong>Zarr</strong> is a data storage format designed for efficiently storing large, multi-dimensional arrays in a way that supports scalability, chunking, compression, and cloud-readiness.</p></li>
<li><p>There are more file formats like <a class="reference external" href="https://arrow.apache.org/docs/python/feather.html">feather</a>, <a class="reference external" href="https://arrow.apache.org/docs/python/parquet.html">parquet</a>, <a class="reference external" href="https://docs.xarray.dev/en/stable/">xarray</a> and <a class="reference external" href="https://numpy.org/doc/stable/reference/routines.io.html">npy</a> to store arrow tables or data frames.</p></li>
</ul>
</section>
<section id="overview-of-data-storage-format">
<h4>Overview of data storage format<a class="headerlink" href="#overview-of-data-storage-format" title="Link to this heading"></a></h4>
<p>Below is an overview of common data formats (✅ for <em>good</em>, 🟨 for <em>ok/depends on a case</em>, and ❌ for <em>bad</em>) adapted from Aalto university’s <a class="reference external" href="https://aaltoscicomp.github.io/python-for-scicomp/work-with-data/#what-is-a-data-format">Python for scientific computing</a>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Name</p></th>
<th class="head text-center"><p>Human <br>readable</p></th>
<th class="head text-center"><p>Space <br>efficiency</p></th>
<th class="head text-center"><p>Arbitrary <br>data</p></th>
<th class="head text-center"><p>Tidy <br>data</p></th>
<th class="head text-center"><p>Array <br>data</p></th>
<th class="head text-center"><p>Long term <br>storage/sharing</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Pickle</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>🟨</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>🟨</p></td>
<td class="text-center"><p>🟨</p></td>
<td class="text-center"><p>❌</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>CSV</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>🟨</p></td>
<td class="text-center"><p>✅</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Feather</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>❌</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Parquet</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>🟨</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>🟨</p></td>
<td class="text-center"><p>✅</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>npy</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>🟨</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>❌</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>HDF5</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>✅</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>NetCDF4</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>✅</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>JSON</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>🟨</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>✅</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Excel</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>🟨</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>🟨</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Graph formats</p></td>
<td class="text-center"><p>🟨</p></td>
<td class="text-center"><p>🟨</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>❌</p></td>
<td class="text-center"><p>✅</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="data-structures-for-ml-dl">
<h3>Data Structures for ML/DL<a class="headerlink" href="#data-structures-for-ml-dl" title="Link to this heading"></a></h3>
<p>ML (and DL) models require numerical input, so we must collect adaquate numerical data before training.
For ML tasks, multimedia data like image, audio, or video formats should be converted into tabular data or numerical arrays that ML models can process.
This conversion enables models to extract meaningful features, such as pixel intensities, audio frequencies or motion patterns, for tasks like classification or prediction.</p>
<section id="numerical-array">
<h4>Numerical array<a class="headerlink" href="#numerical-array" title="Link to this heading"></a></h4>
<p>Numerical array is a collection of numbers arranged in a specific structure that one can perform mathematical operations on. Examples of numerical arrays are scalar (0D), row or column vector (1D), matrix (2D), and tensor (3D), <em>etc.</em></p>
<p>Python offers powerful libraries like NumPy, PyTorch, TensorFlow, and Dask (parallel Numpy) to work with numerical arrays (0D to <em>n</em>D).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># 0D (Scalar)</span>
<span class="n">scalar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>  

<span class="c1"># 1D (Vector)</span>
<span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>  

<span class="c1"># 2D (Matrix)</span>
<span class="n">matrix_2D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>  

<span class="c1"># 3D (Matrix)</span>
<span class="n">matrix_3D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">matrix_3D</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="tensor">
<h4>Tensor<a class="headerlink" href="#tensor" title="Link to this heading"></a></h4>
<p>In ML and DL, a tensor is a mathematical object used to represent and manipulate multidimensional data. It generalizes scalars, vectors, and matrices to higher dimensions, serving as the fundamental data structure in frameworks like TensorFlow and PyTorch.</p>
<p>Why to use tensors in ML/DL (advantages of Tensor)?</p>
<ul class="simple">
<li><p>Generalization of scalars/vectors/matrices: Tensors extend these concepts to any number of dimensions, which is essential for handling complex data like images (3D) and videos (4D+).</p></li>
<li><p>Consistency: Tensors unify data structures across ML/DL frameworks, simplifying model building, training, and deployment.</p></li>
<li><p>Efficient computation: Frameworks like TensorFlow and PyTorch optimize tensor operations for speed (using GPUs/TPUs).</p></li>
<li><p>Neural network representations: Input data (images, text) is converted to tensors.</p></li>
<li><p>Automatic differentiation: Tensors support gradient tracking, which is vital for backpropagation in neural networks.</p></li>
</ul>
<div class="admonition-tensor-creation-and-operations exercise important admonition" id="exercise-0">
<p class="admonition-title">Tensor Creation and Operations</p>
<p>In <a class="reference internal" href="#document-jupyter-notebooks/3-Tensor"><span class="std std-doc">Jupyter Notebook</span></a> we provide a tutorial about Tensor including</p>
<ul class="simple">
<li><p>Tensor creation</p></li>
<li><p>Tensor’s properties (<code class="docutils literal notranslate"><span class="pre">shape</span></code>, <code class="docutils literal notranslate"><span class="pre">dtype</span></code>, <code class="docutils literal notranslate"><span class="pre">ndim</span></code>)</p></li>
<li><p>Tensor operations</p>
<ul>
<li><p>indexing, slicing, transposing</p></li>
<li><p>element-wise operations: addition, subtraction, <em>etc.</em></p></li>
<li><p>matrix multiplication(<code class="docutils literal notranslate"><span class="pre">np.dot</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.matmul</span></code>)</p></li>
<li><p>reshaping, flattening, squeezing, unsqueezing</p></li>
<li><p>reduction operations: sum, mean, max along axes</p></li>
<li><p>broadcasting: Rules and examples</p></li>
</ul>
</li>
<li><p>Tensors in DL frameworks</p>
<ul>
<li><p>moving tensors between CPUs and GPUs (suppose that you can access to GPU cards)</p></li>
</ul>
</li>
</ul>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Key characteristics of big data — volume, variety, velocity, and veracity.</p></li>
<li><p>High-quality, large datasets are essential for training effective machine learning models</p></li>
<li><p>Scientific data has different types, including numerical, textual, and multimedia data, and these data can take different forms, such as tabular arrays, grids, and images.</p></li>
<li><p>Scientific data are stored in various formats including CSV, HDF5, and others with their respective advantages and limitations.</p></li>
<li><p>Tensors as a generalization of numerical data in machine learning (deep learning).</p></li>
<li><p>Tensors allow models to efficiently handle complex, multidimensional data such as images, videos, and audio.</p></li>
</ul>
</div>
</section>
</section>
</section>
<span id="document-04-data-preparation-for-ML"></span><section id="data-preparation-for-machine-learning">
<h2>Data Preparation for Machine Learning<a class="headerlink" href="#data-preparation-for-machine-learning" title="Link to this heading"></a></h2>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Provide an overview of data preparation.</p></li>
<li><p>Load the Penguins dataset.</p></li>
<li><p>Use pandas and seaborn to analyze and visualize the data.</p></li>
<li><p>Identify and manage missing values and outliers in the dataset.</p></li>
<li><p>Encode categorical variables into numerical values suitable for machine learning models.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>30 min teaching</p></li>
<li><p>20 min exercising</p></li>
</ul>
</div>
<p>In <a class="reference internal" href="#document-02-fundamentals-of-ML"><span class="std std-doc">Episode 2: Fundamentals of Machine Learning</span></a>, it is clearly shown that data preparation and processing often consume a significant portion of the ML workflow — often more time than the actual model training, evaluation, and optimization.
Cleaning, transforming, and structuring raw data into a usable format ensures that algorithms can efficiently extract valuable insights.
Additionally, the choice of data formats, such as CSV for simplicity or HDF5 for large-scale datasets, can significantly impact data storage, accessibility, and computational efficiency during both model training and deployment.</p>
<p>In this episode, we will provide an overview of data preparation and introduce available public datasets. Using the Penguins dataset as an example, we will offer demonstrations and hands-on exercises to develop a comprehensive understanding of data preparation including handling missing values and outliers, encoding categorical variables, and other essential preprocessing techniques for ML workflows.</p>
<section id="what-is-data-preparation">
<h3>What is Data Preparation<a class="headerlink" href="#what-is-data-preparation" title="Link to this heading"></a></h3>
<p>Data preparation refers to the process of cleaning, structuring, and transforming raw data into a structured, high-quality format ready for statistical analysis and ML. It’s one of the most critical steps in the ML workflow because high-quality data leads to better model performance. Key procedures include:</p>
<ul class="simple">
<li><p>collecting data from multiple sources,</p></li>
<li><p>handling missing values (imputation or removal),</p></li>
<li><p>detecting and treating outliers,</p></li>
<li><p>encoding categorical variables,</p></li>
<li><p>normalizing or scaling features,</p></li>
<li><p>feature selection and feature engineering.</p></li>
</ul>
</section>
<section id="collecting-data-from-multiple-sources">
<h3>Collecting Data from Multiple Sources<a class="headerlink" href="#collecting-data-from-multiple-sources" title="Link to this heading"></a></h3>
<p>Data preparation begins with collecting raw data from a wide variety of sources, including databases, sensors, APIs, web scraping, surveys, and existing public datasets.</p>
<p>During the data collection process, it is important to ensure consistency and compatibility across all sources. Different sources may have different formats, units, naming conventions, or levels of quality. Careful integration, cleaning, and normalization are required to create a unified dataset suitable for analysis or modeling. Proper documentation of sources and collection methods is also essential to maintain reproducibility and data governance.</p>
<p>Public datasets provide an excellent resource for learning, experimentation, and benchmarking. Some widely used datasets across different domains include:</p>
<ul class="simple">
<li><p>Tabular datasets: Iris, <strong>Penguins</strong>, Titanic, Boston Housing, Wine, <em>etc.</em></p></li>
<li><p>Image datasets: MNIST, CIFAR-10, CIFAR-100, COCO, ImageNet.</p></li>
<li><p>Text datasets: IMDB Reviews, 20 Newsgroups, Sentiment140.</p></li>
<li><p>Audio datasets: LibriSpeech, UrbanSound8K, ESC-50.</p></li>
<li><p>Video datasets: UCF101, Kinetics, HMDB51.</p></li>
</ul>
<p>These datasets are available on platforms like Kaggle, UCI Machine Learning Repository, TensorFlow Datasets, and Hugging Face Datasets, providing accessible resources for practice and innovation.</p>
<p>It should be noted that most of the data available by default is too raw to perform statistical analysis. Proper preprocessing is essential before the data can be used to identify meaningful patterns or to train models for prediction. In the following sections, we use the Penguins dataset as an example to demonstrate essential data preprocessing steps. These include handling missing values, detecting and treating outliers, encoding categorical variables, and performing other necessary transformations to prepare the dataset for ML tasks. Proper preprocessing ensures data quality, reduces bias, and improves the performance and reliability of the models we build.</p>
</section>
<section id="the-penguins-dataset">
<h3>The Penguins Dataset<a class="headerlink" href="#the-penguins-dataset" title="Link to this heading"></a></h3>
<p>The <a class="reference external" href="https://zenodo.org/records/3960218">Palmer Penguins dataset</a> is a widely used open dataset in data science and ML education. This dataset contains information on three penguin species that inhabit islands near the Palmer Archipelago in Antarctica: Adelie, Chinstrap, and Gentoo. Each row in the dataset corresponds to a single penguin and records both physical measurements and categorical attributes. The key numerical features include flipper length (mm), culmen length and depth (bill measurements, in mm), and body mass (g). Alongside these, categorical variables such as species, island, and sex are provided.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/4-penguins-categories.png"><img alt="_images/4-penguins-categories.png" src="_images/4-penguins-categories.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-text">These data were collected from 2007 - 2009 by Dr. Kristen Gorman with the <a class="reference external" href="https://lternet.edu/site/palmer-antarctica-lter/">Palmer Station Long Term Ecological Research Program</a>, part of the <a class="reference external" href="https://lternet.edu/">US Long Term Ecological Research Network</a>. The data were imported directly from the <a class="reference external" href="https://edirepository.org/">Environmental Data Initiative (EDI)</a> Data Portal, and are available for use by CC0 license (“No Rights Reserved”) in accordance with the <a class="reference external" href="https://lternet.edu/data-access-policy/">Palmer Station Data Policy</a>.</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="importing-dataset">
<h3>Importing Dataset<a class="headerlink" href="#importing-dataset" title="Link to this heading"></a></h3>
<p>Seaborn provides the Penguins dataset through its built-in data-loading functions. We can access it using <code class="docutils literal notranslate"><span class="pre">sns.load_dataset('penguin')</span></code> and then have a quick look at the data (code examples are availalbe in the <a class="reference internal" href="#document-jupyter-notebooks/4-Data-Preprocessing"><span class="std std-doc">Jupyter Notebook</span></a>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="n">penguins</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;penguins&#39;</span><span class="p">)</span>
<span class="n">penguins</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you have your own dataset stored in a CSV file, you can easily load it into Python using Pandas with the <code class="docutils literal notranslate"><span class="pre">read_csv()</span></code> function. This is one of the most common ways to bring tabular data into a DataFrame for further analysis and processing.</p>
<p>Beyond CSV files, Pandas also supports a wide variety of other file formats, making it a powerful and flexible tool for data handling. For example, you can use <code class="docutils literal notranslate"><span class="pre">read_excel()</span></code> to import data from Microsoft Excel spreadsheets, <code class="docutils literal notranslate"><span class="pre">read_hdf()</span></code> to work with HDF5 binary stores, and <code class="docutils literal notranslate"><span class="pre">read_json()</span></code> to load data from JSON files. Each of these formats also has a corresponding method for saving data back to disk, such as <code class="docutils literal notranslate"><span class="pre">to_csv()</span></code>, <code class="docutils literal notranslate"><span class="pre">to_excel()</span></code>, <code class="docutils literal notranslate"><span class="pre">to_hdf()</span></code>, and <code class="docutils literal notranslate"><span class="pre">to_json()</span></code>.</p>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td></td>
<td><p>species</p></td>
<td><p>island</p></td>
<td><p>bill_length <br>(mm)</p></td>
<td><p>bill_depth <br>(mm)</p></td>
<td><p>flipper <br>length <br>(mm)</p></td>
<td><p>body_mass <br>(g)</p></td>
<td><p>sex</p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>39.1</p></td>
<td><p>18.7</p></td>
<td><p>181.0</p></td>
<td><p>3750.0</p></td>
<td><p>Male</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>39.5</p></td>
<td><p>17.4</p></td>
<td><p>186.0</p></td>
<td><p>3800.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>40.3</p></td>
<td><p>18.0</p></td>
<td><p>195.0</p></td>
<td><p>3250.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>36.7</p></td>
<td><p>19.3</p></td>
<td><p>193.0</p></td>
<td><p>3450.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-odd"><td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
</tr>
<tr class="row-even"><td><p>339</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
</tr>
<tr class="row-odd"><td><p>340</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>46.8</p></td>
<td><p>14.3</p></td>
<td><p>215.0</p></td>
<td><p>4850.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>341</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>50.4</p></td>
<td><p>15.7</p></td>
<td><p>222.0</p></td>
<td><p>5750.0</p></td>
<td><p>Male</p></td>
</tr>
<tr class="row-odd"><td><p>342</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>45.2</p></td>
<td><p>14.8</p></td>
<td><p>212.0</p></td>
<td><p>5200.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>343</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>49.9</p></td>
<td><p>16.1</p></td>
<td><p>213.0</p></td>
<td><p>5400.0</p></td>
<td><p>Male</p></td>
</tr>
</tbody>
</table>
<p>There are seven columns include:</p>
<ul class="simple">
<li><p><em>species</em>: penguin species (Adelie, Chinstrap, Gentoo)</p></li>
<li><p><em>island</em>: island where the penguin was found (Biscoe, Dream, Torgersen)</p></li>
<li><p><em>bill_length_mm</em>: length of the bill</p></li>
<li><p><em>bill_depth_mm</em>: depth of the bill</p></li>
<li><p><em>flipper_length_mm</em>: length of the flipper</p></li>
<li><p><em>body_mass_g</em>: body mass in grams</p></li>
<li><p><em>sex</em>: male or female</p></li>
</ul>
<p>Looking only at the raw numbers in the <code class="docutils literal notranslate"><span class="pre">penguins</span></code> DataFrame, or even examining the statistical summaries provided by <code class="docutils literal notranslate"><span class="pre">penguins.info()</span></code> and <code class="docutils literal notranslate"><span class="pre">penguins.describe()</span></code>, often does not give us a clear intuition about the patterns and relationships in the data. To truly understand the dataset, we generally prefer to visualize the data, since graphical representations can reveal trends, groupings, and anomalies that may remain hidden in numerical summaries alone.</p>
<p>One nice visualization for datasets with relatively few attributes is the <strong>Pair Plot</strong>, which can be created using <code class="docutils literal notranslate"><span class="pre">sns.pairplot(...)</span></code>. It shows a scatterplot of each attribute plotted against each of the other attributes. By using the <code class="docutils literal notranslate"><span class="pre">hue='species'</span></code> setting for the pairplot the graphs on the diagonal are layered kernel density estimate plots for the different values of the <code class="docutils literal notranslate"><span class="pre">species</span></code> column.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">penguins</span><span class="p">[[</span><span class="s2">&quot;species&quot;</span><span class="p">,</span> <span class="s2">&quot;bill_length_mm&quot;</span><span class="p">,</span> <span class="s2">&quot;bill_depth_mm&quot;</span><span class="p">,</span> <span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">,</span>
	<span class="s2">&quot;body_mass_g&quot;</span><span class="p">]],</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;species&quot;</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-penguins-pairplot.png"><img alt="_images/4-penguins-pairplot.png" src="_images/4-penguins-pairplot.png" style="width: 100%;" />
</a>
</figure>
<div class="admonition-discussion discussion important admonition" id="discussion-0">
<p class="admonition-title">Discussion</p>
<p>Take a look at the pairplot we created. Consider the following questions:</p>
<ul class="simple">
<li><p>Is there any class that is easily distinguishable from the others?</p></li>
<li><p>Which combination of attributes shows the best separation for all 3 class labels at once?</p></li>
<li><p>For pairplot with <code class="docutils literal notranslate"><span class="pre">hue=&quot;sex&quot;</span></code>, which combination of features distinguishes the two sexes best?</p></li>
<li><p>What about the one with <code class="docutils literal notranslate"><span class="pre">hue=&quot;island&quot;</span></code>?</p></li>
</ul>
</div>
</section>
<section id="handling-missing-values">
<h3>Handling Missing Values<a class="headerlink" href="#handling-missing-values" title="Link to this heading"></a></h3>
<p>Upon loading the Penguins dataset into a pandas DataFrame, the initial examination reveals the presence of <code class="docutils literal notranslate"><span class="pre">NaN</span></code> (Not a Number) values within several rows (highlighted in the Jupyter notebook). These placeholders explicitly indicate missing or unavailable data for certain measurements, such as bill length or the sex of particular penguins.</p>
<p>Recognizing these missing values is an important first step, as they must be properly handled before performing any data analysis.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">penguins</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">penguins</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>
<span class="n">penguins_test</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">highlight_null</span><span class="p">(</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">)</span>
</pre></div>
</div>
<section id="numerical-features">
<h4>Numerical features<a class="headerlink" href="#numerical-features" title="Link to this heading"></a></h4>
<p>For numerical features such as bill length, bill depth, flipper length, and body mass, several strategies can be applied. A straightforward approach is to <strong>remove any rows with missing values</strong>, but this is often wasteful and reduces the sample size.</p>
<p>A more effective method is imputation: replacing missing numerical values with a suitable estimate. Common choices include the <code class="docutils literal notranslate"><span class="pre">mean</span></code> or <code class="docutils literal notranslate"><span class="pre">median</span></code> of the feature, depending on the distribution.
Before applying imputation to handle missing numerical values, it is important to first identify where the NaN values occur in the dataset. We can</p>
<ul class="simple">
<li><p>run <code class="docutils literal notranslate"><span class="pre">penguins_test.style.highlight_null(color</span> <span class="pre">=</span> <span class="pre">'red')</span></code>, where NaN values are hightlighed in the output,</p></li>
<li><p>run <code class="docutils literal notranslate"><span class="pre">print(penguins_test.info(),</span> <span class="pre">'\n')</span></code>, which provide the number of non-null entries in each column,</p>
<ul>
<li><p>By comparing the number of non-null entries with the total number of rows, we can quickly identify which features have missing values and how severe the issue is. For example, if the column sex has fewer non-null values than the total number of penguins, we know that sex information is missing for some individuals.</p></li>
</ul>
</li>
<li><p>run <code class="docutils literal notranslate"><span class="pre">print(penguins_test.isnull().mean())</span></code>, which computes the fraction of missing values in each column, giving us a normalized view of missingness across the dataset.</p>
<ul>
<li><p>unlike <code class="docutils literal notranslate"><span class="pre">.info()</span></code>, which only shows counts, this method highlights the relative proportion of missing values, which is particularly helpful when working with large datasets.</p></li>
<li><p>For instance, the <code class="docutils literal notranslate"><span class="pre">.isnull().mean()</span></code> reports that 0.2 (20%) of the entries in <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> are missing, we can decide whether to impute those values or simply drop the rows without significantly reducing the dataset size.</p></li>
</ul>
</li>
</ul>
<p>The next step is to calculate the <code class="docutils literal notranslate"><span class="pre">mean</span></code> and <code class="docutils literal notranslate"><span class="pre">median</span></code> values for the numerical features. To illustrate this process, we can take the <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> feature as an example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">body_mass_g_mean</span> <span class="o">=</span> <span class="n">penguins_test</span><span class="o">.</span><span class="n">body_mass_g</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">body_mass_g_median</span> <span class="o">=</span> <span class="n">penguins_test</span><span class="o">.</span><span class="n">body_mass_g</span><span class="o">.</span><span class="n">median</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  mean value of body_mass_g is </span><span class="si">{</span><span class="n">body_mass_g_mean</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;median value of body_mass_g is </span><span class="si">{</span><span class="n">body_mass_g_median</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1">#   mean value of body_mass_g is 4431.25</span>
<span class="c1"># median value of body_mass_g is 4325.00</span>
</pre></div>
</div>
<p>Rather than directly replacing the missing values, we concatenate new columns containing the <code class="docutils literal notranslate"><span class="pre">mean</span></code> and <code class="docutils literal notranslate"><span class="pre">median</span></code> values for the <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> feature to the end of the Penguins dataset, and than visualize the distribution of this feature.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins_test</span><span class="p">[</span><span class="s1">&#39;BMG_mean&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">penguins_test</span><span class="o">.</span><span class="n">body_mass_g</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">body_mass_g_mean</span><span class="p">)</span>
<span class="n">penguins_test</span><span class="p">[</span><span class="s1">&#39;BMG_median&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">penguins_test</span><span class="o">.</span><span class="n">body_mass_g</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">body_mass_g_median</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-distribution--body-mass-with-mutations.png"><img alt="_images/4-distribution--body-mass-with-mutations.png" src="_images/4-distribution--body-mass-with-mutations.png" style="width: 80%;" />
</a>
</figure>
<div class="admonition-exercise exercise important admonition" id="exercise-0">
<p class="admonition-title">Exercise</p>
<p>How to mutate the missing values with <code class="docutils literal notranslate"><span class="pre">mean</span></code> or <code class="docutils literal notranslate"><span class="pre">median</span></code> values in place for all numerical values (code examples are availalbe in the <a class="reference internal" href="#document-jupyter-notebooks/4-Data-Preprocessing"><span class="std std-doc">Jupyter Notebook</span></a>).</p>
<ul class="simple">
<li><p>for one numerical feature like <code class="docutils literal notranslate"><span class="pre">bill_length_mm</span></code>?</p></li>
<li><p>for all numerical features in the Penguins dataset?</p></li>
</ul>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<ul>
<li><ol class="arabic simple">
<li><p>using the following script</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins_test2</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">penguins_test2</span><span class="p">[</span><span class="s2">&quot;bill_length_mm&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">(),</span> <span class="s2">&quot;bill_length_mm&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">penguins_test2</span><span class="p">[</span><span class="s2">&quot;bill_length_mm&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>2 using the following code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># first, select only the numerical columns</span>
<span class="n">numerical_cols</span> <span class="o">=</span> <span class="n">penguins_test2</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;number&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">columns</span>

<span class="c1"># second, find which of these numerical columns have any missing values </span>
<span class="n">numerical_cols_with_nulls</span> <span class="o">=</span> <span class="n">numerical_cols</span><span class="p">[</span><span class="n">penguins_test2</span><span class="p">[</span><span class="n">numerical_cols</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">()]</span>

<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">numerical_cols_with_nulls</span><span class="p">:</span>
	<span class="c1"># calculate the mean for the specific column</span>
	<span class="n">col_mean</span> <span class="o">=</span> <span class="n">penguins_test2</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
	<span class="c1"># use `.loc` to replace NaNs only in that specific column with its own mean</span>
	<span class="n">penguins_test2</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">penguins_test2</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">isnull</span><span class="p">(),</span> <span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">col_mean</span>
</pre></div>
</div>
</li>
</ul>
</div>
<p>In certain scenarios, imputing missing data with values at the <strong>end of distribution</strong> (EoD) of a variable can be a considered strategy. This approach offers the advantage of computational speed and can theoretically capture the significance of missing entries. Typically, the EoD is calculated as an extreme value such as <code class="docutils literal notranslate"><span class="pre">mean</span> <span class="pre">+</span> <span class="pre">3*std</span></code>, where the <code class="docutils literal notranslate"><span class="pre">mean</span></code> and <code class="docutils literal notranslate"><span class="pre">std</span></code> of the feature can be obtained using the <code class="docutils literal notranslate"><span class="pre">.describe()</span></code> method.</p>
<p>However, as demonstrated in the Penguins dataset tutorial, this type of imputation often generates unrealistic values and distorts the original distribution, particularly for features like <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code>. Consequently, it can lead to biased analyses and should be used with caution.</p>
</section>
<section id="categorical-features">
<h4>Categorical features<a class="headerlink" href="#categorical-features" title="Link to this heading"></a></h4>
<p>For categorical features such as sex, the approach differs.</p>
<ul>
<li><p>One simple method is to replace missing categories with the most frequent value (<code class="docutils literal notranslate"><span class="pre">mode</span></code>), which assumes the missing value follows the majority distribution.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins_test</span><span class="o">.</span><span class="n">sex</span><span class="o">.</span><span class="n">mode</span><span class="p">()</span>

<span class="n">penguins_test</span><span class="o">.</span><span class="n">fillna</span><span class="p">({</span><span class="s1">&#39;sex&#39;</span><span class="p">:</span> <span class="n">penguins_test</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mode</span><span class="p">()[</span><span class="mi">0</span><span class="p">]},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Alternatively, missing values can be treated as a separate category, labeled for example as “Unknown” or “Missing” which allows models to learn if missingness itself carries information.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins_test</span><span class="o">.</span><span class="n">fillna</span><span class="p">({</span><span class="s1">&#39;sex&#39;</span><span class="p">:</span> <span class="s2">&quot;Missing&quot;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Another option is to apply model-based imputation, where missing categorical values are predicted from other features using classification algorithms.</p></li>
</ul>
<p>For all ML tasks we will perform in the following episodes, we adopt a straightforward approach of removing any rows that contain missing values (<code class="docutils literal notranslate"><span class="pre">penguins_test.dropna()</span></code>). This ensures that the dataset is complete and avoids potential errors or biases caused by NaN entries. Although more sophisticated imputation methods exist, dropping rows is a simple and effective strategy when the proportion of missing data is relatively small.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the other dataset, the strategy for handling missing values is not one-size-fits-all; it depends heavily on whether the missing data is numerical or categorical and the underlying mechanism causing the data to be missing. Ignoring these missing entries, such as by simply dropping the affected rows, may introduce significant bias, reduce statistical power, and ultimately lead to inaccurate or misleading conclusions.
After any imputation, it is essential to perform sanity checks to ensure the imputed values are plausible and to document the methodology transparently. Properly handling missing data in this way transforms an incomplete dataset into a robust and reliable foundation for generating accurate insights and building powerful predictive models.</p>
</div>
</section>
</section>
<section id="handling-outliers">
<h3>Handling Outliers<a class="headerlink" href="#handling-outliers" title="Link to this heading"></a></h3>
<p>Outliers are values that are too far from the rest of observations in columns. For instance, if the body mass of most of penguins in the dataset varies between 3000-6000 g, an observation of 7500 g will be considered as an outlier since such an observation occurs rarely.</p>
<p>We obtain the EoD value of the <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> feature and then check if this value is the outlier for this feature.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins</span><span class="p">[</span><span class="s1">&#39;body_mass_g&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">penguins_test</span><span class="p">[</span><span class="s1">&#39;body_mass_g&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

<span class="c1"># EoD value = 7129.0199920504665</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-body-mass-outlier.png"><img alt="_images/4-body-mass-outlier.png" src="_images/4-body-mass-outlier.png" style="width: 75%;" />
</a>
</figure>
<p>There are several approaches to identify outliers, and one of the most commonly used methods is the <strong>Interquartile Range (IQR)</strong> method. The IQR measures the spread of the middle 50% of the data and is calculated by subtracting the first quartile (25th percentile, Q1) from the third quartile (75th percentile, Q3). Once the IQR is obtained, we can determine the boundaries for detecting outliers. The lower limit is defined as Q1 minus 1.5 times the IQR, and the upper limit is defined as Q3 plus 1.5 times the IQR.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;25% quantile = </span><span class="si">{</span><span class="n">penguins_test_BMG_outlier</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;75% quantile = </span><span class="si">{</span><span class="n">penguins_test_BMG_outlier</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">IQR</span> <span class="o">=</span> <span class="n">penguins_test_BMG_outlier</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span> <span class="o">-</span> <span class="n">penguins_test_BMG_outlier</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">lower_bmg_limit</span> <span class="o">=</span> <span class="n">penguins_test_BMG_outlier</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">IQR</span><span class="p">)</span>
<span class="n">upper_bmg_limit</span> <span class="o">=</span> <span class="n">penguins_test_BMG_outlier</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">IQR</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;lower limt of IQR = </span><span class="si">{</span><span class="n">lower_bmg_limit</span><span class="si">}</span><span class="s2"> and upper limit of IQR = </span><span class="si">{</span><span class="n">upper_bmg_limit</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 25% quantile = 3550.00</span>
<span class="c1"># 75% quantile = 4781.25</span>
<span class="c1"># lower limt of IQR = 1703.125 and upper limit of IQR = 6628.125</span>
</pre></div>
</div>
<p>Any data points that fall below the lower limit or above the upper limit are considered outliers, and these points are subsequently removed from the dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins_test_BMG_outlier</span><span class="p">[</span><span class="n">penguins_test_BMG_outlier</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">upper_bmg_limit</span><span class="p">]</span>

<span class="n">penguins_test_BMG_outlier</span><span class="p">[</span><span class="n">penguins_test_BMG_outlier</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">lower_bmg_limit</span><span class="p">]</span>

<span class="n">penguins_test_BMG_outlier_remove_IQR</span> <span class="o">=</span> <span class="n">penguins_test_BMG_outlier</span><span class="p">[</span><span class="n">penguins_test_BMG_outlier</span><span class="p">[</span><span class="s2">&quot;BMG_eod&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">upper_bmg_limit</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are four main techniques for handling outliers in a dataset:</p>
<ul class="simple">
<li><p>Remove outliers entirely — This approach simply deletes the rows containing outlier values, which can be effective if the outliers are errors or rare events that are not relevant to the analysis.</p></li>
<li><p>Treat outliers as missing values — Outliers can be replaced with NaN and then handled using imputation methods described in previous sections, such as replacing them with the mean, median, or mode.</p></li>
<li><p>Apply discretization or binning — By grouping numerical values into bins, outliers are included in the tail bins along with other extreme values, which reduces their impact while preserving the overall structure of the data.</p></li>
<li><p>Cap or censor outliers — Extreme values can be limited to a maximum or minimum threshold, often derived from statistical techniques such as the IQR or standard deviation limits. This approach reduces the influence of outliers without completely removing them from the dataset.</p></li>
</ul>
</div>
<div class="admonition-the-meanstandard-deviation-approach exercise important admonition" id="exercise-1">
<p class="admonition-title"><strong>The mean–standard deviation approach</strong></p>
<p>Instead of using the IQR method, the upper and lower thresholds for detecting outliers can also be calculated with the mean-std deviation approach.</p>
<p>In this exercise (code examples are availalbe in the <a class="reference internal" href="#document-jupyter-notebooks/4-Data-Preprocessing"><span class="std std-doc">Jupyter Notebook</span></a>), you will</p>
<ul>
<li><p>Compute the <code class="docutils literal notranslate"><span class="pre">mean</span></code> and <code class="docutils literal notranslate"><span class="pre">std</span></code> of the <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> feature.</p></li>
<li><p>Calculate the upper and lower limits for outlier detection using the formulas.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}lower\_limit = mean - 3.0 \times std\\upper\_limit = mean + 3.0 \times std\end{aligned}\end{align} \]</div>
</li>
<li><p>Identify the outliers and replace them with either the <code class="docutils literal notranslate"><span class="pre">mean</span></code> or the <code class="docutils literal notranslate"><span class="pre">median</span></code> values of the <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> feature.</p></li>
</ul>
</div>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mean</span> <span class="pre">=</span> <span class="pre">penguins_test_BMG_outlier[&quot;body_mass_g&quot;].mean()</span></code> and <code class="docutils literal notranslate"><span class="pre">std</span> <span class="pre">=</span> <span class="pre">penguins_test_BMG_outlier[&quot;body_mass_g&quot;].std()</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lower_bmg_limit</span> <span class="pre">=</span> <span class="pre">mean</span> <span class="pre">-</span> <span class="pre">(3.0</span> <span class="pre">*</span> <span class="pre">std)</span></code> and <code class="docutils literal notranslate"><span class="pre">upper_bmg_limit</span> <span class="pre">=</span> <span class="pre">mean</span> <span class="pre">+</span> <span class="pre">(3.0</span> <span class="pre">*</span> <span class="pre">std)</span></code></p></li>
<li><p>determination of outliers</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">penguins_test_BMG_outlier[penguins_test_BMG_outlier[&quot;body_mass_g&quot;]</span> <span class="pre">&gt;</span> <span class="pre">upper_bmg_limit]</span></code> and <code class="docutils literal notranslate"><span class="pre">penguins_test_BMG_outlier[penguins_test_BMG_outlier[&quot;body_mass_g&quot;]</span> <span class="pre">&gt;</span> <span class="pre">upper_bmg_limit]</span></code></p></li>
</ul>
</li>
<li><p>imputation outliers with <code class="docutils literal notranslate"><span class="pre">mean</span></code> or <code class="docutils literal notranslate"><span class="pre">median</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">penguins_test_BMG_outlier.loc[penguins_test_BMG_outlier[&quot;body_mass_g&quot;]</span> <span class="pre">&lt;</span> <span class="pre">lower_bmg_limit,</span> <span class="pre">&quot;body_mass_g&quot;]</span> <span class="pre">=</span> <span class="pre">mean</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">penguins_test_BMG_outlier.loc[penguins_test_BMG_outlier[&quot;body_mass_g&quot;]</span> <span class="pre">&lt;</span> <span class="pre">lower_bmg_limit,</span> <span class="pre">&quot;body_mass_g&quot;]</span> <span class="pre">=</span> <span class="pre">penguins_test_BMG_outlier[&quot;body_mass_g&quot;].median()</span></code></p></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="encoding-categorical-variables">
<h3>Encoding Categorical Variables<a class="headerlink" href="#encoding-categorical-variables" title="Link to this heading"></a></h3>
<p>In the previous sections, we adopted a straightforward approach to handling missing values by simply removing any rows that contained NaN values, whether they were in numerical or categorical features. While this step gives us a cleaner dataset, it is not sufficient on its own to proceed with ML tasks.</p>
<p>The reason is that most ML algorithms are designed to work only with numerical data. They cannot directly process textual or symbolic categories such as species, island, or sex in the Penguins dataset. To make these categorical variables usable in modeling, we need to encode categorical variables into a numerical format while preserving their information.</p>
<p>There are two widely used encoding techniques: <strong>One-hot encoding (OHE)</strong> and <strong>Label Encoding</strong>.</p>
<section id="one-hot-encoding">
<h4>One-hot encoding<a class="headerlink" href="#one-hot-encoding" title="Link to this heading"></a></h4>
<p>The OHE method creates a new binary column for each category in a feature. For example, the species feature with values {Female, Male, and NaN} would be transformed into three new columns, each indicating the presence (1) or absence (0) of that category for a given row.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">OneHotEncoder</span>

<span class="n">penguins_sex</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[[</span><span class="s2">&quot;species&quot;</span><span class="p">,</span> <span class="s2">&quot;island&quot;</span><span class="p">,</span> <span class="s2">&quot;sex&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">sparse_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># `sparse_output=False` to get a dense array</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins_sex</span><span class="p">[[</span><span class="s1">&#39;sex&#39;</span><span class="p">]])</span>

<span class="n">encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Female&quot;</span><span class="p">,</span> <span class="s2">&quot;Male&quot;</span><span class="p">,</span> <span class="s2">&quot;NaN&quot;</span><span class="p">])</span>

<span class="n">penguins_sex_onehotencoding</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">penguins_sex</span><span class="p">,</span> <span class="n">encoded</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">penguins_sex_onehotencoding</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-penguins-sex-ohe-encoding.png"><img alt="_images/4-penguins-sex-ohe-encoding.png" src="_images/4-penguins-sex-ohe-encoding.png" style="width: 75%;" />
</a>
</figure>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>OHE works well when the number of categories is small and when categories are unordered. However, it can lead to very large datasets if a feature contains many unique categories (high cardinality).</p>
</div>
</section>
<section id="label-encoding">
<h4>Label encoding<a class="headerlink" href="#label-encoding" title="Link to this heading"></a></h4>
<p>Label encoding transforms a categorical feature into a single numerical column by assigning a unique integer to each category in a feature. For example, the sex feature with values {Female, Male, NaN} could be encoded as Female = 0, Male = 1, and missing values handled separately or imputed beforehand (in our case, NaN is encoded as  NaN = 2).</p>
<p>Unlike one-hot encoding, which creates multiple columns, label encoding produces only one column, making it more memory-efficient. However, it introduces an artificial ordinal relationship between categories (<em>e.g.</em>, implying Dream &gt; Biscoe), which may not be meaningful and can affect algorithms that assume numeric order matters, such as linear regression or distance-based methods.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins_sex</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sex_LE&quot;</span><span class="p">])</span>

<span class="n">penguins_sex_labelencoding</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">penguins_sex</span><span class="p">,</span> <span class="n">encoded</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Beyond these two common methods, there are several other encoding strategies, especially useful for more complex datasets:</p>
<ul class="simple">
<li><p><strong>Ordinal Encoding</strong>: A variation of label encoding designed specifically for variables that have a natural and meaningful order (<em>e.g.</em>, Small = 0, Medium = 1, Large = 2).</p></li>
<li><p><strong>Binary Encoding</strong>: Converts categories into binary code, achieving a good compromise between one-hot and label encoding for high-cardinality data.</p></li>
<li><p><strong>Target Encoding</strong> (Mean Encoding): Replaces each category with the average value of the target variable for that category. It is powerful but prone to overfitting if not carefully implemented.</p></li>
<li><p><strong>Frequency Encoding</strong>: Replaces categories with their frequency of occurrence in the dataset. This can be useful for dealing with high-cardinality features.</p></li>
</ul>
<p>The choice of encoding method is a consequential modeling decision. It should be guided by the type of categorical data (nominal <em>vs.</em> ordinal), the number of unique categories, the type of ML algorithm being used. A proper encoding ensures that categorical features are accurately represented in numerical form, allowing algorithms to learn patterns effectively without introducing bias or noise.</p>
</section>
<section id="the-get-dummies-function-in-pandas">
<h4>The <code class="docutils literal notranslate"><span class="pre">get_dummies()</span></code> function in Pandas<a class="headerlink" href="#the-get-dummies-function-in-pandas" title="Link to this heading"></a></h4>
<p>In addition to OHE and LE, we can also use the <code class="docutils literal notranslate"><span class="pre">.get_dummies()</span></code> in Pandas to handle categorical variables by converting them into a one-hot encoded (dummy variable) format.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">penguins_sex</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="n">penguins_sex_dummyencoding</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">penguins_sex</span><span class="p">,</span> <span class="n">dummy_encoded</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function ignores <code class="docutils literal notranslate"><span class="pre">NaN</span></code> values by default (it simply leaves them out).</p>
</div>
</section>
</section>
<section id="feature-engineering">
<h3>Feature Engineering<a class="headerlink" href="#feature-engineering" title="Link to this heading"></a></h3>
<p>Feature engineering is a part of the broader data processing pipeline in ML workflows. It involves using domain knowledge to select, modify, or create new features — variables or attributes — from existing data to help algorithms better understand patterns and relationships.</p>
<p>Feature engineering is crucial because the quality of features directly impacts a model’s predictive power. Well-crafted features can simplify complex patterns, reduce overfitting, and improve model interpretability, leading to better generalization and performance on unseen data. By tailoring features to the problem at hand, feature engineering bridges the gap between raw data and actionable insights, often making the difference between a mediocre and a high-performing model.</p>
<p>Feature engineering is closely related to data preprocessing, but they serve different purposes.</p>
<ul class="simple">
<li><p>Data processing (or data preprocessing) is about cleaning and preparing data — handling missing values, removing duplicates, correcting data types, and ensuring consistency. This step makes the data <strong>usable</strong>.</p></li>
<li><p>Feature engineering, on the other hand, comes after basic processing and focuses on improving the predictive power of dataset.</p></li>
<li><p>In essence, <strong>data preprocessing ensures data quality</strong>, while <strong>feature engineering enhances data value</strong> for ML models.</p></li>
<li><p>Both are essential steps in building effective and accurate predictive systems.</p></li>
</ul>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Data Preparation is a critical, foundational step in the machine learning workflow.</p></li>
<li><p>Introduction to the Palmer Penguins dataset, and demonstration of loading the dataset into a pandas DataFrame for analysis.</p></li>
<li><p>Statistical analysis and Visualization of the Penguins dataset using Pandas and Seaborn.</p></li>
<li><p>Identification and handling missing numerical values and outliers.</p></li>
<li><p>Convertion of categorical features into numerical formats.</p></li>
<li><p>Feature engineering is the process of creating, transforming, or selecting the right features from raw data to improve the performance of machine learning models.</p></li>
</ul>
</div>
</section>
</section>
<span id="document-05-supervised-ML-classification"></span><section id="supervised-learning-i-classification">
<h2>Supervised Learning (I): Classification<a class="headerlink" href="#supervised-learning-i-classification" title="Link to this heading"></a></h2>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Describe the basic concepts of classification tasks, including inputs (features), outputs (labels), and common algorithms.</p></li>
<li><p>Preprocess data in the Penguins dataset by handling missing values, managing outliers, and encoding categorical features.</p></li>
<li><p>Perform classification tasks using representative algorithms (<em>e.g.</em>, k-NN, Logistic Regression, Naive Bayes, Support Vector Machine, Decision Tree, Random Forest, Gradient Boosting, Multi-Layer Perceptron, and Neural Networks).</p></li>
<li><p>Evaluate model performance with metrics such as accuracy, precision, recall, F1-score, and confusion matrices.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>40 min teaching/demonstration</p></li>
<li><p>40 min exercises</p></li>
</ul>
</div>
<section id="classification">
<h3>Classification<a class="headerlink" href="#classification" title="Link to this heading"></a></h3>
<p>Classification is a supervised ML task in which a model predicts discrete class labels based on input features. It involves training the model on labeled data so that it can assign new and unseen data to predefined categories or classes by learning patterns from the training dataset.</p>
<p>In binary classification, the model predicts one of two classes, such as spam or not spam for emails. Multiclass classification extends this to multiple categories, like classifying images as cats, dogs, or birds.</p>
<p>Common algorithms for classification tasks include k-Nearest Neighbors (KNN), Logistic Regression, Naive Bayes, Support Vector Machine (SVM), Decision Trees, Random Forests, Gradient Boosting, and Neural Networks.</p>
<p>In this episode we will perform supervised classification to categorize penguins into three species — Adelie, Chinstrap, and Gentoo — based on their physical measurements (flipper length, body mass, <em>etc.</em>). We will build and train multiple classifier models, and then evaluate their performance using metrics such as accuracy, precision, recall, and F1 score.
By comparing the results, we aim to identify which model provides the most accurate and reliable classification for this task.</p>
</section>
<section id="data-preparation">
<h3>Data Preparation<a class="headerlink" href="#data-preparation" title="Link to this heading"></a></h3>
<p>In the previous episode, <a class="reference internal" href="#document-04-data-preparation-for-ML"><span class="std std-doc">Episode 4: Data Preparation for Machine Learning</span></a>, we discussed data preparation steps, including handling missing values, detecting outliers, and encoding categorical variables.</p>
<p>In this episode, we will revisit these steps, with particular emphasis on encoding categorical variables. For the classification task, we will treat the categorical variable <code class="docutils literal notranslate"><span class="pre">species</span></code> as the label (target variable) and use the remaining columns as features to predict the penguins species.
To achieve this, we transform the categorical features <code class="docutils literal notranslate"><span class="pre">island</span></code> and <code class="docutils literal notranslate"><span class="pre">sex</span></code>, as well as the <code class="docutils literal notranslate"><span class="pre">species</span></code> label, into numerical format (code examples are availalbe in the <a class="reference internal" href="#document-jupyter-notebooks/5-ML-Classifion"><span class="std std-doc">Jupyter Notebook</span></a>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>

<span class="n">penguins</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;penguins&#39;</span><span class="p">)</span>
<span class="n">penguins_classification</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># encode `species` column with 0=Adelie, 1=Chinstrap, and 2=Gentoo</span>
<span class="n">penguins_classification</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;species&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span>

<span class="c1"># encode `island` column with 0=Biscoe, 1=Dream and 2=Torgersen</span>
<span class="n">penguins_classification</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;island&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;island&#39;</span><span class="p">])</span>

<span class="c1"># encode `sex` column with 0=Female, and 1=Male</span>
<span class="n">penguins_classification</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;sex&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div class="admonition-discussion discussion important admonition" id="discussion-0">
<p class="admonition-title">Discussion</p>
<ul class="simple">
<li><p>why to use <code class="docutils literal notranslate"><span class="pre">species</span></code>?</p></li>
<li><p>why not to use the other categorical variables (<code class="docutils literal notranslate"><span class="pre">island</span></code> and <code class="docutils literal notranslate"><span class="pre">sex</span></code>)?</p></li>
</ul>
</div>
</section>
<section id="data-processing">
<h3>Data Processing<a class="headerlink" href="#data-processing" title="Link to this heading"></a></h3>
<p>In this episode, data processing will focus on two essential steps: <strong>data splitting</strong> and <strong>feature scaling</strong></p>
<section id="data-splitting">
<h4>Data splitting<a class="headerlink" href="#data-splitting" title="Link to this heading"></a></h4>
<p>Data splitting involves two important substeps: splitting into features and labels, and splitting into training and testing sets.</p>
<p>The first substep is to split the dataset into features and labels. Features (also called predictors or independent variables) are the input values used to make predictions, while labels (or target variables) represent the output the model is trying to predict.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">penguins_classification</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;species&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The second substep is to divide the Penguins dataset into training and testing sets. The training set is used to fit and train the models, allowing it to learn patterns and relationships from the data, and the testing set, on the other hand, is reserved for evaluating the model’s performance on unseen data.</p>
<p>A common split is 80% for training and 20% for testing, which provides enough data for training while still retaining a meaningful set for testing.
This step is typically performed using the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function from <code class="docutils literal notranslate"><span class="pre">sklearn.model_selection</span></code>, where setting a fixed <code class="docutils literal notranslate"><span class="pre">random_state</span></code> ensures reproducibility of the results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of examples for training is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="si">}</span><span class="s2"> and test is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="feature-scaling">
<h4>Feature scaling<a class="headerlink" href="#feature-scaling" title="Link to this heading"></a></h4>
<p>Feature scaling is to standardize or normalize the range of independent variables (features) in a dataset.
In many datasets, features can have different units or scales. For example, in the Penguins dataset, body mass is measured in grams and can range in the thousands, while flipper length is measured in millimeters and typically ranges in the hundreds. These differences in scale can unintentionally bias ML algorithms, making features with larger values dominate the learning process, leading to biased and inaccurate models.</p>
<p>Scaling transforms these features to a common, limited range, such as [0, 1] or a distribution with a mean of 0 and a standard deviation of 1, without distorting the differences in the ranges of values or losing information.
This is particularly important for algorithms that rely on distance calculations, such as k-Nearest Neighbors (k-NN), Support Vector Machines (SVM), and clustering methods. Similarly, gradient-based optimization methods (used in neural networks and logistic regression) converge faster and more reliably when input features are scaled.
Without scaling, the algorithm might oscillate inefficiently or struggle to find the optimal solution. Furthermore, it helps ensure that regularization penalties are applied uniformly across all coefficients, preventing the model from unfairly penalizing features with smaller natural ranges.</p>
<p>Two of the most common methods for feature scaling are <strong>Normalization</strong> (Min-Max Scaling) and <strong>Standardization</strong> (Z-score Normalization).</p>
<ul>
<li><p>Normalization (Min-Max Scaling)</p>
<ul>
<li><p>This technique rescales the features to a fixed range, typically [0, 1].</p></li>
<li><p>It is calculated by subtracting the minimum value of the feature and then dividing by the range (max - min), and its formula is</p>
<div class="math notranslate nohighlight">
\[X\_scaled = \frac{(X - X\_min)}{(X\_max - X\_min)}\]</div>
</li>
<li><p>This method is useful when the distribution is not Gaussian or when the algorithm requires input values bounded within a specific range (<em>e.g.</em>, neural networks often use activation functions that expect inputs in the [0,1] range).</p></li>
</ul>
</li>
<li><p>Standardization (Z-score Normalization)</p>
<ul>
<li><p>This technique transforms the data to have a mean of 0 and a standard deviation of 1.</p></li>
<li><p>It is calculated by subtracting the mean value (μ) of the feature and then dividing by the standard deviation (σ), and its formula is</p>
<div class="math notranslate nohighlight">
\[X\_scaled = \frac{X - \mu}{\sigma}.\]</div>
</li>
<li><p>Standardization is less affected by outliers than Min-Max scaling and is often the preferred choice for algorithms that assume data is centered (like SVM and PCA).</p></li>
</ul>
</li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-feature-scaling.png"><img alt="_images/5-feature-scaling.png" src="_images/5-feature-scaling.png" style="width: 95%;" />
</a>
</figure>
<p>In practice, these transformations are easily applied using libraries like scikit-learn with the <code class="docutils literal notranslate"><span class="pre">MinMaxScaler</span></code> and <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> classes, which efficiently learn the parameters (<code class="docutils literal notranslate"><span class="pre">mean</span></code>, <code class="docutils literal notranslate"><span class="pre">min</span></code>, <code class="docutils literal notranslate"><span class="pre">max</span></code>) from the training data and apply them consistently to avoid data leakage.</p>
<p>In this episode, we will apply feature standardization to both the training and testing sets. The implementation can be easily achieved using <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing</span></code>, as shown in the code below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="training-model-evaluating-model-performance">
<h3>Training Model &amp; Evaluating Model Performance<a class="headerlink" href="#training-model-evaluating-model-performance" title="Link to this heading"></a></h3>
<p>After preparing the Penguins dataset by handling missing values, encoding categorical variables, and splitting it into features/labels and training/testing sets, the next step is to apply classification algorithms.
In this episode, we will experiment with k-Nearest Neighbors (KNN), Naive Bayes, Decision Trees, Random Forests, and Neural Networks to predict penguins species based on their physical measurements. Each of these algorithms offers a distinct approach to pattern recognition and generalization. By applying them to the same prepared dataset, we can make a fair and meaningful comparison of their predictive performance.</p>
<p>The workflow for training and evaluating a classification model generally follows these steps:</p>
<ul class="simple">
<li><p>Choose a model class and import it, <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">sklearn.neighbors</span> <span class="pre">import</span> <span class="pre">XXX</span></code>.</p></li>
<li><p>Set model hyperparameters by instantiating the class with desired values, <code class="docutils literal notranslate"><span class="pre">xxx_model</span> <span class="pre">=</span> <span class="pre">XXX(&lt;...</span> <span class="pre">hyperparameters</span> <span class="pre">...&gt;)</span></code>.</p></li>
<li><p>Train the model on the preprocessed training data using the <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> method, <code class="docutils literal notranslate"><span class="pre">xxx_model.fit(X_train_scaled,</span> <span class="pre">y_train)</span></code>.</p></li>
<li><p>Make predictions on the testing data with the <code class="docutils literal notranslate"><span class="pre">.predict()</span></code> method, <code class="docutils literal notranslate"><span class="pre">y_pred_xxx</span> <span class="pre">=</span> <span class="pre">xxx_model.predict(X_test_scaled)</span></code>.</p></li>
<li><p>Evaluate model performance using appropriate metrics, <code class="docutils literal notranslate"><span class="pre">score_xxx</span> <span class="pre">=</span> <span class="pre">accuracy_score(y_test,</span> <span class="pre">y_pred_xxx)</span></code>.</p></li>
<li><p>Visualize the results, for example by plotting a confusion matrix or other diagnostic charts to better understand model performance.</p></li>
</ul>
<section id="k-nearest-neighbors-knn">
<h4>k-Nearest Neighbors (KNN)<a class="headerlink" href="#k-nearest-neighbors-knn" title="Link to this heading"></a></h4>
<p>One intuitive and widely used method for classification is the k-Nearest Neighbors (KNN) algorithm. KNN is a non-parametric, instance-based approach that predicts a sample’s label by considering the majority class of its <em>k</em> closest neighbors in the training set. Unlike many other algorithms, <strong>KNN does not require a traditional training phase</strong>; instead, <strong>it stores the entire dataset and performs the necessary computations at prediction step</strong>. This makes it a lazy learner — simple to implement but potentially expensive during inference, especially with large datasets.</p>
<p>Below is an example illustrating how KNN determines the class of a new query point. Given a query point, KNN first calculates the distance between this point and all points in the training set. It then identifies the <em>k</em> closest points, and the class that appears most frequently among these neighbors is assigned as the predicted label for the query point. The choice of <em>k</em> plays a crucial role in performance: a small <em>k</em> can make the model overly sensitive to noise, while a large <em>k</em> may oversmooth the decision boundaries and obscure important local patterns.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-knn-example.png"><img alt="_images/5-knn-example.png" src="_images/5-knn-example.png" style="width: 90%;" />
</a>
</figure>
<p>Let’s create a KNN model. Here, we set <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">=</span> <span class="pre">3</span></code>, meaning that the algorithm will consider the 3 nearest neighbors to determine the class of a data point. We then train the model on the training set using the <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neighbors</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="n">knn_model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">knn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>After fitting the model to the training dataset, we use the trained KNN model to predict the species on the testing set and evaluate its performance.</p>
<p>For classification tasks, metrics such as accuracy, precision, recall, and the F1-score provide a comprehensive assessment of model performance:</p>
<ul class="simple">
<li><p><strong>Accuracy</strong> measures the proportion of correctly classified instances across all species (Adelie, Chinstrap, Gentoo). It provides an overall sense of how often the model is correct but can be misleading when the dataset is imbalanced.</p></li>
<li><p><strong>Precision</strong> quantifies the proportion of correct positive predictions for each species, while <strong>recall</strong> measures the proportion of actual positives that are correctly identified.</p></li>
<li><p><strong>F1-score</strong> is the harmonic mean of precision and recall, offering a balanced metric for each class. It is particularly useful when dealing with imbalanced class distributions, as it accounts for both false positives and false negatives.</p></li>
</ul>
<div class="dropdown callout admonition" id="callout-0">
<p class="admonition-title">Relations among different matrics</p>
<p>In classification tasks, model predictions can be compared against the true labels to assess performance. This comparison is often summarized using four key concepts: True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN).</p>
<p>Suppose we focus on identifying Adelie penguins as the positive class.</p>
<ul class="simple">
<li><p>A True Positive occurs when the model correctly predicts a penguin as Adelie and it truly belongs to that species.</p></li>
<li><p>A True Negative happens when the model correctly identifies a penguin as not Adelie (<em>i.e.</em>, Chinstrap or Gentoo).</p></li>
<li><p>A False Positive arises when the model incorrectly predicts a penguin as Adelie when it is actually another species.</p></li>
<li><p>A False Negative occurs when an Adelie penguin is mistakenly predicted as Chinstrap or Gentoo.</p></li>
</ul>
<p>These four outcomes form the basis of performance metrics such as accuracy, precision, recall, and F1-score, which help evaluate how well the model distinguishes between species.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># predict on testing data</span>
<span class="n">y_pred_knn</span> <span class="o">=</span> <span class="n">knn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># evaluate model performance</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">score_knn</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for k-Nearest Neighbors:&quot;</span><span class="p">,</span> <span class="n">score_knn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">))</span>
</pre></div>
</div>
<p>In classification tasks, a <strong>confusion matrix</strong> is a powerful tool for evaluating model performance by comparing predicted labels with true labels. For a multiclass problem like the Penguins dataset, the confusion matrix is an <strong>N x N</strong> matrix, where N represents the number of target classes (here, <strong>N=3</strong> for the three penguins species).
Each cell <em>(i, j)</em> shows the number of instances where the true class was <em>i</em> and the model predicted class <em>j</em>. Diagonal elements correspond to correct predictions, while off-diagonal elements indicate misclassifications. This visualization provides an intuitive overview of how often the model predicts correctly and where it tends to make errors.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">fig_name</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;OrRd&#39;</span><span class="p">,</span>
                <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Adelie&quot;</span><span class="p">,</span> <span class="s2">&quot;Chinstrap&quot;</span><span class="p">,</span> <span class="s2">&quot;Gentoo&quot;</span><span class="p">],</span>
                <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Adelie&#39;</span><span class="p">,</span> <span class="s1">&#39;Chinstrap&#39;</span><span class="p">,</span> <span class="s1">&#39;Gentoo&#39;</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted Label&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;True Label&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fig_name</span><span class="p">)</span>


<span class="n">cm_knn</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">)</span>

<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_knn</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using KNN algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;5-confusion-matrix-knn.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/5-confusion-matrix-knn.png"><img alt="_images/5-confusion-matrix-knn.png" src="_images/5-confusion-matrix-knn.png" style="width: 75%;" />
</a>
<figcaption>
<p><span class="caption-text">The first row: there are 28 Adelie penguins in the test data, and all these penguins are identified as Adelie (valid). The second row: there are 20 Chinstrap pengunis in the test data, with 2 identified as Adelie (invalid), and 18 identified as Chinstrap (valid). The third row: there are 19 Gentoo penguins in the test data, and all these penguins are identified as Gentoo (valid).</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The choice of <code class="docutils literal notranslate"><span class="pre">k</span></code> can greatly affect the accuracy of KNN. Always try multiple <code class="docutils literal notranslate"><span class="pre">k</span></code> values and compare their performance. For the Penguins dataset, test different k values (<em>e.g.</em>, 3, 5, 7, 9, …) to find the optimal k that gives the best classification results (accuracy score).</p>
</div>
</section>
<section id="logistic-regression">
<h4>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Link to this heading"></a></h4>
<p><strong>Logistic Regression</strong> is a fundamental classification algorithm to predict categorical outcomes. Despite its name, logistic regression is not a regression algorithm but a classification method that predicts the <strong>probability</strong> of an instance belonging to a particular class.</p>
<p>For binary classification, it uses the logistic (<strong>sigmoid</strong>) function to map a linear combination of input features to a probability between 0 and 1, which is then thresholded (typically at 0.5) to assign a class.</p>
<p>For multiclass classification, logistic regression can be extended using approaches such as one-vs-rest (OvR) or softmax regression.</p>
<ul class="simple">
<li><p>In OvR, a separate binary classifier is trained for each species, treating that species as the positive class (blue area) and all other species as the negative class (red area).</p></li>
<li><p>Softmax regression generalizes the logistic function to compute probabilities across all classes simultaneously, assigning each instance to the class with the highest predicted probability.</p></li>
</ul>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="_images/5-logistic-regression-example.png"><img alt="_images/5-logistic-regression-example.png" src="_images/5-logistic-regression-example.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-text">(Upper left) the sigmoid function; (upper middle) the softmax regression process: three input features to the softmax regression model resulting in three output vectors where each contains the predicted probabilities for three possible classes; (upper right) a bar chart of softmax outputs in which each group of bars represents the predicted probability distribution over three classes; (lower subplots) three binary classifiers distinguish one class from the other two classes using the one-vs-rest approach.</span><a class="headerlink" href="#id2" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The process of creating a Logistic Regression model and fitting it to the training data is very similar to the approach used for the KNN model described earlier, with the main difference being the choice of classifier. A code example and the resulting confusion matrix plot are provided below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">lr_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">)</span>
<span class="n">lr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_lr</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_lr</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Logistic Regression:&quot;</span><span class="p">,</span> <span class="n">score_lr</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lr</span><span class="p">))</span>

<span class="n">cm_lr</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lr</span><span class="p">)</span>

<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_lr</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Logistic Regression algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;5-confusion-matrix-lr.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-confusion-matrix-lr.png"><img alt="_images/5-confusion-matrix-lr.png" src="_images/5-confusion-matrix-lr.png" style="width: 75%;" />
</a>
</figure>
</section>
<section id="naive-bayes">
<h4>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Link to this heading"></a></h4>
<p>The <strong>Naive Bayes</strong> algorithm is a simple yet powerful probabilistic classifier based on <a class="reference external" href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ Theorem</a>. It assumes that all features are and equally important — a condition that often does not hold in practice, which can introduce some bias. However, this independence assumption greatly simplifies computations by allowing conditional probabilities to be expressed as the product of individual feature probabilities. Given an input instance, the algorithm calculates the posterior probability for each class and assigns the instance to the class with the highest probability.</p>
<p>Logistic Regression and Naive Bayes are both popular algorithms for classification tasks, but they differ significantly in their approach, assumptions, and underlying mechanics. Below is an example comparing Logistic Regression and Naive Bayes decision boundaries on a synthetic dataset with two features. The visualization highlights their fundamental differences: <strong>Logistic Regression learns a linear decision boundary directly, whereas Naive Bayes models feature distributions for each class under the independence assumption</strong>.</p>
<div class="dropdown callout admonition" id="callout-1">
<p class="admonition-title"><strong>Logistic Regression</strong> <em>vs.</em> <strong>Naive Bayes</strong></p>
<ul class="simple">
<li><p>Logistic Regression is a <strong>discriminative</strong> model that directly estimates the probability of a data point belonging to a particular class by fitting a linear combination of features. In the context of the Penguins dataset, Logistic Regression uses features such as bill length and flipper length to compute a weighted sum, which is then transformed into probabilities for penguins species. The model assumes a linear relationship between the features and the log-odds of the classes and optimizes parameters using maximum likelihood estimation. This makes Logistic Regression sensitive to feature scaling and correlations. It is generally robust to noise and can tolerate moderately correlated features, but it may struggle with highly non-linear relationships unless additional feature engineering is applied.</p></li>
<li><p>Naive Bayes, by contrast, is a <strong>generative</strong> model that applies Bayes’ theorem to estimate the probability of a class given the input features, assuming conditional independence between features. For the Penguins dataset, it estimates the likelihood of features (<em>e.g.</em>, bill depth) for each species and combines these with prior probabilities to predict the most likely species. The “naive” independence assumption often does not hold in practice (<em>e.g.</em>, bill length and depth may be correlated), but it simplifies computation and allows Naive Bayes to be highly efficient, especially for high-dimensional data. It is less sensitive to irrelevant features and does not require feature scaling. However, it can underperform when feature dependencies are strong or when the data distribution deviates from the model’s assumptions (<em>e.g.</em>, Gaussian for continuous features in Gaussian Naive Bayes). Zero probabilities must be carefully handled, typically via smoothing techniques.</p></li>
</ul>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-naive-bayes-example.png"><img alt="_images/5-naive-bayes-example.png" src="_images/5-naive-bayes-example.png" style="width: 95%;" />
</a>
</figure>
<p>To apply Naive Bayes, we use <code class="docutils literal notranslate"><span class="pre">GaussianNB</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn.naive_bayes</span></code>, which assumes that the features follow a Gaussian (normal) distribution — making it suitable for continuous numerical data such as bill length and body mass.</p>
<ul class="simple">
<li><p>Because Naive Bayes relies on probabilities, <strong>feature scaling is not required</strong>; however, <strong>handling missing values and encoding categorical variables numerically remains necessary</strong>.</p></li>
<li><p>While Naive Bayes may not always match the performance of more complex models like Random Forests, it offers fast training, low memory requirements, and reliable performance for simpler classification tasks.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="n">nb_model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">nb_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_nb</span> <span class="o">=</span> <span class="n">nb_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_nb</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Naive Bayes:&quot;</span><span class="p">,</span> <span class="n">score_nb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">))</span>

<span class="n">cm_nb</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_nb</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Naive Bayes algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;4-confusion-matrix-nb.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-confusion-matrix-nb.png"><img alt="_images/5-confusion-matrix-nb.png" src="_images/5-confusion-matrix-nb.png" style="width: 75%;" />
</a>
</figure>
</section>
<section id="support-vector-machine-svm">
<h4>Support Vector Machine (SVM)<a class="headerlink" href="#support-vector-machine-svm" title="Link to this heading"></a></h4>
<p>Previously, we presented an example using a Logistic Regression classifier, which produces a linear decision boundary to separate two classes based on their features. It works by fitting this linear boundary using the logistic function, making it particularly effective when the data is linearly separable. A notable characteristic of Logistic Regression is that the decision boundary typically lies in the region where the predicted probabilities of the two classes are closest — essentially where the model is most uncertain.</p>
<p>However, when there is a large gap between two well-separated classes — as can occur when distinguishing cats from dogs based on weight and size — Logistic Regression faces an inherent limitation: an infinite number of possible solutions. The algorithm has no built-in mechanism to select a single “optimal” boundary when multiple valid linear separators exist within the wide margin between classes. As a result, it may place the decision boundary somewhere in that gap, creating a broad, undefined region with little or no supporting data. While this may not affect accuracy on clearly separated data, it can reduce the model’s robustness when new or noisy data points appear near that boundary.</p>
<p>Below is another example of separating cats from dogs based on ear length and weight. In addition to the linear decision boundary produced by the Logistic Regression classifier, we can identify three other linear boundaries that also achieve good separation between the two classes. The question then arises: which boundary is truly better, and how can we evaluate their performance on unseen data?</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-svm-example-large-gap.png"><img alt="_images/5-svm-example-large-gap.png" src="_images/5-svm-example-large-gap.png" style="width: 95%;" />
</a>
</figure>
<p>To better handle such situations, we can turn to the <strong>Support Vector Machine (SVM)</strong> algorithm. Unlike Logistic Regression, SVM focuses on maximizing the margin — the distance between the decision boundary and the closest data points from each class, known as support vectors (as illustrated in the figure below). When a large gap exists between two classes, SVM takes advantage of this space by positioning the boundary near the center of the gap while maintaining the maximum margin. This results in a more stable and robust classifier, especially when the classes are well-separated.</p>
<p>Unlike Logistic Regression, which considers all data points to estimate probabilities, SVM relies primarily on the most critical examples — those closest to the decision boundary — making it less sensitive to outliers and more precise in defining class separations.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="_images/5-svm-example-with-max-margin-separation.png"><img alt="_images/5-svm-example-with-max-margin-separation.png" src="_images/5-svm-example-with-max-margin-separation.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-text">The SVM classification boundary for distinguishing cats and dogs based on ear length and weight. The solid black line represents the maximum margin hyperplane (decision boundary), while the dashed green lines indicate the positive and negative hyperplanes that define the margin. The black circles highlight the support vectors — the critical data points that determine the width of the margin.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>To apply SVM, we use <code class="docutils literal notranslate"><span class="pre">SVC</span></code> (Support Vector Classification) from <code class="docutils literal notranslate"><span class="pre">sklearn.svm</span></code>. By default, it assumes a nonlinear relationship between features, modeled using the <code class="docutils literal notranslate"><span class="pre">rbf</span></code> (Radial Basis Function) kernel. This kernel enables the model to learn complex decision boundaries by implicitly mapping input features into a higher-dimensional space.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can also experiment with other kernels, such as <code class="docutils literal notranslate"><span class="pre">linear</span></code>, <code class="docutils literal notranslate"><span class="pre">poly</span></code>, or <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>, to explore different types of decision boundaries.</p>
</div>
<p>By adjusting hyperparameters such as <code class="docutils literal notranslate"><span class="pre">C</span></code> (regularization strength) and <code class="docutils literal notranslate"><span class="pre">gamma</span></code> (kernel coefficient), we can control the trade-off between margin width and classification accuracy. Below is a code example demonstrating how to apply <code class="docutils literal notranslate"><span class="pre">SVC</span></code> with the <code class="docutils literal notranslate"><span class="pre">rbf</span></code> kernel to classify penguins.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>

<span class="n">svm_model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">svm_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_svm</span> <span class="o">=</span> <span class="n">svm_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_svm</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Support Vector Machine:&quot;</span><span class="p">,</span> <span class="n">score_svm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm</span><span class="p">))</span>

<span class="n">cm_svm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm</span><span class="p">)</span>

<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_svm</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Support Vector Machine algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;5-confusion-matrix-svm.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-confusion-matrix-svm.png"><img alt="_images/5-confusion-matrix-svm.png" src="_images/5-confusion-matrix-svm.png" style="width: 75%;" />
</a>
</figure>
</section>
<section id="decision-tree">
<h4>Decision Tree<a class="headerlink" href="#decision-tree" title="Link to this heading"></a></h4>
<p>The <strong>Decision Tree</strong> algorithm is a versatile and highly interpretable method for classification tasks. Its core idea is to recursively split the dataset into smaller subsets based on feature thresholds, creating a tree-like structure of decisions that maximizes the separation of target classes.</p>
<p>For example, a decision tree can be used to classify cats and dogs based on two or three features, illustrating how the algorithm partitions the feature space to distinguish between classes.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="_images/5-decision-tree-example.png"><img alt="_images/5-decision-tree-example.png" src="_images/5-decision-tree-example.png" style="width: 95%;" />
</a>
<figcaption>
<p><span class="caption-text">(Upper): Decision boundary separating cats and dogs based on two features (ear length and weight), along with the corresponding decision tree structure.
(lower): Decision boundaries separating cats and dogs based on three features (ear length, weight, and tail length), and the corresponding decision tree structure.</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Below is a code example demonstrating the Decision Tree classifier applied to the penguins classification task.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">dt_model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">)</span>
<span class="n">dt_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_dt</span> <span class="o">=</span> <span class="n">dt_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_dt</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Decision Tree:&quot;</span><span class="p">,</span> <span class="n">score_dt</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">))</span>

<span class="n">cm_dt</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_dt</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Decision Tree algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;5-confusion-matrix-dt.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-confusion-matrix-dt.png"><img alt="_images/5-confusion-matrix-dt.png" src="_images/5-confusion-matrix-dt.png" style="width: 75%;" />
</a>
</figure>
<p>We visualize the Decision Tree structure to better understand how penguins are classified based on their physical characteristics.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">plot_tree</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">dt_model</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Decision Tree Structure for Penguins Species Classification&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-decision-tree-structure.png"><img alt="_images/5-decision-tree-structure.png" src="_images/5-decision-tree-structure.png" style="width: 100%;" />
</a>
</figure>
</section>
<section id="optional-random-forest">
<h4>(Optional) Random Forest<a class="headerlink" href="#optional-random-forest" title="Link to this heading"></a></h4>
<p>While Decision Trees are easy to interpret and visualize, they have some notable drawbacks. One primary issue is their tendency to overfit the training data, particularly when the tree is allowed to grow deep without constraints such as maximum depth or minimum samples per split. Overfitting causes the model to capture noise in the training data, which can lead to poor generalization on unseen data — for example, misclassifying a Gentoo penguin as a Chinstrap due to overly specific splits. Additionally, decision trees are sensitive to small variations in the data; even slight changes, such as a few noisy measurements, can result in a significantly different tree structure, reducing the model’s stability and reliability.</p>
<p>To address these limitations, we can use an ensemble learning technique called <strong>Random Forest</strong>. A Random Forest builds on the concept of decision trees by creating a large collection of them, each trained on a randomly selected subset of the data and features. By aggregating the predictions of multiple trees — typically through majority voting for classification — Random Forest reduces overfitting, improves generalization, and mitigates the inherent instability in individual decision trees.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Ensemble learning</strong> is a ML approach that combines multiple individual models (often called base learners) to create a stronger, more accurate, and more robust overall model. The idea is that by aggregating the predictions of several models, the ensemble can reduce errors, improve generalization, and mitigate weaknesses of individual models. There are two main types of ensemble learning techniques:</p>
<ul class="simple">
<li><p><strong>Bagging</strong> (<strong>Bootstrap Aggregating</strong>): Multiple models are trained independently on random subsets of the data, and their predictions are averaged (for regression) or voted on (for classification). Random Forest is a classic example of bagging applied to decision trees.</p></li>
<li><p><strong>Boosting</strong>: Models are trained sequentially, with each new model focusing on the errors made by previous models. Examples include AdaBoost, Gradient Boosting, and XGBoost.</p></li>
</ul>
</div>
<p>The figure below illustrates how a Random Forest improves upon a single Decision Tree when classifying cats and dogs based on synthetic measurements of ear length and weight.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="_images/5-random-forest-example.png"><img alt="_images/5-random-forest-example.png" src="_images/5-random-forest-example.png" style="width: 100%;" />
</a>
<figcaption>
<p><span class="caption-text">Top row shows the classification boundaries for both models. On the left, a single Decision Tree creates rigid, rectangular decision regions that precisely follow axis-aligned splits in the training data. While this achieves a good separation of the training samples, the jagged boundaries suggest potential overfitting to noise. In contrast, the Random Forest (right) produces smoother, more nuanced decision boundaries through majority voting across 100 trees. The blended purple transition zones represent areas where individual trees disagree, demonstrating how the ensemble averages out erratic predictions from any single tree. Bottom row reveals why Random Forests are more robust by examining three constituent trees. Tree #1 prioritizes ear length for its initial split, Tree #2 begins with weight, and Tree #3 uses a completely different weight threshold.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Below is a code example demonstrating the application of the Random Forest classifier to the penguins classification task.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">rf_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_rf</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_rf</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_rf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Random Forest:&quot;</span><span class="p">,</span> <span class="n">score_rf</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_rf</span><span class="p">))</span>

<span class="n">cm_rf</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_rf</span><span class="p">)</span>

<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_rf</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Random Forest algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;5-confusion-matrix-rf.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-confusion-matrix-rf.png"><img alt="_images/5-confusion-matrix-rf.png" src="_images/5-confusion-matrix-rf.png" style="width: 75%;" />
</a>
</figure>
<p>In addition to the confusion matrix, feature importance in a Random Forest (and also in a Decision Tree) model provides valuable insight into which input features contribute most to the model’s predictions. Random Forest calculates feature importance by measuring how much each feature reduces impurity — such as Gini impurity or entropy — when used to split the data across all trees in the forest. Features that produce greater reductions in impurity are considered more important. These importance scores are then normalized to provide a relative ranking, helping to identify which features most strongly influence the model’s predictions. This information is particularly useful for interpreting model behavior, selecting meaningful features, and understanding the underlying structure of the data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In Decision Tree and Random Forest, impurity measures how “mixed” the classes are in a given node. A pure node contains only instances of a single class, while an impure node contains a mixture of classes. Impurity metrics help the tree decide which feature and threshold to use when splitting the data to create nodes that are as pure as possible.</p>
<p>Gini impurity and entropy are metrics used to measure impurity of a dataset or a node.</p>
<p>During training, the algorithm evaluates all possible splits for a feature. It chooses the split that maximizes purity, <em>i.e.</em>, <strong>minimizes Gini impurity</strong> or maximizes information gain (<strong>reduction in entropy</strong>).</p>
</div>
<p>The greater the total reduction in impurity attributed to a feature, the more important it is considered. These importance scores are then normalized to provide a relative ranking, helping identify which features have the most influence on predicting the output class. This information is particularly useful for interpreting model behavior, selecting meaningful features, and understanding the underlying structure of the data.</p>
<p>Below is a code example showing how to plot feature importance using a Random Forest model to classify penguins into three categories.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">importances</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">importances</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:orange&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature Importance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Features&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Random Forest Feature Importance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="_images/5-random-forest-feature-importrance.png"><img alt="_images/5-random-forest-feature-importrance.png" src="_images/5-random-forest-feature-importrance.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-text">Illustration of feature importance for penguin classification. Longer bars indicate features with greater influence on the model’s decisions, showing that the Random Forest relies more heavily on these measurements to identify species.</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="optional-gradient-boosting">
<h4>(Optional) Gradient Boosting<a class="headerlink" href="#optional-gradient-boosting" title="Link to this heading"></a></h4>
<p>We have trained the model using a Decision Tree classifier, providing an intuitive starting point for classifying penguin species based on physical measurements. However, this classifier is sensitive to small fluctuations in the dataset, which can often lead to overfitting, especially when the tree grows deep.</p>
<p>To address the limitations of a single decision tree, we turned to Random Forest, an ensemble method that builds multiple decision trees on different random subsets of the data and features. By averaging the predictions of all trees or taking a majority vote in classification, Random Forest reduces overfitting and improves generalization. This approach balances model complexity with predictive performance and provides a reliable estimate of feature importance, helping identify which physical attributes are most influential in distinguishing penguin species.</p>
<p>While Random Forest provides robustness and improved accuracy over individual trees, we can further enhance performance using <strong>Gradient Boosting</strong>.</p>
<ul class="simple">
<li><p>Like Random Forest, Gradient Boosting is an ensemble learning technique, but it builds a strong classifier by combining many weak learners (typically shallow decision trees) in a sequential manner.</p></li>
<li><p>Unlike Random Forest, which grows multiple trees independently and in parallel using random subsets of the training data, Gradient Boosting constructs trees one at a time, with each new tree trained to correct the errors of its predecessors.</p></li>
</ul>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="_images/5-random-forest-vs-gradient-boosting.png"><img alt="_images/5-random-forest-vs-gradient-boosting.png" src="_images/5-random-forest-vs-gradient-boosting.png" style="width: 80%;" />
</a>
<figcaption>
<p><span class="caption-text">Iillustration of the <a class="reference external" href="https://medium.com/&#64;mrmaster907/introduction-random-forest-classification-by-example-6983d95c7b91">Random Forest</a> and <a class="reference external" href="https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-021-01701-9">Gradient Boosting</a> algorithms.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>In this code example below, we apply Gradient Boosting algorithm to classify penguin species. We use <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> from scikit-learn due to its simplicity and strong baseline performance.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="n">gb_model</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">gb_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_gb</span> <span class="o">=</span> <span class="n">gb_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_gb</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_gb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Gradient Boosting:&quot;</span><span class="p">,</span> <span class="n">score_gb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_gb</span><span class="p">))</span>

<span class="n">cm_gb</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_gb</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_gb</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Gradient Boosting algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;5-confusion-matrix-gb.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-confusion-matrix-gb.png"><img alt="_images/5-confusion-matrix-gb.png" src="_images/5-confusion-matrix-gb.png" style="width: 75%;" />
</a>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This progression — from the simplicity of a single Decision Tree, to the robustness of Random Forest, and finally to the precision of Gradient Boosting — mirrors the evolution of <strong>tree-based methods</strong> in modern ML. While Random Forest remains excellent for baseline performance, Gradient Boosting often achieves state-of-the-art results on structured data, such as ecological measurements, provided the learning rate and tree depth are carefully tuned.</p>
</div>
</section>
<section id="multi-layer-perceptron">
<h4>Multi-Layer Perceptron<a class="headerlink" href="#multi-layer-perceptron" title="Link to this heading"></a></h4>
<p>A <strong>Multilayer Perceptron</strong> (MLP) is a type of artificial neural network consisting of multiple layers of interconnected perceptrons (or neurons) designed to mimic certain aspects of human brain function. Each neuron (illustrated in the figure below) has the following characteristics:</p>
<ul class="simple">
<li><p>Input: one or more inputs (<code class="docutils literal notranslate"><span class="pre">x_1</span></code>, <code class="docutils literal notranslate"><span class="pre">x_2</span></code>, …), <em>e.g.</em>, features from the input data expressed as floating-point numbers.</p></li>
<li><p>Operations: Typically, each neuron conducts three main operations:</p>
<ul>
<li><p>Compute the weighted sum of the inputs where (<code class="docutils literal notranslate"><span class="pre">w_1</span></code>, <code class="docutils literal notranslate"><span class="pre">w_2</span></code>, …) are the corresponding weights.</p></li>
<li><p>Add a bias term to the weighted sum.</p></li>
<li><p>Apply an activation function to the result.</p></li>
</ul>
</li>
<li><p>Output: The neuron produces a single output value.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-neuron-activation-function.png"><img alt="_images/5-neuron-activation-function.png" src="_images/5-neuron-activation-function.png" style="width: 80%;" />
</a>
</figure>
<p>A common equation for the output of a neuron is</p>
<div class="math notranslate nohighlight">
\[output = Activation(\sum_i (x_i * w_i) + bias).\]</div>
<p>An <strong>activation function</strong> is a mathematical transformation that converts the weighted sum of a neuron’s inputs into its output signal. By introducing non-linearity into the network, activation functions enable neural networks to learn complex patterns and make sophisticated decisions based on the weighted inputs.</p>
<p>Below are some commonly used activation functions in neural networks and DL models. Each plays a crucial role in introducing non-linearities, allowing the network to capture intricate patterns and relationships in data.</p>
<ul class="simple">
<li><p><strong>Sigmoid</strong>: With its characteristic S-shaped curve, the sigmoid function maps inputs to a smooth 0-1 range, making it historically popular for binary classification tasks.</p></li>
<li><p><strong>Hyperbolic tangent</strong> (tanh): Similar to sigmoid but ranging from -1 to 1, tanh often provides stronger gradients during training.</p></li>
<li><p><strong>Rectified Linear Unit</strong> (ReLU): Outputs zero for negative inputs and the identity for positive inputs. ReLU has become the default choice for many architectures due to its computational efficiency and its ability to mitigate the vanishing gradient problem.</p></li>
<li><p><strong>Linear</strong>: This identity function serves as a reference, showing network behavior without any non-linear transformation.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-activation-function.png"><img alt="_images/5-activation-function.png" src="_images/5-activation-function.png" style="width: 80%;" />
</a>
</figure>
<p>A single neuron (perceptron) can learn simple patterns but is limited in modeling complex relationships. By combining multiple neurons into layers and connecting them into a network, we create a powerful computational framework capable of approximating highly non-linear functions. In a Multilayer Perceptron (MLP), neurons are organized into an input layer, one or more hidden layers, and an output layer.</p>
<p>The image below illustrates a three-layer perceptron network with 3, 4, and 2 neurons in the input, hidden, and output layers, respectively.</p>
<ul class="simple">
<li><p>The input layer receives raw data, such as pixel values or measurements, and passes it to the hidden layer.</p></li>
<li><p>The hidden layer contains multiple neurons that process the information and progressively extract higher-level features. Each neuron in the hidden layer is fully connected to neurons in adjacent layers, forming a dense network of weighted connections.</p></li>
<li><p>The output layer produces the network’s predictions, whether it’s a classification, regression output, or some other task.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-mlp-network.png"><img alt="_images/5-mlp-network.png" src="_images/5-mlp-network.png" style="width: 80%;" />
</a>
</figure>
<p>In the penguin classification task, we build a three-layer perceptron using scikit-learn’s <code class="docutils literal notranslate"><span class="pre">MLPClassifier</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn.neural_network</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neural_network</span><span class="w"> </span><span class="kn">import</span> <span class="n">MLPClassifier</span>

<span class="n">mlp_model</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                   <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span>
                   <span class="n">learning_rate_init</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                   <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">mlp_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>The model is configured with:</p>
<ul class="simple">
<li><p>an input layer matching the number of features (6 per penguin),</p></li>
<li><p>a hidden layer (<em>e.g.</em>, 16 neurons) to capture non-linear relationships, and</p></li>
<li><p>an output layer with three nodes (one per penguin class), using <code class="docutils literal notranslate"><span class="pre">relu</span></code> activation for the hidden layer.</p></li>
</ul>
<p>The hyperparameters used to construct this MLP are listed below:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">adam</span></code>, the optimization algorithm used to update weight parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">alpha</span></code>, the L2 regularization term (penalty). Setting this to 0 disables regularization, meaning the model won’t penalize large weights. This may cause overfitting if the dataset is small or noisy.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, the number of samples per mini-batch during training. Smaller batches lead to more frequent updates (finer learning) but can increase noise and training time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>, specifies the learning rate schedule. “constant” means that the learning rate keeps fixed throughout training. Other options like “invscaling” or “adaptive” would adjust the learning rate during training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate_init=0.001</span></code>, the initial learning rate (fixed here). A smaller value means slower learning, which may require more iterations but offers more stability.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_iter</span></code>, the maximum number of training iterations (epochs).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_state=123</span></code>, controls the random number generation for weight initialization and data shuffling, ensuring reproducible results.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_iter_no_change=10</span></code>, if the validation score does not improve for 10 consecutive iterations, training will stop early. This is a form of early stopping to prevent overfitting or unnecessary computation.</p></li>
</ul>
<p>After training the model, we evaluate its accuracy on the testing set and visualize the results by computing and plotting the confusion matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred_mlp</span> <span class="o">=</span> <span class="n">mlp_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_mlp</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Neural Network:&quot;</span><span class="p">,</span> <span class="n">score_mlp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlp</span><span class="p">))</span>

<span class="n">cm_mlp</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlp</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_mlp</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Multi-Layer Perceptron algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;5-confusion-matrix-mlp.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-confusion-matrix-mlp.png"><img alt="_images/5-confusion-matrix-mlp.png" src="_images/5-confusion-matrix-mlp.png" style="width: 75%;" />
</a>
</figure>
</section>
<section id="optional-deep-neural-networks">
<h4>(Optional) Deep Neural Networks<a class="headerlink" href="#optional-deep-neural-networks" title="Link to this heading"></a></h4>
<p>MLP is a foundational neural network architecture, consisting of an input layer, one or more hidden layers, and an output layer. While MLP excels at learning complex patterns from tabular data, its shallow depth (typically 1-2 hidden layers) limits its ability to handle very high-dimensional or abstract data such as raw images, audio, or text.</p>
<p>To overcome these limitations, Deep Neural Network (DNN) extends the MLP framework by adding multiple hidden layers. These additional layers allow the model to learn highly abstract features through deep hierarchical representations: early layers might capture basic features (like edges or shapes), while deeper layers recognize complex objects or semantic patterns. This depth enables DNN to outperform traditional MLP in complex tasks requiring high-level feature extraction, such as computer vision and natural language processing.</p>
<div class="admonition-dnn-architectures callout admonition" id="callout-2">
<p class="admonition-title">DNN architectures</p>
<p>DNNs have specialized architectures designed to handle different types of data (<em>e.g.</em>, spatial, temporal, and sequential data) and tasks more effectively.</p>
<ul class="simple">
<li><p>A standard feedforward deep neural network consists of stacked fully connected layers</p></li>
<li><p><strong>Convolutional neural networks</strong> (CNNs) are particularly well-suited for image data. They use convolutional layers to automatically extract local features like edges, textures, and shapes, significantly reducing the number of parameters and improving generalization on visual tasks.</p></li>
<li><p><strong>Recurrent neural network</strong> (RNN) is designed for sequential data such as time series, speech, or natural language. RNNs include loops that allow information to persist across time steps, enabling the model to learn dependencies over sequences. More advanced versions, like Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), address the limitations of basic RNNs by managing long-term dependencies more effectively.</p></li>
<li><p>In addition to CNNs and RNNs, the <strong>Transformer</strong> architecture has emerged as the state-of-the-art in many language and vision tasks. Transformers rely entirely on attention mechanisms rather than recurrence or convolutions, enabling them to model global relationships in data more efficiently. This flexibility has made them the foundation of powerful models like BERT, GPT, and Vision Transformers (ViTs). These specialized DL architectures illustrate how tailoring the network design to the structure of the data can lead to significant performance gains and more efficient learning.</p></li>
</ul>
</div>
<p>Here, we use the Keras API to construct a small DNN and apply it to the penguin classification task, demonstrating how even a compact architecture can effectively distinguish between penguin species (Adelie, Chinstrap, and Gentoo).</p>
<p>In this example, we exclude the categorical features <code class="docutils literal notranslate"><span class="pre">island</span></code> and <code class="docutils literal notranslate"><span class="pre">sex</span></code> from both the training and testing datasets. The target label <code class="docutils literal notranslate"><span class="pre">species</span></code> is then encoded using the <code class="docutils literal notranslate"><span class="pre">pd.get_dummies()</span></code> function in Pandas. Afterward, we split the data into training and testing sets and standardize the feature values to ensure consistent scaling during model training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="kn">import</span> <span class="n">keras</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">penguins_classification</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;species&#39;</span><span class="p">,</span><span class="s1">&#39;island&#39;</span><span class="p">,</span> <span class="s1">&#39;sex&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Adelie&#39;</span><span class="p">,</span> <span class="s1">&#39;Chinstrap&#39;</span><span class="p">,</span> <span class="s1">&#39;Gentoo&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of examples for training is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="si">}</span><span class="s2"> and test is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>When building a DNN with Keras, there are two common approaches: using the <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> API step by step, or defining all layers at once within the <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> constructor. Here, we adopt the first approach, whereas the second approach is used in the Jupyter notebook to construct the same DNN.</p>
<ul class="simple">
<li><p>We start by creating an empty model with <code class="docutils literal notranslate"><span class="pre">keras.Sequential()</span></code>, which initializes a linear container for stacking sequential layers.</p></li>
<li><p>Next, we define each layer separately using the <code class="docutils literal notranslate"><span class="pre">Dense</span></code> class, specifying the number of neurons and activation function for each layer.</p></li>
<li><p>Finally, we add all the layers using <code class="docutils literal notranslate"><span class="pre">keras.Model()</span></code> to the sequential container, resulting in a trainable model.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>

<span class="n">dnn_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">input_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span> <span class="c1"># 4 input features</span>

<span class="n">hidden_layer1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
<span class="n">hidden_layer1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">hidden_layer1</span><span class="p">)</span>

<span class="n">hidden_layer2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">hidden_layer1</span><span class="p">)</span>
<span class="c1">#hidden_layer2 = Dropout(0.0)(hidden_layer2)</span>

<span class="n">hidden_layer3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">hidden_layer2</span><span class="p">)</span>

<span class="n">output_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)(</span><span class="n">hidden_layer3</span><span class="p">)</span> <span class="c1"># 3 classes</span>

<span class="n">dnn_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">keras.layers.Dropout()</span></code> is a regularization technique in Keras used to reduce overfitting by randomly setting a fraction of input units to zero during training. For example, <code class="docutils literal notranslate"><span class="pre">Dropout(0.2)</span></code> means that 20% of the outputs of a specific layer will be randomly set to zero in each training step.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-dnn-network-dropout.png"><img alt="_images/5-dnn-network-dropout.png" src="_images/5-dnn-network-dropout.png" style="width: 80%;" />
</a>
</figure>
<p>We can use <code class="docutils literal notranslate"><span class="pre">dnn_model.summary()</span></code> to print a concise summary of a DNN’s architecture. It provides provides an overview of the model’s layers, their output shapes, and the number of trainable parameters, making it easier to understand and debug the network.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-dnn-summary.png"><img alt="_images/5-dnn-summary.png" src="_images/5-dnn-summary.png" style="width: 80%;" />
</a>
</figure>
<p>Now that we have designed a DNN that, in theory, should be capable of classifying penguins, we need to specify two critical components before training: (1) a loss function to quantify prediction errors, and (2) an optimizer to adjust the model’s weights during training.</p>
<ul class="simple">
<li><p><strong>Loss function</strong>: For multi-class classification, we select categorical cross-entropy, which penalizes incorrect probabilistic predictions. In Keras, this is implemented via the <code class="docutils literal notranslate"><span class="pre">keras.losses.CategoricalCrossentropy</span></code> class. This loss function works naturally with the <code class="docutils literal notranslate"><span class="pre">softmax</span></code> activation function we applied in the output layer. For a full list of available loss functions in Keras, see the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses%3E">documentation</a>.</p></li>
<li><p><strong>Optimizer</strong>: The optimizer determines how efficiently the model converges during training. Keras provides many options, each with its advantages, but here we use the widely adopted <code class="docutils literal notranslate"><span class="pre">Adam</span></code> (adaptive moment estimation) optimizer. Adam has several parameters, and the default values generally perform well, so we will use it with its defaults.</p></li>
</ul>
<p>We use <code class="docutils literal notranslate"><span class="pre">model.compile()</span></code> to combine the chosen loss function and optimier before starting training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">keras.optimizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Adam</span>

<span class="n">dnn_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">CategoricalCrossentropy</span><span class="p">())</span>
</pre></div>
</div>
<p>Now we are ready to train the DNN model. Here, we vary only the number of <code class="docutils literal notranslate"><span class="pre">epochs</span></code>. One training epoch means that every sample in the training data has been shown to the neural network once and used to update its parameters. During training, we set <code class="docutils literal notranslate"><span class="pre">batch_size=16</span></code> to balance memory efficiency with gradient stability, and <code class="docutils literal notranslate"><span class="pre">verbose=1</span></code> to display a progress bar showing the loss and metrics for each epoch in real time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">dnn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> method returns a history object, which contains a history attribute holding the training loss and other metrics for each epoch. Plotting the training loss can provide valuable insight into how learning progresses. For example, we can use Seaborn to plot the training loss with epochs <code class="docutils literal notranslate"><span class="pre">sns.lineplot(x=history.epoch,</span> <span class="pre">y=history.history['loss'],</span> <span class="pre">c=&quot;tab:orange&quot;,</span> <span class="pre">label='Training</span> <span class="pre">Loss')</span></code>.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-dnn-loss.png"><img alt="_images/5-dnn-loss.png" src="_images/5-dnn-loss.png" style="width: 80%;" />
</a>
</figure>
<p>Finally, we evaluate the model’s performance on the testing set by computing its accuracy and visualizing the results with a confusion matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># predict class probabilities</span>
<span class="n">y_pred_dnn_probs</span> <span class="o">=</span> <span class="n">dnn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># convert probabilities to class labels</span>
<span class="n">y_pred_dnn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred_dnn_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">score_dnn</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred_dnn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Deep Neutron Network:&quot;</span><span class="p">,</span> <span class="n">score_dnn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred_dnn</span><span class="p">))</span>

<span class="n">cm_dnn</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred_dnn</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_dnn</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using DNN algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;5-confusion-matrix-dnn.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-confusion-matrix-dnn.png"><img alt="_images/5-confusion-matrix-dnn.png" src="_images/5-confusion-matrix-dnn.png" style="width: 75%;" />
</a>
</figure>
</section>
</section>
<section id="comparison-of-trained-models">
<h3>Comparison of Trained Models<a class="headerlink" href="#comparison-of-trained-models" title="Link to this heading"></a></h3>
<p>To evaluate the performance of different algorithms in classifying penguin species, we compare their accuracy scores and confusion matrices. The algorithms we the adopted in the previous sections include:</p>
<ul class="simple">
<li><p>Instance-based: k-Nearest Neighbors (KNN).</p></li>
<li><p>Probability-based: Logistic Regression, and Naive Bayes.</p></li>
<li><p>Hyperplane-based: Support Vector Machine (SVM).</p></li>
<li><p>Tree-based methods: Decision Tree, Random Forest, and Gradient Boosting.</p></li>
<li><p>Network-based models: Multi-Layer Perceptron (MLP) and Deep Neural Networks (DNN).</p></li>
</ul>
<p>Each model was trained on the same training set and evaluated on a common testing set, with consistent preprocessing applied across all methods.</p>
<p>Performance under current training settings:</p>
<ul class="simple">
<li><p>MLP achieved the highest accuracy, demonstrating its effectiveness in capturing complex patterns and feature interactions in the Penguins dataset.</p></li>
<li><p>Naive Bayes showed slightly lower accuracy, likely due to its strong independence assumption between features, which does not fully hold in this dataset.</p></li>
<li><p>The other algorithms provided moderate performance.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-scores-for-all-models.png"><img alt="_images/5-scores-for-all-models.png" src="_images/5-scores-for-all-models.png" style="width: 80%;" />
</a>
</figure>
<p>The confusion matrices provided deeper insight into class-level prediction performance:</p>
<ul class="simple">
<li><p>MLP demonstrated well-balanced performance across all three penguin species.</p></li>
<li><p>Naive Bayes, in contrast, confused Adelie and Chinstrap penguins, likely due to overlapping feature distributions between these species.</p></li>
<li><p>other algorithms had a limited number of misclassifications, primarily between Adelie and Chinstrap.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-compare-confusion-matrices.png"><img alt="_images/5-compare-confusion-matrices.png" src="_images/5-compare-confusion-matrices.png" style="width: 100%;" />
</a>
</figure>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference external" href="https://enccs.github.io/deep-learning-intro/">Introduction to Deep Learning</a></p></li>
</ul>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Provided a fundamental introducton to classification tasks, covering basic concepts.</p></li>
<li><p>Demonstrated essential steps for data preparation and processing using the Penguins dataset.</p></li>
<li><p>Applied a range of classification algorithms — instance-based, probability-based, margin-based, tree-based, and neural network-based — to classify penguin species.</p></li>
<li><p>Evaluated and compared model performance using metrics such as accuracy scores and confusion matrices.</p></li>
</ul>
</div>
</section>
</section>
<span id="document-06-supervised-ML-regression"></span><section id="supervised-learning-ii-regression">
<h2>Supervised Learning (II): Regression<a class="headerlink" href="#supervised-learning-ii-regression" title="Link to this heading"></a></h2>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Understand the fundamental concept of regression (overfitting, cross-validation, gradient search, …)</p></li>
<li><p>Distinguish between types of regression: simple vs. multiple regression, linear vs. non-linear regression, and other specialized regression methods.</p></li>
<li><p>Perform regression tasks using representative algorithms (<em>e.g.</em>, k-NN, Linear Regression, Polynomial Regression, Support Vector Regression, Decision Tree, and Multi-Layer Perceptron)</p></li>
<li><p>Evaluate model performance with metrics such as Root Mean Squared Error (RMSE) and the R-squared (R²) score, and visualize predictive curves.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>40 min teaching/demonstration</p></li>
<li><p>40 min exercises</p></li>
</ul>
</div>
<section id="regression">
<h3>Regression<a class="headerlink" href="#regression" title="Link to this heading"></a></h3>
<p>Regression is a type of supervised machine learning task where the goal is to predict a continuous numerical value based on input features. Unlike classification, which assigns outputs to discrete categories, regression models produce real-valued predictions.</p>
<p>Although the Penguins dataset is most commonly used for classification tasks, it can also be applied to regression problems by choosing a continuous target variable. From the pairplot, we can observe a strong visual relationship between body mass and flipper length, indicating a clear positive correlation. Consequently, we select these two features for the regression task, aiming to estimate body mass based on flipper length.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-penguins-pairplot.png"><img alt="_images/4-penguins-pairplot.png" src="_images/4-penguins-pairplot.png" style="width: 100%;" />
</a>
</figure>
<p>Depending on the model construction approach, in this episode we explore a variety of regression algorithms to predict penguin body mass based on flipper length. These models are selected to represent different categories of machine learning approaches, ranging from simple, interpretable methods to more complex, flexible ones.</p>
<ul class="simple">
<li><p><strong>KNN Regression</strong>: Predictions are made based on the average of the closest training samples. This non-parametric, instance-based model captures local patterns in the data effectively.</p></li>
<li><p><strong>Linear Models</strong>: Standard Linear Regression and Regularized Regression assume a straight-line relationship between flipper length and body mass. These models are interpretable and efficient, providing a solid baseline for comparison.</p></li>
<li><p><strong>Non-linear Models</strong>: To account for possible non-linear trends, we include Polynomial Regression with higher-degree terms and Support Vector Regression (SVR) with <code class="docutils literal notranslate"><span class="pre">rbf</span></code> kernels, which can model more complex relationships.</p></li>
<li><p><strong>Tree-based Models</strong>: Decision Trees, Random Forests, and Gradient Boosting offer robust alternatives by recursively partitioning the feature space or combining ensembles to improve accuracy and handle non-linearities effectively.</p></li>
<li><p><strong>Neural Networks</strong>: These serve as universal function approximators, capable of learning intricate patterns in the data, but typically require larger datasets and more computational resources.</p></li>
</ul>
<p>Each model’s performance is rigorously assessed using cross-validated metrics such as Root Mean Squared Error (RMSE) and R². The resulting predictive curves illustrate how well each model captures the biological relationship between flipper length and body mass.</p>
</section>
<section id="data-preparation">
<h3>Data Preparation<a class="headerlink" href="#data-preparation" title="Link to this heading"></a></h3>
<p>Similar to the procedures adopted in previous episodes, we follow the same preprocessing steps for the Penguins dataset, including handling missing values and detecting outliers. For the regression task, categorical features are not needed, so encoding them is unnecessary.</p>
</section>
<section id="data-processing">
<h3>Data Processing<a class="headerlink" href="#data-processing" title="Link to this heading"></a></h3>
<p>Below is the code script to extract <code class="docutils literal notranslate"><span class="pre">flipper_length_mm</span></code> and <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> features from the main dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">penguins_regression</span><span class="p">[[</span><span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins_regression</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
<p>In this episode, we first perform feature scaling, followed by splitting the data into training and testing sets. The <code class="docutils literal notranslate"><span class="pre">inverse_transform()</span></code> method reverts transformed data back to its original scale or format.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
   
<span class="c1"># standardize feature and target</span>
<span class="n">scaler_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler_y</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler_X</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_scaled</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_train_scaled</span><span class="p">,</span> <span class="n">y_test_scaled</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y_scaled</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="n">X_train_orig</span> <span class="o">=</span> <span class="n">scaler_X</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">y_train_orig</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_train_scaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="o">...</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/6-spliting-training-testing-dataset.png"><img alt="_images/6-spliting-training-testing-dataset.png" src="_images/6-spliting-training-testing-dataset.png" style="width: 80%;" />
</a>
</figure>
</section>
<section id="training-model-evaluating-model-performance">
<h3>Training Model &amp; Evaluating Model Performance<a class="headerlink" href="#training-model-evaluating-model-performance" title="Link to this heading"></a></h3>
<section id="k-nearest-neighbors-knn">
<h4>k-Nearest Neighbors (KNN)<a class="headerlink" href="#k-nearest-neighbors-knn" title="Link to this heading"></a></h4>
<p>We begin by applying the KNN algorithm to the penguin regression task, as illustrated in the code example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neighbors</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNeighborsRegressor</span>

<span class="n">knn_model</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">knn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train_scaled</span><span class="p">)</span>

<span class="c1"># predict on test data</span>
<span class="n">y_pred_knn_scaled</span> <span class="o">=</span> <span class="n">knn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_knn</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_knn_scaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
</pre></div>
</div>
<p>For the regression task, we use Root Mean Squared Error (RMSE) and R² score as evaluation metrics.</p>
<ul class="simple">
<li><p>RMSE measures the average magnitude of prediction errors, providing insight into how closely the model’s predictions match the actual values</p></li>
<li><p>R² score indicates the proportion of variance in the target variable that is explained by the model, reflecting its overall goodness of fit.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># evaluate model performance</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>

<span class="n">rmse_knn</span> <span class="o">=</span> <span class="n">root_mean_squared_error</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">)</span>
<span class="n">r2_value_knn</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;K-Nearest Neighbors RMSE: </span><span class="si">{</span><span class="n">rmse_knn</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, R²: </span><span class="si">{</span><span class="n">r2_value_knn</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>To visualize the KNN algorithm for the regression task, we plot the <strong>predictive curve</strong>, which maps input values to predicted outputs. This curve illustrates how KNN responds to changes in a single feature. Since KNN is a non-parametric, instance-based method, it does not learn a fixed equation during training. Instead, predictions are made by averaging the target values of the <em>k</em> nearest training examples for each input.</p>
<p>The resulting predictive curve is typically piecewise-smooth, adapting to local patterns in the data. That is, the curve may bend or flatten depending on regions where data points are dense or sparse.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/6-regression-predictive-curve-knn-5.png"><img alt="_images/6-regression-predictive-curve-knn-5.png" src="_images/6-regression-predictive-curve-knn-5.png" style="width: 80%;" />
</a>
</figure>
<p>This makes the predictive curve an especially useful tool for assessing whether KNN is underfitting (<em>e.g.</em>, when <em>k</em> is large) or overfitting (e.g., when <em>k</em> is small). By adjusting <em>k</em> and observing changes in the curve’s shape, we can intuitively tune the model’s <strong>bias-variance tradeoff</strong>.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/6-regression-predictive-curve-knn-1357.png"><img alt="_images/6-regression-predictive-curve-knn-1357.png" src="_images/6-regression-predictive-curve-knn-1357.png" style="width: 80%;" />
</a>
</figure>
<div class="admonition-the-bias-variance-tradeoff callout admonition" id="callout-0">
<p class="admonition-title">The bias-variance tradeoff</p>
<p>The bias-variance tradeoff is a fundamental concept in machine learning that describes the balance between model simplicity and model flexibility when trying to make accurate predictions.</p>
<ul class="simple">
<li><p>Bias measures how much a model’s predictions systematically differ from the true values. High bias means the model is too simple and cannot capture the underlying patterns in the data, which leads to underfitting.</p></li>
<li><p>Variance measures how much a model’s predictions change when trained on different datasets. High variance means the model is too sensitive to small fluctuations in the training data, which leads to overfitting.</p></li>
</ul>
</div>
</section>
<section id="linear-regression">
<h4>Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading"></a></h4>
<p>Having explored a KNN regressor to predict penguin body mass from flipper length, we now turn to a fundamental and interpretable alternative: the Linear Regression model. While KNN makes predictions based on the average mass of the most similar observations, linear regression aims to identify a single, global linear relationship between the two variables. This approach fits a straight line through the data that minimizes the overall prediction error, producing a model that is typically less computationally intensive and offers immediate insight into the underlying trend.</p>
<p>The core concept of this linear model is a simple equation:</p>
<div class="math notranslate nohighlight">
\[body\_mass = \beta_0 + \beta_1 × flipper\_length\]</div>
<ul class="simple">
<li><p>the coefficient, $\beta_1$, represents the model’s estimate of how much a penguin’s body mass increases for each additional millimeter of flipper length</p></li>
<li><p>the intercept, $\beta_0$, indicates the theoretical body mass for a penguin with a flipper length of zero. While this value is not biologically meaningful, it is necessary to position the line correctly.</p></li>
</ul>
<p>The fitted values of $\beta_1$ and $\beta_0$ can be accessed via <code class="docutils literal notranslate"><span class="pre">model.coef_</span></code> and <code class="docutils literal notranslate"><span class="pre">model.intercept_</span></code>, respectively. ​This equation provides a direct and interpretable rule: for any given flipper length, we can calculate a precise predicted body mass with given $\beta_1$ and $\beta_0$.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">linear_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">linear_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train_scaled</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">linear_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">y_pred_linear_scaled</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_linear</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_linear_scaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
</pre></div>
</div>
<p>Once trained, we evaluate the linear regression model’s predictive performance on the testing set using the same metrics: RMSE and R² score. In the Penguins dataset, a high R² indicates that flipper length is a strong predictor of body mass, while a low RMSE reflects precise predictions. These metrics also allow for direct comparison with KNN and other models, such as Polynomial Regression and tree-based methods that will be discussed below, highlighting situations where the simple linear assumption is sufficient and where it may fall short.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rmse_linear</span> <span class="o">=</span> <span class="n">root_mean_squared_error</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_linear</span><span class="p">)</span>
<span class="n">r2_value_linear</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_linear</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Linear Regression RMSE: </span><span class="si">{</span><span class="n">rmse_linear</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, R²: </span><span class="si">{</span><span class="n">r2_value_linear</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The resulting predictive curve is shown below.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/6-regression-predictive-curve-linear.png"><img alt="_images/6-regression-predictive-curve-linear.png" src="_images/6-regression-predictive-curve-linear.png" style="width: 80%;" />
</a>
</figure>
<p><strong>Residual analysis</strong></p>
<p>While metrics like RMSE and R-squared scores provide a high-level summary of model performance, <strong>residual analysis</strong> allows us to examine the model more deeply and verify the key assumptions of linear regression, ensuring that its conclusions are valid and reliable. Residuals are the differences between the observed body mass values and the values predicted by the model.</p>
<p>From the figure below, we can see that the residuals are randomly scattered around zero, with no apparent systematic patterns. This indicates that the linear model is largely unbiased and effectively captures the main trend between flipper length and body mass.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/6-regression-linear-residual-analysis.png"><img alt="_images/6-regression-linear-residual-analysis.png" src="_images/6-regression-linear-residual-analysis.png" style="width: 100%;" />
</a>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If we notice certain patterns, <em>i.e.</em>, residuals that consistently increase or decrease with larger flipper lengths, it suggests that the relationship between body mass and flipper length might not be purely linear. Similarly, if the residuals fan out, showing greater spread at higher predicted values, it indicates heteroscedasticity, meaning the model errors are not consistent across the range of predictions. Such patterns imply that a simple linear regression model may not fully capture the variability in body mass.</p>
</div>
<p>Another key aspect of residual analysis involves assessing normality, as linear regression assumes normally distributed residuals for reliable inference. For the Penguins dataset, this can be evaluated using a histogram or a Q-Q (quantile-quantile) plot of the residuals.</p>
<p>The histogram of residuals illustrates the distribution of prediction errors across the dataset. In the Penguins dataset, these residuals should form a roughly symmetric, bell-shaped curve centered at zero. This indicates that the model is not systematically over-predicting or under-predicting body mass, and that most errors are relatively small, with fewer large deviations.</p>
<p>The Q-Q plot compares the distribution of the residuals to a theoretical normal distribution. On the plot, the x-axis represents the expected quantiles from a standard normal distribution, while the y-axis shows the quantiles of the observed residuals. If the residuals are normally distributed, the points should align closely with the diagonal reference line.</p>
<p><strong>Overfitting and underfitting</strong></p>
<p>In the previous section, we evaluated the Linear Regression model on the testing dataset and calculated metrics such as RMSE and R² to understand its predictive performance. While this gives a good indication of how well this model generalizes to unseen data, it only tells half the story.</p>
<p>To get a complete picture, it is important to also assess this model’s performance on the training dataset and compare it with the testing results. This comparison is the primary diagnostic tool for identifying a model’s fundamental flaw: whether it is learning the underlying signal or merely memorizing the data.</p>
<p>By calculating performance metrics like RMSE and R-squared for both training and testing datasets, we can identify potential issues such as overfitting and underfitting.</p>
<ul class="simple">
<li><p><strong>Overfitting</strong> occurs when the model performs extremely well on the training data but poorly on the testing data. This indicates that the model has memorized the training patterns, including noise, rather than capturing the true underlying relationship.</p></li>
<li><p><strong>Underfitting</strong> happens when the model performs poorly on both training and testing datasets, suggesting that it is too simple to capture the relevant trends in the data.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># --- Training data predictions ---</span>
<span class="n">y_pred_train_scaled</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span>
<span class="n">y_pred_train</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_train_scaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="n">rmse_linear_train</span> <span class="o">=</span> <span class="n">root_mean_squared_error</span><span class="p">(</span><span class="n">y_train_orig</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span>
<span class="n">r2_linear_train</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_train_orig</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Linear Regression (Train) RMSE: </span><span class="si">{</span><span class="n">rmse_linear_train</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, R²: </span><span class="si">{</span><span class="n">r2_linear_train</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># --- Testing data predictions ---</span>
<span class="n">y_pred_test_scaled</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_test_scaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="n">rmse_linear_test</span> <span class="o">=</span> <span class="n">root_mean_squared_error</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>
<span class="n">r2_linear_test</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Linear Regression (Test)  RMSE: </span><span class="si">{</span><span class="n">rmse_linear_test</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, R²: </span><span class="si">{</span><span class="n">r2_linear_test</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Linear Regression (Train) RMSE: 387.10, R²: 0.77</span>
<span class="c1"># Linear Regression (Test)  RMSE: 411.85, R²: 0.72</span>
</pre></div>
</div>
<p>The trained Linear Regression model for predicting penguin body mass based on flipper length in the penguins dataset achieves comparable RMSE and R² scores on both the training and testing datasets, indicating a fairly good model.</p>
<div class="admonition-regularized-regression-ridge-and-lasso exercise important admonition" id="exercise-0">
<p class="admonition-title">Regularized Regression (Ridge and Lasso)</p>
<p>To address overfitting and underfitting, regularized regression methods, such as <strong>Ridge</strong> and <strong>Lasso</strong> regression, extend linear regression by adding a penalty term to the standard cost function. This penalty discourages the model from relying too heavily on any single feature or from becoming overly complex by forcing coefficient values to be small.</p>
<ul class="simple">
<li><p><strong>Ridge Regression</strong> (L2 regularization) shrinks coefficients towards zero but never entirely eliminates them, which is highly effective for handling correlated features and improving stability. This is common in the penguins dataset when predictors like flipper length and bill length are correlated.</p></li>
<li><p><strong>Lasso Regression</strong> (L1 regularization) can drive some coefficients to exactly zero, effectively performing automatic feature selection and creating a simpler, more interpretable model. For instance, Lasso might retain flipper length while discarding less predictive features, improving generalization.</p></li>
</ul>
<p>In this exercise (code examples are availalbe in the <a class="reference internal" href="#document-jupyter-notebooks/6-ML-Regression"><span class="std std-doc">Jupyter Notebook</span></a>), we will</p>
<ul class="simple">
<li><p>Train the Penguins dataset using Ridge and Lasso regression models, and compare their fitted parameters, RMSE, and R² scores.</p></li>
<li><p>Conduct a residual analysis to evaluate whether the regularized regression models achieve better performance than the standard linear regression model.</p></li>
</ul>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-UmlkZ2UgUmVncmVzc2lvbg==" aria-selected="true" class="sphinx-tabs-tab group-tab" id="tab-0-UmlkZ2UgUmVncmVzc2lvbg==" name="UmlkZ2UgUmVncmVzc2lvbg==" role="tab" tabindex="0">Ridge Regression</button><button aria-controls="panel-0-TGFzc28gUmVncmVzc2lvbg==" aria-selected="false" class="sphinx-tabs-tab group-tab" id="tab-0-TGFzc28gUmVncmVzc2lvbg==" name="TGFzc28gUmVncmVzc2lvbg==" role="tab" tabindex="-1">Lasso Regression</button></div><div aria-labelledby="tab-0-UmlkZ2UgUmVncmVzc2lvbg==" class="sphinx-tabs-panel group-tab" id="panel-0-UmlkZ2UgUmVncmVzc2lvbg==" name="UmlkZ2UgUmVncmVzc2lvbg==" role="tabpanel" tabindex="0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">ridge_model</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">20.0</span><span class="p">)</span>
<span class="n">ridge_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train_scaled</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ridge_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">ridge_model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">y_pred_ridge_scaled</span> <span class="o">=</span> <span class="n">ridge_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_ridge</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_ridge_scaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="n">rmse_ridge</span> <span class="o">=</span> <span class="n">root_mean_squared_error</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_ridge</span><span class="p">)</span>
<span class="n">r2_value_ridge</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_ridge</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Regularized Regression (Ridge) RMSE: </span><span class="si">{</span><span class="n">rmse_ridge</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, R²: </span><span class="si">{</span><span class="n">r2_value_ridge</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div><div aria-labelledby="tab-0-TGFzc28gUmVncmVzc2lvbg==" class="sphinx-tabs-panel group-tab" hidden="true" id="panel-0-TGFzc28gUmVncmVzc2lvbg==" name="TGFzc28gUmVncmVzc2lvbg==" role="tabpanel" tabindex="0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lasso</span>

<span class="n">lasso_model</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">lasso_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train_scaled</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lasso_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">lasso_model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">y_pred_lasso_scaled</span> <span class="o">=</span> <span class="n">lasso_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_lasso</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_lasso_scaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="n">rmse_lasso</span> <span class="o">=</span> <span class="n">root_mean_squared_error</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_lasso</span><span class="p">)</span>
<span class="n">r2_value_lasso</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_lasso</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Regularized Regression (Lasso) RMSE: </span><span class="si">{</span><span class="n">rmse_lasso</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, R²: </span><span class="si">{</span><span class="n">r2_value_lasso</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div><figure class="align-center">
<a class="reference internal image-reference" href="_images/6-regression-predictive-curve-linear-ridge-lasso.png"><img alt="_images/6-regression-predictive-curve-linear-ridge-lasso.png" src="_images/6-regression-predictive-curve-linear-ridge-lasso.png" style="width: 80%;" />
</a>
</figure>
</div>
</div>
</section>
<section id="polynomial-regression">
<h4>Polynomial Regression<a class="headerlink" href="#polynomial-regression" title="Link to this heading"></a></h4>
<p>In the previous section, we assumed that penguin body mass is linearly proportional to flipper length, and after training, we have verified that this assumption holds reasonably well. However, for other applications, if two variables are explicitly not linearly related, and a simple linear model may fail to capture the underlying patterns. In such cases, we can resort to polynomial regression to capture non-linear relationship by including higher-degree terms of the predictor variable.</p>
<p>In the context of the Penguins dataset, polynomial regression extends linear regression by modeling body mass as a polynomial function of flipper length with the formula as</p>
<div class="math notranslate nohighlight">
\[body\_mass = \beta_0 + \beta_1 × flipper\_length + \beta_2 × flipper\_length^2 + \beta_3 × flipper\_length^3 + ...\]</div>
<p>This approach allows the model to fit a curved relationship, which might be relevant if, for example, body mass increases more rapidly with flipper length for larger penguins, as seen in species like Gentoo.</p>
<p>The process of training a Polynomial Regression model is similar to Linear Regression. We first transform the original feature (flipper length) by adding polynomial terms (<em>e.g.</em>, $flipper_length^2$ and higher-degree terms), creating a feature matrix that the Polynomial Regression model uses to fit a non-linear curve while still employing Linear Regression techniques on the transformed features.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">degree3</span><span class="o">=</span><span class="mi">3</span>
<span class="n">poly3_model</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree3</span><span class="p">),</span> <span class="n">LinearRegression</span><span class="p">())</span>
<span class="n">poly3_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train_scaled</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">poly3_model</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;linearregression&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">poly3_model</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;linearregression&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="n">y_pred_poly3_scaled</span> <span class="o">=</span> <span class="n">poly3_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_poly3</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_poly3_scaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="n">rmse_poly3</span> <span class="o">=</span> <span class="n">root_mean_squared_error</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_poly3</span><span class="p">)</span>
<span class="n">r2_value_poly3</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_poly3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Polynomial Regression (degree=</span><span class="si">{</span><span class="n">degree3</span><span class="si">}</span><span class="s2">) RMSE: </span><span class="si">{</span><span class="n">rmse_poly3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, R²: </span><span class="si">{</span><span class="n">r2_value_poly3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The Polynomial Regression model is trained by minimizing the sum of squared and cubic residuals, and performance is evaluated using metrics like RMSE and R². Compared with Linear Regression, we see that a third-degree polynomial regression model provides a marginally better fit than the simple linear model.</p>
<p>Below we present the predictive curves for Polynomial Regression models with degrees 3 and 5, alongside the curve for Linear Regression. In addition, we report the evaluation metrics (RMSE and R²) on the testing dataset to provide a quantitative comparison.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/6-regression-predictive-curve-linear-poly35.png"><img alt="_images/6-regression-predictive-curve-linear-poly35.png" src="_images/6-regression-predictive-curve-linear-poly35.png" style="width: 80%;" />
</a>
</figure>
<table class="docutils align-center" id="id1">
<caption><span class="caption-text">Performance metrics (RMSE and R²) on the testing dataset</span><a class="headerlink" href="#id1" title="Link to this table"></a></caption>
<thead>
<tr class="row-odd"><th class="head text-center"><p>Model</p></th>
<th class="head text-center"><p>RMSE</p></th>
<th class="head text-center"><p>R²</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Linear Regression</p></td>
<td class="text-center"><p>411.85</p></td>
<td class="text-center"><p>0.72</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Ridge Regression</p></td>
<td class="text-center"><p>414.18</p></td>
<td class="text-center"><p>0.72</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Lasso Regression</p></td>
<td class="text-center"><p>437.19</p></td>
<td class="text-center"><p>0.69</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Polynomial Regression <br>(degree=3)</p></td>
<td class="text-center"><p>407.47</p></td>
<td class="text-center"><p>0.73</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Polynomial Regression <br>(degree=5)</p></td>
<td class="text-center"><p>415.55</p></td>
<td class="text-center"><p>0.72</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Support Vector Regression</p></td>
<td class="text-center"><p>424.24</p></td>
<td class="text-center"><p>0.70</p></td>
</tr>
</tbody>
</table>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>It is crucial to approach this added complexity with caution. While higher-degree polynomials can achieve very close fits to the training data, they are also highly prone to overfitting. For example, a model with a very high degree (<em>e.g.</em>, degree = 10) may contort itself to pass through nearly every training point, capturing random noise rather than the true underlying biological relationship. As a result, such a model would likely perform poorly on unseen test data, sacrificing generalizability for apparent short-term accuracy.</p>
</div>
<p>Additionally, residual analysis can provide further information on the model performance.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/6-regression-linear-poly5-residual-analysis.png"><img alt="_images/6-regression-linear-poly5-residual-analysis.png" src="_images/6-regression-linear-poly5-residual-analysis.png" style="width: 100%;" />
</a>
</figure>
<p>Compared with Linear Regression, the Polynomial Regression model with degree = 5 shows signs of overfitting, as evidenced by systematic deviations in the residuals and the asymmetric distribution of prediction errors across the dataset.</p>
<p>Overall, Polynomial Regression serves as a simple yet powerful extension of Linear Regression. It enables us to capture non-linear relationships while still benefiting from the interpretability and computational efficiency of a linear framework applied to transformed features.
The key challenge lies in selecting the appropriate polynomial degree. Too low a degree may underfit the data, missing important trends, while too high a degree risks memorizing noise and overfitting. To strike the right balance, rigorous evaluation techniques — such as cross-validation on the training set — are typically used to identify the optimal degree, followed by a final assessment on the testing set. By carefully tuning complexity, polynomial regression can deliver genuine improvements in predictive accuracy and provide a more faithful representation of the often non-linear patterns found in the natural world.</p>
</section>
<section id="support-vector-machine">
<h4>Support Vector Machine<a class="headerlink" href="#support-vector-machine" title="Link to this heading"></a></h4>
<p>In the previous episode, we introduced the SVM model, which is widely recognized for its effectiveness in classification tasks by finding an optimal hyperplane that maximizes the margin between classes. Here, we adapt the same principles to regression through <strong>Support Vector Regression</strong> (SVR). Unlike Linear Regression or Polynomial Regression, which minimize squared errors, SVR builds on the concepts of margins and support vectors, and aims to find a tube (or a channel) (ε-insensitive zone) of a specified width that captures as many data points as possible, while only the points lying outside this tube (the support vector) affect the model’s predictions.</p>
<p>The core challenge SVR faces is that, by its fundamental nature, it seeks a linear relationship (a flat hyperplane). In many real-world problems, such as predicting a penguin’s body mass from its flipper length, the underlying relationship, while roughly linear, may contain subtle non-linear patterns that a straight line cannot fully capture.</p>
<p>Rather than manually generating polynomial features, which can be computationally expensive and impractical in high-dimensional spaces, kernel functions are used to capture non-linear relationships by implicitly projecting (rather than explicitly transforming) the input data into higher-dimensional feature spaces.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Several kernel types are commonly used in SVR, each imparting different characteristics to the model:</p>
<ul class="simple">
<li><p><strong>Linear Kernel</strong>: The simplest kernel, which does not perform any transformation and assumes a linear relationship between features and the target variable. It is fast and interpretable but lacks flexibility for modeling complex patterns.</p></li>
<li><p><strong>Polynomial Kernel</strong>: This kernel enables the model to fit polynomial curves of a specified degree $d$ with contronable flexibility. While more adaptable than a linear kernel, it can be sensitive to the chosen degree and may perform poorly when extrapolating beyond the training range.</p></li>
<li><p><strong>Radial Basis Function (RBF) Kernel</strong>: The most widely used kernel for non-linear problems, capable of generating highly flexible and smooth curves. It is versatile and effective for capturing complex relationships in the data.</p></li>
</ul>
</div>
<p>For the penguin regression task, we use the RBF kernel in the SVR model to capture potential non-linear relationships between flipper length and body mass that a simple linear model might not be able to detect.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVR</span>

<span class="n">svr_model</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">100.0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">svr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train_scaled</span><span class="p">)</span>

<span class="n">y_pred_svr_scaled</span> <span class="o">=</span> <span class="n">svr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_svr</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_svr_scaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="n">rmse_svr</span> <span class="o">=</span> <span class="n">root_mean_squared_error</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_svr</span><span class="p">)</span>
<span class="n">r2_value_svr</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_svr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Support Vector Regression RMSE: </span><span class="si">{</span><span class="n">rmse_svr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, R²: </span><span class="si">{</span><span class="n">r2_value_svr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/6-regression-predictive-curve-linear-poly3-svr.png"><img alt="_images/6-regression-predictive-curve-linear-poly3-svr.png" src="_images/6-regression-predictive-curve-linear-poly3-svr.png" style="width: 80%;" />
</a>
</figure>
<p>For the Penguins dataset, SVR can potentially outperform Linear Regression if the relationship between flipper length and body mass is non-linear, as it can flexibly adapt to complex patterns without requiring explicit polynomial features. However, in this regression task, SVR with a (non-linear) RBF kernel underperforms compared to the Linear Regression model. There are two main reasons for this:</p>
<ul class="simple">
<li><p>The relationship between flipper length and body mass is fundamentally linear or only mildly non-linear, so the flexibility of SVR is not necessary.</p></li>
<li><p>The hyperparameters (<code class="docutils literal notranslate"><span class="pre">gamma</span></code>, <code class="docutils literal notranslate"><span class="pre">C</span></code>, <code class="docutils literal notranslate"><span class="pre">epsilon</span></code> in <code class="docutils literal notranslate"><span class="pre">svr_model</span> <span class="pre">=</span> <span class="pre">SVR(kernel='rbf',</span> <span class="pre">gamma=0.1,</span> <span class="pre">C=100.0,</span> <span class="pre">epsilon=1.0)</span></code>) used for the SVR model may not be optimal. In this case, tuning the hyperparameters using techniques like grid search or cross-validation could improve performance.</p></li>
</ul>
<div class="admonition-tuning-hyperparameter exercise important admonition" id="exercise-1">
<p class="admonition-title"><strong>Tuning hyperparameter</strong></p>
<p>In this exercise (code examples are availalbe in the <a class="reference internal" href="#document-jupyter-notebooks/6-ML-Regression"><span class="std std-doc">Jupyter Notebook</span></a>, we will use <strong>grid search</strong> combined with <strong>cross-validation</strong> to find the optimal hyperparameters for the SVR model (code example is available in the Jupyter Notebook). We will:</p>
<ul class="simple">
<li><p>Compare RMSE and R² values to evaluate predictive performance.</p></li>
<li><p>Plot predictive curves to visually assess how well the model fits the data.</p></li>
</ul>
<div class="dropdown callout admonition" id="callout-1">
<p class="admonition-title"><strong>Grid search</strong> and <strong>Cross validation</strong></p>
<p>Grid search is a method used to find the best combination of hyperparameters for a ML model. It will search all possible combinations of the hyperparameter values you specify, trains the model for each combination, and evaluates it using a validation set. After testing all combinations, it picks the set of hyperparameters that gives the best performance.</p>
<p>Cross-validation is a method to check how well a model will perform on unseen data.</p>
<ul class="simple">
<li><p>Instead of just splitting the data once into training and testing sets, cross-validation splits the data into several parts (folds).</p></li>
<li><p>The model is trained on some folds and tested on the remaining fold. This process is repeated so that every fold gets a turn as the test set.</p></li>
<li><p>The performance scores from all folds are averaged, giving a more reliable estimate of how the model will do in real situations.</p></li>
<li><p>Here is a one example: The dataset is split into 5 parts. The model trains on 4 parts and tests on 1 part. This is repeated 5 times, each time with a different test part.</p></li>
</ul>
</div>
</div>
</section>
<section id="decision-tree">
<h4>Decision Tree<a class="headerlink" href="#decision-tree" title="Link to this heading"></a></h4>
<p>In addition to instance-based models such as KNN and margin-based models like SVR, we can also apply tree-based methods for the regression task to predict a penguin’s body mass from its flipper length. One of the most intuitive and interpretable approaches in this family is the <strong>Decision Tree Regressor</strong>.</p>
<p>A Decision Tree Regressor is a non-linear model that partitions the feature space (flipper length) into distinct regions based on feature thresholds and assigns a constant value (the average body mass) to each region. For the penguin regression task, the Decision Tree Regressor recursively splits the dataset into groups of penguins with similar flipper lengths. At each split, the model selects the threshold that minimizes the variance of body mass within the resulting groups. This recursive process continues until stopping criteria are met, such as reaching a maximum tree depth or a minimum number of samples per leaf.</p>
<p>Below is a code example demonstrating the Decision Tree Regressor (<code class="docutils literal notranslate"><span class="pre">max_depth</span> <span class="pre">=</span> <span class="pre">3</span></code>) applied to the penguins regression task.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="n">dt3_model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">dt3_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train_scaled</span><span class="p">)</span>

<span class="n">y_pred_dt3_scaled</span> <span class="o">=</span> <span class="n">dt3_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_dt3</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_dt3_scaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="n">rmse_dt3</span> <span class="o">=</span> <span class="n">root_mean_squared_error</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_dt3</span><span class="p">)</span>
<span class="n">r2_value_dt3</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_dt3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decision Tree (depth=3) RMSE: </span><span class="si">{</span><span class="n">rmse_dt3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, R²: </span><span class="si">{</span><span class="n">r2_value_dt3</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Predictions for a new penguin are straightforward. Once the tree is built, the model follows the decision path down the tree according to the penguin’s feature values until it reaches a leaf node. The predicted value is then the mean (or median) body mass of the training samples in that leaf. For example, if a leaf node contains 15 penguins with an average body mass of 3850 grams, any new penguin whose features lead it to this leaf will be predicted to have a mass of 3850 g.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/6-regression-predictive-curve-decision-tree-35.png"><img alt="_images/6-regression-predictive-curve-decision-tree-35.png" src="_images/6-regression-predictive-curve-decision-tree-35.png" style="width: 80%;" />
</a>
</figure>
<p>When applying a Decision Tree Regressor to the penguin regression tesk, the tree depth plays a crucial role in shaping the fitted curve. With a relatively shallow tree, such as depth = 3, the model makes only a few splits on flipper length, resulting in broad, step-like regions where body mass is predicted as the average within each group. This provides a coarse approximation of the relationship, capturing the general trend but missing finer variations.</p>
<p>Increasing the tree depth to 5 allows for more splits, creating narrower regions and a fitted curve that follows the data more closely. While this improves flexibility and reduces bias, it also increases the risk of capturing noise in the training set, leading to overfitting. Comparing fitted curves at different depths illustrates <strong>the classic trade-off in decision trees: shallow trees may underfit, while deeper trees may fit the training data too closely</strong>.</p>
<div class="admonition-random-forest-and-gradient-boosting-regressions exercise important admonition" id="exercise-2">
<p class="admonition-title">Random Forest and Gradient Boosting Regressions</p>
<p>We have discussed the limitations of Decision Tree algorithm, which can be mitigated using powerful ensemble methods such as Random Forest and Gradient Boosting. In this exercise (code examples are availalbe in the <a class="reference internal" href="#document-jupyter-notebooks/6-ML-Regression"><span class="std std-doc">Jupyter Notebook</span></a>), we will:</p>
<ul class="simple">
<li><p>Apply Random Forest and Gradient Boosting Regressors to the penguin regression task using initial (arbitrary) hyperparameters.</p></li>
<li><p>Optimize hyperparameters via grid search and cross-validation to improve predictive performance.</p></li>
<li><p>Plot predictive curves to visually evaluate how well each model fits the data.</p></li>
</ul>
</div>
</section>
<section id="multi-layer-perceptron">
<h4>Multi-Layer Perceptron<a class="headerlink" href="#multi-layer-perceptron" title="Link to this heading"></a></h4>
<p>For this penguins task, we will explore implementations using three popular frameworks: the user-friendly scikit-learn, the high-level deep learning library Keras, and the more granular, research-oriented PyTorch.</p>
<p>In Scikit-learn, the <code class="docutils literal notranslate"><span class="pre">MLPRegressor</span></code> class offers a convenient interface for training small- to medium-sized neural networks, requiring minimal configuration while still providing flexibility for most regression tasks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neural_network</span><span class="w"> </span><span class="kn">import</span> <span class="n">MLPRegressor</span>

<span class="n">mlp_model</span> <span class="o">=</span> <span class="n">MLPRegressor</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> 
                         <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">mlp_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train_scaled</span><span class="p">)</span>

<span class="n">y_pred_mlp_scaled</span> <span class="o">=</span> <span class="n">mlp_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">y_pred_mlp</span> <span class="o">=</span> <span class="n">scaler_y</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">y_pred_mlp_scaled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="n">rmse_mlp</span> <span class="o">=</span> <span class="n">root_mean_squared_error</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_mlp</span><span class="p">)</span>
<span class="n">r2_value_mlp</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test_orig</span><span class="p">,</span> <span class="n">y_pred_mlp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Multi-Layer Perceptron RMSE: </span><span class="si">{</span><span class="n">rmse_mlp</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, R²: </span><span class="si">{</span><span class="n">r2_value_mlp</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/6-regression-predictive-curve-linear-mlp.png"><img alt="_images/6-regression-predictive-curve-linear-mlp.png" src="_images/6-regression-predictive-curve-linear-mlp.png" style="width: 80%;" />
</a>
</figure>
<p>For greater control and scalability, frameworks like Keras (built on TensorFlow) and PyTorch allow us to design custom neural network architectures. We can specify the number of hidden layers, the number of neurons per layer, activation functions (<em>e.g.</em>, ReLU or tanh), and optimization algorithms (<em>e.g.</em>, stochastic gradient descent or Adam). These frameworks also offer tools for monitoring training, adjusting learning rates, and preventing overfitting through techniques such as regularization or dropout.</p>
<div class="admonition-dnns-using-keras-tensorflow-and-pytorch exercise important admonition" id="exercise-3">
<p class="admonition-title">DNNs using Keras (TensorFlow) and PyTorch</p>
<p>Code examples are availalbe in the <a class="reference internal" href="#document-jupyter-notebooks/6-ML-Regression"><span class="std std-doc">Jupyter Notebook</span></a>:</p>
<ul class="simple">
<li><p>Construct DNNs using Keras and PyTorch.</p></li>
<li><p>Apply DNNs to the penguin regression task using given hyperparameters.</p></li>
<li><p>Optimize hyperparameters listed below to improve predictive performance</p>
<ul>
<li><p>architecture hyperparameters</p>
<ul>
<li><p>number of layers</p></li>
<li><p>number of neurons per layer</p></li>
<li><p>activation functions (<em>e.g.</em>, <code class="docutils literal notranslate"><span class="pre">ReLU</span></code>, <code class="docutils literal notranslate"><span class="pre">tanh</span></code>, <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>)</p></li>
</ul>
</li>
<li><p>training hyperparameters</p>
<ul>
<li><p>optimizers (<em>e.g.</em>, SGD, Adam, RMSprop)</p></li>
<li><p>learning rate</p></li>
<li><p>batch size</p></li>
<li><p>number of epochs</p></li>
</ul>
</li>
<li><p>regularization hyperparameters</p>
<ul>
<li><p>dropout rate</p></li>
<li><p>early stopping parameters</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Plot predictive curves for visualization</p></li>
</ul>
</div>
</section>
</section>
<section id="comparison-of-trained-models">
<h3>Comparison of Trained Models<a class="headerlink" href="#comparison-of-trained-models" title="Link to this heading"></a></h3>
<figure class="align-default">
<img alt="_images/6-rmse-r2-scores_alla.png" src="_images/6-rmse-r2-scores_alla.png" />
</figure>
<p><strong>Summary of regression models</strong></p>
<ul class="simple">
<li><p>Best performance: Polynomial Regression (degree=3) gave the lowest RMSE (407.47) and the highest R² (0.73), slightly outperforming Linear Regression.</p></li>
<li><p>Strong performers: Linear Regression, Ridge, Polynomial Regression (degree=5), Decision Tree (depth=3), MLP, and Deep Neural Networks (Keras, PyTorch) showed similar results (RMSE ≈ 412–415, R² ≈ 0.71–0.72).</p></li>
<li><p>Moderate performers: Random Forest (depth=5), Decision Tree (depth=5), and SVR (optimized) performed decently (R² ≈ 0.70–0.72) but not better than simpler models.</p></li>
<li><p>Weaker performers: Plain SVR, Lasso, Gradient Boosting, and KNN trailed behind with higher RMSEs (&gt;430) and lower R² values (≈0.68–0.69).</p></li>
</ul>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">Hyperparameter optimization</a></p></li>
<li><p><a class="reference external" href="https://drbeane.github.io/python_dsci/pages/grid_search.html">Grid search</a></p></li>
<li><p><a class="reference external" href="https://thatdatatho.com/detailed-introduction-cross-validation-machine-learning/">Introduction to Cross-Validation in Machine Learning</a></p></li>
</ul>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>We explored regression as a supervised learning task for predicting penguins body mass from their flipper length.</p></li>
<li><p>Starting with simple models like Linear Regression, we gradually introduced more advanced approaches, including Polynomial Regression, Support Vector Regression, tree-based models, and neural networks.</p></li>
<li><p>All models were evaluated using matrics including RMSE and R² scores, and visualized with predictive curves.</p></li>
<li><p>Simple models (Linear and Polynomial Regression) performed as well as or better than complex models (SVR, trees, ensembles, neural network-based models).</p></li>
<li><p>This indicates that the relationship between flipper length and body mass is mostly linear, with mild non-linear patterns.</p></li>
</ul>
</div>
</section>
</section>
<span id="document-07-unsupervised-ML-clustering"></span><section id="unsupervised-learning-i-clustering">
<h2>Unsupervised Learning (I): Clustering<a class="headerlink" href="#unsupervised-learning-i-clustering" title="Link to this heading"></a></h2>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Explain what unsupervised learning is and how clustering fits into it.</p></li>
<li><p>Understand main ideas behind centroid-based (K-Means), hierarchical, density-based (DBSCAN), model-based (GMM), and graph-based (Spectral Clustering) methods.</p></li>
<li><p>Apply representative clustering algorithms in practice.</p></li>
<li><p>Understand how to evaluate clustering quality using confusion matrices, silhouette scores, or visualizations.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>40 min teaching/demonstration</p></li>
<li><p>40 min exercises</p></li>
</ul>
</div>
<section id="unsupervised-learning">
<h3>Unsupervised Learning<a class="headerlink" href="#unsupervised-learning" title="Link to this heading"></a></h3>
<p>In <a class="reference internal" href="#document-05-supervised-ML-classification"><span class="std std-doc">Episode 5</span></a> and <a class="reference internal" href="#document-06-supervised-ML-regression"><span class="std std-doc">Episode 6</span></a>, we have explored supervised learning, where each training example includes both input features and the corresponding output.
This setup enables the models to learn a direct mapping from inputs to targets. For the Penguins dataset, both classification and regression models, such as logistic/linear regression, decision trees, and neural networks, were applied to either classify penguin species or predict body mass based on flipper length.</p>
<p>It is important to emphasize that <strong>supervised learning</strong> depends heavily on labeled data, which may not always be available in real-world scenarios. Collecting labeled data can be expensive, time-consuming, or even impossible.
In such cases, we turn to <strong>unsupervised learning</strong> to uncover patterns and structure in the data.</p>
<p>In unsupervised learning, the dataset contains only the input features without associated labels. The goal is to discover hidden patterns, structures within the data, and derive insights without explicit supervision.
Unsupervised learning is essential for analyzing the vast amounts of raw data generated in real-world applications, from scientific research to business intelligence. Its significance can be seen across several key areas:</p>
<ul class="simple">
<li><p><strong>Exploratory Data Analysis</strong> (EDA): Techniques such as clustering and dimensionality reduction are fundamental for understanding the structure of complex datasets. They can reveal natural groupings, trends, and correlations that might otherwise remain hidden, providing a crucial first step in any data-driven investigation.</p></li>
<li><p><strong>Anomaly Detection</strong>: Unsupervised learning is vital for maintaining security and operational integrity. By modeling “normal” behavior, algorithms can identify unusual patterns, such as fraudulent financial transactions, network intrusions, or rare mechanical failures, without needing labeled examples of every type of anomaly.</p></li>
<li><p><strong>Feature Engineering</strong> and <strong>Representation Learning</strong>: Methods like <strong>Principal Component Analysis</strong> (PCA) can compress data into its most informative components, reducing noise and improving the efficiency and performance of downstream supervised models.</p></li>
</ul>
<p>In this and the following episodes, we will apply <strong>Clustering</strong> and <strong>Dimensionality Reduction</strong> methods on the Penguins dataset to explore its underlying structure and uncover hidden patterns without the guidance of pre-existing labels.
By employing clustering methods like K-means, we aim to identify species-specific clusters or other biologically meaningful subgroups among Adelie, Gentoo, and Chinstrap penguins. Additionally, dimensionality reduction techniques like PCA will simplify the dataset’s feature space, enabling visualization of complex relationships and enhancing subsequent analyses.
These approaches will deepen our understanding of penguin characteristics, reveal outliers, and complement supervised methods by providing a robust framework for exploratory data analysis.</p>
</section>
<section id="clustering">
<h3>Clustering<a class="headerlink" href="#clustering" title="Link to this heading"></a></h3>
<p>Clustering is one of the most widely used techniques in unsupervised learning, where the goal is to group similar data points together without using predefined labels. For example, if we cluster penguins based on their physical characteristics such as flipper length, body mass, bill length, and bill depth, we may be able to separate them into natural groups that correspond to their species – even without explicitly providing species labels.</p>
<p>Clustering, however, presents several fundamental challenges. One major issue is determining the optimal number of clusters (<em>k</em>), which is often non-trivial. Many algorithms, such as k-means, require specifying the number of clusters in advance, which may not be immediately obvious from the data.</p>
<p>As illustrated in the figure below, the data could be grouped into two, three, or four clusters, depending on the level of granularity chosen. Selecting too few clusters may oversimplify the structure and miss important patterns, while choosing too many clusters can lead to overfitting, where random noise is mistakenly treated as meaningful groups.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/7-rawdata-into-clusters.png"><img alt="_images/7-rawdata-into-clusters.png" src="_images/7-rawdata-into-clusters.png" style="width: 80%;" />
</a>
</figure>
<p>In addition, the quality and scale of features also have a significant impact on clustering results. Features with larger scales can dominate distance computations, making preprocessing steps such as standardization essential.</p>
<p>It should be emphasized that the interpretation of clustering results is subjective. While an algorithm can identify groups, it is up to the analyst to determine whether those groups are meaningful or merely artifacts of the algorithm.</p>
<p>Clustering outcomes are also highly sensitive to the choice of algorithm and distance metric. For instance, K-Means tends to find spherical clusters, whereas DBSCAN (Density-Based Spatial Clustering of Applications with Noise) can detect arbitrarily shaped clusters and identify outliers, may leading to very different conclusions using the same dataset.</p>
<p>In this episode, we will apply multiple clustering algorithms to evaluate their performance on the Penguins dataset.</p>
</section>
<section id="data-preparation">
<h3>Data Preparation<a class="headerlink" href="#data-preparation" title="Link to this heading"></a></h3>
<p>Following the procedures used in previous episodes, we apply the same preprocessing steps to the Penguins dataset, including handling missing values and detecting outliers. For the clustering task, categorical features are not required, so encoding them is unnecessary.</p>
</section>
<section id="data-processing">
<h3>Data Processing<a class="headerlink" href="#data-processing" title="Link to this heading"></a></h3>
<p>The data processing is straightforward for the clustering task: we simply extract the numerical variables and apply standardization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;penguins&#39;</span><span class="p">)</span>
<span class="n">penguins_clustering</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">penguins_clustering</span><span class="p">[[</span><span class="s1">&#39;bill_length_mm&#39;</span><span class="p">,</span> <span class="s1">&#39;bill_depth_mm&#39;</span><span class="p">,</span> <span class="s1">&#39;flipper_length_mm&#39;</span><span class="p">,</span> <span class="s1">&#39;body_mass_g&#39;</span><span class="p">]]</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-model-evaluating-model-performance">
<h3>Training Model &amp; Evaluating Model Performance<a class="headerlink" href="#training-model-evaluating-model-performance" title="Link to this heading"></a></h3>
<section id="k-means-clustering">
<h4>K-Means Clustering<a class="headerlink" href="#k-means-clustering" title="Link to this heading"></a></h4>
<p>K-Means clustering, a centroid-based partitioning method, is a widely used unsupervised learning algorithm that divides data into k distinct, non-overlapping clusters. K-Means operates on a simple yet powerful principle: each cluster is represented by its centroid, which is the mean position of all points within the cluster.</p>
<p>The algorithm begins by randomly initializing k centroids in the feature space and then iteratively refines their positions through a two-step <strong>expectation</strong>-<strong>maximization</strong> process:</p>
<ul class="simple">
<li><p>In the expectation step, each data point is assigned to its nearest centroid based on Euclidean distance, forming preliminary clusters.</p></li>
<li><p>In the maximization step, the centroids are recalculated as the mean of all points assigned to each cluster. This process repeats until convergence, typically when centroid positions stabilize or cluster assignments no longer change significantly.</p></li>
</ul>
<figure class="align-default" id="id1">
<img alt="_images/7-kmeans-description-expectation-maximization.png" src="_images/7-kmeans-description-expectation-maximization.png" />
<figcaption>
<p><span class="caption-text">align: center
width: 100%</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>We build a <code class="docutils literal notranslate"><span class="pre">kmeans</span></code> model using the <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> class from <code class="docutils literal notranslate"><span class="pre">sklearn.cluster</span></code> with specified parameters. By fitting the constructed <code class="docutils literal notranslate"><span class="pre">kmeans</span></code> model, we can obtain the cluster ID to which each point belongs.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_clusters=k</span></code>, number of clusters to find from the dataset</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_init=10</span></code>, number of times KMeans runs with different centroid seeds (default 10 or more)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_state</span></code>, ensures reproducibility</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># we know there are 3 species</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</pre></div>
</div>
<p>Evaluating clustering efficiency can be tricky because, unlike supervised learning, we don’t always have “true labels”. Depending on whether we have ground truth or not, there are two main evaluation approaches:</p>
<ul class="simple">
<li><p>If we don’t know the actual labels, we can measure how well each data point fits within its assigned cluster compared to other clusters, such as the <strong>Silhouette Score</strong>.</p></li>
<li><p>If we know the actual labels (<em>e.g.</em>, penguin species), we can measure how well clustering recovers them, such as the <strong>Adjusted Rand Index</strong> (ARI).</p></li>
</ul>
<p>Here we adopte these two matrics to evaluate model performance.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins_cluster</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">clusters</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">silhouette_score</span><span class="p">,</span> <span class="n">adjusted_rand_score</span>

<span class="n">sil_score</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">clusters</span><span class="p">)</span>
<span class="n">ari_score</span> <span class="o">=</span> <span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">],</span> <span class="n">clusters</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Silhouette Score: </span><span class="si">{</span><span class="n">sil_score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adjusted Rand Index (vs true species): </span><span class="si">{</span><span class="n">ari_score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Higher ARI values indicate that the clustering results align closely with the true groupings. An ARI of 1.0 represents perfect agreement, and of 0 corresponds to random clustering. Negative ARI values suggest clustering performance worse than random chance.</p>
<p>The Silhouette Score ranges between -1 to +1.</p>
<ul class="simple">
<li><p>A score of +1 indicates that samples in the dataset are well-matched to their own cluster and clearly separated from other clusters.</p></li>
<li><p>A score of 0 suggests that samples lie on the boundary between clusters.</p></li>
<li><p>A score of -1 implies that samples may have been incorrectly assigned to clusters.</p></li>
</ul>
<p>We take a further step to visualize the distributions of penguins by comparing their true labels with the clusters determined by the <code class="docutils literal notranslate"><span class="pre">kmeans</span></code> model.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/7-kmeans-penguins-label-cluster.png"><img alt="_images/7-kmeans-penguins-label-cluster.png" src="_images/7-kmeans-penguins-label-cluster.png" style="width: 100%;" />
</a>
</figure>
<p>We have 333 penguins, and from the plots above it is difficult to determine how many penguins belong to each cluster and what their species are. This can be clarified by examining the distribution of penguin species across clusters and computing a cross-tabulation of two categorical variables using the <code class="docutils literal notranslate"><span class="pre">.crosstab()</span></code> method in Pandas.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cross_tab</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">],</span> <span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span>

<span class="n">cross_tab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">stacked</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/7-kmeans-penguins-in-clusters.png"><img alt="_images/7-kmeans-penguins-in-clusters.png" src="_images/7-kmeans-penguins-in-clusters.png" style="width: 80%;" />
</a>
</figure>
<p><strong>Determination of optimal number of clusters</strong></p>
<p>At the beginning of this section, we set <code class="docutils literal notranslate"><span class="pre">n_clusters</span> <span class="pre">=</span> <span class="pre">3</span></code>, because we already knew that the Penguins dataset contains three species. However, in many real-world applications, the true number of groups or clusters is not known in advance. In such cases, it becomes essential to estimate the appropriate number of clusters before performing the actual clustering task.</p>
<p>To address this, we employ two widely-used heuristic methods, the <strong>Elbow Method</strong> and the <strong>Silhouette Score analysis</strong>, to determine the optimal cluster number <em>k</em>.</p>
<ul class="simple">
<li><p>The Elbow Method quantifies the quality of the clustering using <strong>Within-Cluster Sum of Squares</strong> (WCSS), which measures how tightly the data points in each cluster are grouped around their centroid.</p>
<ul>
<li><p>Intuitively, we want clusters that are tight and cohesive, which corresponds to a low WCSS.</p></li>
<li><p>As we increase the number of clusters <em>k</em>, the WCSS will always decrease because the clusters become smaller and tighter. Beyond a certain point, the improvement of k becomes marginal contribution to WCSS.</p></li>
<li><p>By plotting the WCSS against the number of clusters, we look for the <strong>elbow</strong> point in the curve, which represents a good balance between model complexity and cluster compactness.</p></li>
</ul>
</li>
<li><p>The Silhouette Score Method evaluates the quality of clustering by measuring how similar each data point is to its own cluster compared to other clusters.</p>
<ul>
<li><p>For a single data point, the silhouette coefficient compares the average distance to points in its own cluster (cohesion) to the average distance to points in the nearest neighboring cluster (separation).</p></li>
</ul>
</li>
</ul>
<p>We rerun the computation using the K-Means algorithm across a range of cluster values. For each tested number of clusters, we compute both the WCSS and the Silhouette Score. By plotting these metrics against the number of clusters <em>k</em>, we can visually assess the trade-offs and identify the most suitable cluster count. The code example and corresponding output are shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_clusters</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">wcss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">silhouette_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_clusters</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

    <span class="n">wcss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>

    <span class="n">silhouette_avg</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
    <span class="n">silhouette_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">silhouette_avg</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Clusters: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, WCSS: </span><span class="si">{</span><span class="n">kmeans</span><span class="o">.</span><span class="n">inertia_</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, Silhouette: </span><span class="si">{</span><span class="n">silhouette_avg</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="_images/7-kmeans-optimal-parameter.png" src="_images/7-kmeans-optimal-parameter.png" />
</figure>
<figure class="align-default">
<img alt="_images/7-kmeans-234-clusters.png" src="_images/7-kmeans-234-clusters.png" />
</figure>
<div class="admonition-why-does-k-means-suggest-grouping-the-penguins-into-2-clusters discussion important admonition" id="discussion-0">
<p class="admonition-title">Why does K-Means suggest grouping the penguins into 2 clusters?</p>
<p>K-Means suggesting 2 clusters instead of 3 in the Penguins dataset is actually a common outcome, and it happens for several reasons:</p>
<ul class="simple">
<li><p>feature overlap between species:</p>
<ul>
<li><p>Some penguin species, like Adelie and Chinstrap, have very similar measurements for features such as bill length, bill depth, flipper length, and body mass.</p></li>
<li><p>K-Means uses Euclidean distance, so if two species’ points are close in feature space, the algorithm may group them into a single cluster.</p></li>
</ul>
</li>
<li><p>data scaling or feature selection:</p>
<ul>
<li><p>Features with larger scales or high correlation can dominate the distance calculation.</p></li>
<li><p>If preprocessing is not optimal, K-Means may prioritize grouping based on dominant features rather than species distinctions.</p></li>
</ul>
</li>
<li><p>K-Means assumes spherical clusters:</p>
<ul>
<li><p>K-Means works best when clusters are roughly spherical and equally sized.</p></li>
<li><p>If clusters have different shapes, densities, or overlap, K-Means may merge two clusters to minimize WCSS, resulting in fewer clusters than the actual number of species.</p></li>
</ul>
</li>
<li><p>Elbow or Silhouette methods suggest 2:</p>
<ul>
<li><p>When using the elbow method, the WCSS curve may show a clear “elbow” at k=2, indicating that adding a third cluster doesn’t significantly reduce WCSS.</p></li>
<li><p>Similarly, the average Silhouette Score might be highest for k=2, because splitting the overlapping species into separate clusters reduces cohesion.</p></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="hierarchical-clustering">
<h4>Hierarchical Clustering<a class="headerlink" href="#hierarchical-clustering" title="Link to this heading"></a></h4>
<p><strong>Hierarchical clustering</strong> is an unsupervised learning method that builds a hierarchy of clusters by either divisive (top-down) or agglomerative (bottom-up) strategies. In the agglomerative approach, each data point starts as its own cluster, and the algorithm iteratively merges the closest clusters based on a distance metric. This continues until all points are merged into a single cluster. The result can be visualized as a <strong>dendrogram</strong>, a tree-like diagram showing the nested structure of clusters at different levels of granularity.</p>
<div class="admonition-hierarchical-clustering-vs-decision-tree callout admonition" id="callout-0">
<p class="admonition-title">Hierarchical Clustering <em>vs.</em> Decision Tree</p>
<p>Hierarchical clustering is conceptually similar to a decision tree in some ways, but it is not the same as a decision tree or random forest.</p>
<ul class="simple">
<li><p>Similarity is that both build a tree-like structure</p></li>
<li><p>Key differences</p>
<ul>
<li><p>Purpose of the tree</p>
<ul>
<li><p>In hierarchical clustering, the tree (dendrogram) represents nested clusters and shows the order in which points/clusters are merged or split.</p></li>
<li><p>In decision trees, the tree represents decision rules to predict a target variable.</p></li>
</ul>
</li>
<li><p>Supervised <em>vs.</em> unsupervised algorithms</p></li>
<li><p>With and without ensemble concept</p>
<ul>
<li><p>Random forest is an ensemble of decision trees and focuses on improving prediction accuracy and reducing overfitting.</p></li>
<li><p>Hierarchical clustering has no ensemble concept or predictive objective; it is purely descriptive.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>In short, <strong>Hierarchical Clustering is structurally similar to a tree (like a dendrogram)</strong>, but <strong>it is unsupervised and descriptive, unlike Decision Trees or Random Forests, which are supervised and predictive</strong>.</p>
</div>
<p>We first use SciPy and then Scikit-Learn packages for the clustering task, for the purpose of comparison.
In SciPy, hierarchical clustering involves two steps: <strong>computing the linkage matrix</strong> (<code class="docutils literal notranslate"><span class="pre">linkage()</span></code>), and then <strong>extracting clusters from it</strong> (<code class="docutils literal notranslate"><span class="pre">fcluster()</span></code>). In the code listed below,</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">linkage()</span></code> computes the full hierarchical clustering tree (linkage matrix), storing all merges, distances, and cluster sizes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fcluster()</span></code>, cuts the dendrogram at a specified threshold to produce a flat clustering, here forming 3 clusters (<code class="docutils literal notranslate"><span class="pre">t=3</span></code>, criterion=’maxclust’).</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.cluster.hierarchy</span><span class="w"> </span><span class="kn">import</span> <span class="n">dendrogram</span><span class="p">,</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">fcluster</span>

<span class="c1"># compute linkage matrix</span>
<span class="n">linked</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;ward&#39;</span><span class="p">)</span>

<span class="c1"># assign 3 clusters based on dendrogram cut</span>
<span class="n">labels_scipy</span> <span class="o">=</span> <span class="n">fcluster</span><span class="p">(</span><span class="n">linked</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;maxclust&#39;</span><span class="p">)</span>

<span class="n">penguins_cluster</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;hier_cluster_scipy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels_scipy</span>
</pre></div>
</div>
<p>Next we plot the dendrogram to visualize clustering structure.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/7-hierarcical-dendrogram.png"><img alt="_images/7-hierarcical-dendrogram.png" src="_images/7-hierarcical-dendrogram.png" style="width: 100%;" />
</a>
</figure>
<p>Here, we move to the Scikit-learn package and employ <code class="docutils literal notranslate"><span class="pre">AgglomerativeClustering</span></code> to construct the clustering model with hyperparameters.</p>
<ul class="simple">
<li><p>The parameter <code class="docutils literal notranslate"><span class="pre">linkage</span></code> determines how the distance between clusters is calculated. There are several options for this parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">ward</span></code>,  minimizes the variance of merged clusters (only works with Euclidean distance).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">complete</span></code>, maximum distance between points in clusters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">average</span></code>, average distance between points in clusters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">single</span></code>, minimum distance between points in clusters.</p></li>
</ul>
</li>
<li><p>The parameter <code class="docutils literal notranslate"><span class="pre">metric</span></code> is the distance metric used to compute the distance between points.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">euclidean</span></code> is the standard straight-line distance in feature space.</p></li>
<li><p>There are also other options like <code class="docutils literal notranslate"><span class="pre">manhattan</span></code>, <code class="docutils literal notranslate"><span class="pre">cosine</span></code>, <em>etc.</em></p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">AgglomerativeClustering</span>

<span class="n">hc</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">,</span> <span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;ward&#39;</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">hc</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

<span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;hier_cluster_aggl&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
</pre></div>
</div>
<p>We can examine the number of penguins in each species within the clusters determined by the two models, and visualize their distributions using a confusion matrix.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/7-hierarchical-clusters-from-scipy-scikit-learn.png"><img alt="_images/7-hierarchical-clusters-from-scipy-scikit-learn.png" src="_images/7-hierarchical-clusters-from-scipy-scikit-learn.png" style="width: 100%;" />
</a>
</figure>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/7-hierarchical-confusion-matrix.png"><img alt="_images/7-hierarchical-confusion-matrix.png" src="_images/7-hierarchical-confusion-matrix.png" style="width: 100%;" />
</a>
</figure>
</section>
<section id="dbscan">
<h4>DBSCAN<a class="headerlink" href="#dbscan" title="Link to this heading"></a></h4>
<p><strong>DBSCAN</strong> (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together points that are closely packed while marking points in low-density regions as outliers. Unlike K-Means, DBSCAN does not require specifying the number of clusters in advance, making it particularly useful when the number of natural clusters is unknown. It is also capable of identifying clusters of arbitrary shapes, unlike centroid-based methods that assume roughly spherical clusters. This makes DBSCAN robust to clusters with irregular shapes or varying sizes.</p>
<p>We build a model using the <code class="docutils literal notranslate"><span class="pre">DBSCAN</span></code> class from <code class="docutils literal notranslate"><span class="pre">sklearn.cluster</span></code> with specified parameters. DBSCAN relies on two key parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">eps</span></code>, the radius that defines the neighborhood around a point.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">min_samples</span></code>, the minimum number of points required within a point’s eps neighborhood for it to be considered a core point.</p></li>
<li><p>Below we use <code class="docutils literal notranslate"><span class="pre">eps=0.55</span></code> and <code class="docutils literal notranslate"><span class="pre">min_samples=5</span></code> in the code.</p>
<ul>
<li><p>You can experiment with other <code class="docutils literal notranslate"><span class="pre">eps</span></code> values (<em>e.g.</em>, 0.50 and 0.80) while keeping <code class="docutils literal notranslate"><span class="pre">min_samples=5</span></code> to observe how the clustering results change.</p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">DBSCAN</span>

<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.55</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

<span class="c1"># evaluate clustering (only if at least 2 clusters found)</span>
<span class="n">n_clusters</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">in</span> <span class="n">labels</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DBSCAN found </span><span class="si">{</span><span class="n">n_clusters</span><span class="si">}</span><span class="s2"> clusters (and </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">labels</span><span class="o">==-</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2"> noise points).&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">n_clusters</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">sil_score</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>  <span class="c1"># FIXED</span>
    <span class="n">ari_score</span> <span class="o">=</span> <span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">],</span> <span class="n">labels</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Silhouette Score: </span><span class="si">{</span><span class="n">sil_score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adjusted Rand Index (vs true species): </span><span class="si">{</span><span class="n">ari_score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DBSCAN classifies samples into three types:</p>
<ul class="simple">
<li><p><strong>core points</strong>: points with at least <code class="docutils literal notranslate"><span class="pre">min_samples</span></code> neighbors within <code class="docutils literal notranslate"><span class="pre">eps</span></code>.</p></li>
<li><p><strong>border points</strong>: points within <code class="docutils literal notranslate"><span class="pre">eps</span></code> of a core point but with fewer than <code class="docutils literal notranslate"><span class="pre">min_samples</span></code> neighbors themselves.</p></li>
<li><p><strong>noise points</strong> (outliers): points that are neither core nor border points.</p></li>
</ul>
</div>
<p>Next, we visualize the distributions of penguins in each cluster, including any points identified as noise.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/7-dbscan-point-types.png"><img alt="_images/7-dbscan-point-types.png" src="_images/7-dbscan-point-types.png" style="width: 80%;" />
</a>
</figure>
<p>We further examine the distribution of penguin species across clusters using the cross-tabulation (<code class="docutils literal notranslate"><span class="pre">.crosstab()</span></code>) method in Pandas.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cross_tab</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;dbscan_cluster&#39;</span><span class="p">],</span> <span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span>

<span class="n">cross_tab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">stacked</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/7-dbscan-penguins-in-clusters.png"><img alt="_images/7-dbscan-penguins-in-clusters.png" src="_images/7-dbscan-penguins-in-clusters.png" style="width: 80%;" />
</a>
</figure>
<div class="admonition-exercise exercise important admonition" id="exercise-0">
<p class="admonition-title">Exercise</p>
<p>In this exercise (code examples are availalbe in the <a class="reference internal" href="#document-jupyter-notebooks/7-ML-Clustering"><span class="std std-doc">Jupyter Notebook</span></a>, we will</p>
<ul class="simple">
<li><p>Experiment with <code class="docutils literal notranslate"><span class="pre">eps</span></code> values (0.50 and 0.80) while keeping <code class="docutils literal notranslate"><span class="pre">min_samples=5</span></code> to observe how the clustering results change.</p></li>
<li><p>Computations with more combinations of <code class="docutils literal notranslate"><span class="pre">eps</span></code> and <code class="docutils literal notranslate"><span class="pre">min_samples</span></code>.</p></li>
<li><p>Explore methods to find optimal hyperparameters (using grid search and cross-validation).</p></li>
</ul>
</div>
</section>
<section id="gaussian-mixture-models">
<h4>Gaussian Mixture Models<a class="headerlink" href="#gaussian-mixture-models" title="Link to this heading"></a></h4>
<p>After exploring centroid-based methods like K-Means, hierarchical clustering models, and density-based approaches such as DBSCAN, we now turn our attention to model-based clustering algorithms.
Unlike the previous methods that rely primarily on distance metrics or density thresholds, model-based clustering assumes that the data come from a mixture of underlying probability distributions. Each distribution corresponds to a cluster, and the algorithm tries to estimate both the parameters of the distributions and the clusters.</p>
<p>Since model-based clustering assumes that a dataset is generated from a mixture of underlying probability distributions, a variety of algorithms have been developed to handle different types of distributions. The choice of model depends on the nature of the data.</p>
<ul class="simple">
<li><p>When dealing with continuous numerical features that approximately follow a bell-shaped distribution, <strong>Gaussian Mixture Models</strong> (GMMs) are the most common choice.</p></li>
<li><p>If the data consists of count values, mixture models based on Poisson distributions can be used.</p></li>
<li><p>For categorical data, methods like Latent Class Analysis (LCA), which treats clusters as latent categorical variables, are often applied.</p></li>
<li><p>In more flexible Bayesian frameworks, Dirichlet Process Mixture Models allow the number of clusters to be inferred directly from the data, avoiding the need to predefine it.</p></li>
</ul>
<p>The GMM assumes that data points are generated from a mixture of Gaussian distributions, each representing a cluster. Instead of assigning points strictly to one cluster (like K-Means), GMM assigns each point a probability of belonging to each cluster, making it a soft clustering method.</p>
<p>In the following example, we construct the GMM model with the specified hyperparameters.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_components=3</span></code> means the number of Gaussian distributions (clusters) to fit</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">covariance_type</span></code> controls the form of the covariance matrix for each Gaussian distribution</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">full</span></code> means that each cluster has its own general covariance matrix (most flexible, allows ellipsoidal shapes).</p></li>
<li><p>other options like <code class="docutils literal notranslate"><span class="pre">tied</span></code>, <code class="docutils literal notranslate"><span class="pre">diag</span></code>, and <code class="docutils literal notranslate"><span class="pre">spherical</span></code> corresponding to clusters with different shapes</p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.mixture</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianMixture</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">adjusted_rand_score</span><span class="p">,</span> <span class="n">silhouette_score</span>

<span class="c1"># build GMM model with 3 components (clusters)</span>
<span class="n">gmm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
<span class="n">labels_gmm</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</pre></div>
</div>
<p>Following the steps in the <a class="reference internal" href="#document-jupyter-notebooks/7-ML-Clustering"><span class="std std-doc">Jupyter Notebook</span></a>, we can 1) examine the distribution of penguin species across clusters using the cross-tabulation method, 2) visualize the distribution of penguins within each cluster, and 3) illustrate their distributions with a confusion matrix.</p>
<p>Here, we specifically highlight the distributions penguins data points in clusters and the shapes clusters obtained from the KMeans and GMM models.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/7-gmm-elliptical-clusters.png"><img alt="_images/7-gmm-elliptical-clusters.png" src="_images/7-gmm-elliptical-clusters.png" style="width: 100%;" />
</a>
</figure>
</section>
<section id="spectral-clustering">
<h4>Spectral Clustering<a class="headerlink" href="#spectral-clustering" title="Link to this heading"></a></h4>
<p>Following centroid-based, density-based, and model-based methods, we now turn our attention to <strong>Spectral Clustering</strong> algorithms.</p>
<p>Spectral Clustering represents a fundamentally different approach: rather than relying purely on distances between points or density, it leverages graph theory and the eigenstructure of similarity matrices to uncover clusters. That is, the main idea of this method is to represent the dataset as a graph where each node is a data point and edges encode the similarity between points (<em>e.g.</em>, using a Gaussian kernel). By computing the eigenvectors of the graph Laplacian, the algorithm transforms the original data into a lower-dimensional space where clusters become more distinguishable. Once in this space, standard clustering techniques, such as K-Means, are applied to assign cluster labels.</p>
<p>This method is especially powerful for datasets with complex, <strong>non-convex</strong> cluster shapes, where traditional algorithms like K-Means or Hierarchical Clustering may fail to capture the true underlying structure.</p>
<p>We adopted similar procedures to build the model, examine the distribution of penguin species across clusters using the cross-tabulation method, and visualize the distribution of penguins within each cluster.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">SpectralClustering</span>

<span class="c1"># build model using Spectral Clustering (graph-based)</span>
<span class="n">spectral</span> <span class="o">=</span> <span class="n">SpectralClustering</span><span class="p">(</span>
    <span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">affinity</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span>   <span class="c1"># Gaussian kernel</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>        <span class="c1"># controls width of the Gaussian</span>
    <span class="n">assign_labels</span><span class="o">=</span><span class="s1">&#39;kmeans&#39;</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">spectral</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
<span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;spectral_cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>

<span class="c1"># evaluate clustering</span>
<span class="n">ari</span> <span class="o">=</span> <span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">],</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">sil</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adjusted Rand Index (vs species): </span><span class="si">{</span><span class="n">ari</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Silhouette Score: </span><span class="si">{</span><span class="n">sil</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/7-spectral-confusion-matrix-kmeans-gmm.png"><img alt="_images/7-spectral-confusion-matrix-kmeans-gmm.png" src="_images/7-spectral-confusion-matrix-kmeans-gmm.png" style="width: 100%;" />
</a>
</figure>
<div class="dropdown callout admonition" id="callout-1">
<p class="admonition-title">Spectral Clustering <em>vs.</em> Gaussian Mixture Models <em>vs.</em> K-Means</p>
<p>From the confusion matrix shown above, it seems that Spectral Clustering and K-Means models are less effective than the GMM model on the Penguins dataset. Main reasons may attribute to:</p>
<ul class="simple">
<li><p>Small dataset size:</p>
<ul>
<li><p>The Penguins dataset has only 333 samples.</p></li>
<li><p>Spectral clustering computes the eigenvectors of the similarity matrix, which can be less stable with small datasets, leading to variability in cluster assignments.</p></li>
</ul>
</li>
<li><p>Small number of features/low dimensionality</p>
<ul>
<li><p>The Penguins dataset typically uses only 4 numerical features. In such low-dimensional, fairly well-separated data, simpler methods like K-Means or Gaussian Mixture Models often perform just as well or better.</p></li>
<li><p>Spectral clustering shines when clusters are non-convex or complexly shaped in high-dimensional spaces.</p></li>
</ul>
</li>
</ul>
</div>
<div class="admonition-exercise exercise important admonition" id="exercise-1">
<p class="admonition-title">Exercise</p>
<p>Here, we apply these two models to the classic <strong>two-moon dataset</strong>, a well-known synthetic dataset with non-linearly separable clusters. This allows us to visually and quantitatively evaluate how each algorithm performs in capturing complex, non-convex structures and to compare their strengths and limitations in a controlled setting.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/7-spectral-kmeans-two-moon-dataset.png"><img alt="_images/7-spectral-kmeans-two-moon-dataset.png" src="_images/7-spectral-kmeans-two-moon-dataset.png" style="width: 80%;" />
</a>
</figure>
<p><strong>Spectral Clustering excels for datasets with complex, non-convex cluster shapes, where traditional algorithms like K-Meansmay fail to capture the true underlying structure</strong>.</p>
</div>
</section>
</section>
<section id="comparison-of-clustering-models">
<h3>Comparison of Clustering Models<a class="headerlink" href="#comparison-of-clustering-models" title="Link to this heading"></a></h3>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/7-comparison-sil-ari-scores.png"><img alt="_images/7-comparison-sil-ari-scores.png" src="_images/7-comparison-sil-ari-scores.png" style="width: 100%;" />
</a>
</figure>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Method</p></th>
<th class="head text-center"><p>Type/Approach</p></th>
<th class="head text-center"><p>Key Characteristics</p></th>
<th class="head text-center"><p>Pros</p></th>
<th class="head text-center"><p>Limitations</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>K-Means</p></td>
<td class="text-center"><p>Centroid-based</p></td>
<td class="text-center"><p>Partitions data into <br>k clusters by minimizing <br>within-cluster variance; <br>clusters represented by centroids</p></td>
<td class="text-center"><p>Simple, fast, widely used; <br>interpretable</p></td>
<td class="text-center"><p>Assumes spherical clusters; <br>sensitive to initialization and outliers; <br>requires specifying k</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Hierarchical Clustering <br>(SciPy)</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>Similar to Scikit-Learn, <br>uses linkage matrix <br>and fcluster to assign clusters</p></td>
<td class="text-center"><p>Flexible; supports different <br>distance metrics and linkage methods</p></td>
<td class="text-center"><p>Requires careful selection of <br>threshold to cut dendrogram; <br>can be slow for large data</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Hierarchical Clustering <br>(Scikit-Learn)</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>Builds a hierarchy of clusters <br>either bottom-up (agglomerative) <br>or top-down (divisive); <br>linkage criteria define <br>merge/split decisions</p></td>
<td class="text-center"><p>Dendrogram visualization; <br>no need to pre-specify number of clusters</p></td>
<td class="text-center"><p>Computationally expensive for large datasets; <br>choice of linkage affects results</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>DBSCAN</p></td>
<td class="text-center"><p>Density-based</p></td>
<td class="text-center"><p>Groups points <br>based on density; identifies core, border, <br>and noise points; no need to specify <br>number of clusters</p></td>
<td class="text-center"><p>Detects arbitrarily shaped clusters; <br>robust to outliers; <br>identifies noise</p></td>
<td class="text-center"><p>Sensitive to eps and min_samples; <br>struggles with varying densities</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>GMM</p></td>
<td class="text-center"><p>Model-based</p></td>
<td class="text-center"><p>Assumes data generated <br>from a mixture of Gaussian distributions; <br>each cluster has mean, covariance, and weight</p></td>
<td class="text-center"><p>Can model elliptical clusters; <br>provides probabilities for cluster membership</p></td>
<td class="text-center"><p>Sensitive to initialization; <br>may converge to local optima; <br>assumes Gaussian distribution</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Spectral Clustering</p></td>
<td class="text-center"><p>Graph-based</p></td>
<td class="text-center"><p>Uses graph Laplacian of similarity matrix; <br>clusters derived from eigenvectors; <br>can handle non-convex shapes</p></td>
<td class="text-center"><p>Captures complex structures; <br>good for connected or non-spherical clusters</p></td>
<td class="text-center"><p>Computationally expensive for large datasets; <br>sensitive to affinity and connectivity; <br>may fail on disconnected graphs</p></td>
</tr>
</tbody>
</table>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Clustering is about grouping data points that are similar to each other, without using labels.</p></li>
<li><p>Representative algorithms for clustering tasks: K-Means (centroid-based), Hierarchical, DBSCAN (density-based), Gaussian Mixture Models (model-based), and Spectral Clustering (graph-based).</p></li>
<li><p>Use tools like Silhouette score or visual plots to see how well clusters are separated.</p></li>
<li><p><strong>No single algorithm is “best”, and the right method depends on your data: size, type, shape, and what you want to achieve</strong>.</p></li>
</ul>
</div>
</section>
</section>
<span id="document-08-unsupervised-ML-dimensionality-reduction"></span><section id="unsupervised-learning-ii-dimensionality-reduction">
<h2>Unsupervised Learning (II): Dimensionality Reduction<a class="headerlink" href="#unsupervised-learning-ii-dimensionality-reduction" title="Link to this heading"></a></h2>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Understand the motivation for dimensionality reduction.</p></li>
<li><p>Explain key dimensionality reduction techniques (PCA, t-SNE, UMAP).</p></li>
<li><p>Describe the idea of PCA (linear method that keeps most variance) and apply PCA to correlate features with components.</p></li>
<li><p>Interpret explained variance ratio and decide how many components to keep.</p></li>
<li><p>Use PCA loadings and correlation circles to see how features contribute.</p></li>
<li><p>Recognize t-SNE and UMAP as nonlinear methods for complex data visualization.</p></li>
<li><p>Connect dimensionality reduction with clustering.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>40 min teaching/demonstration</p></li>
<li><p>40 min exercises</p></li>
</ul>
</div>
<section id="from-clustering-to-dimensionality-reduction-simplifying-complexity">
<h3>From Clustering to Dimensionality Reduction: Simplifying Complexity<a class="headerlink" href="#from-clustering-to-dimensionality-reduction-simplifying-complexity" title="Link to this heading"></a></h3>
<p>In the <a class="reference internal" href="#document-07-unsupervised-ML-clustering"><span class="std std-doc">last episode</span></a>, we talked about clustering, which is a core unsupervised ML technique that groups similar data points into clusters based on their features, without requiring labeled data. The fundamental value of clustering lies in its ability to reveal segments and patterns that are not immediately obvious, with applications ranging from customer segmentation in marketing to anomaly detection in network security.</p>
<p>Despite its usefulness, clustering comes with notable limitations. A major challenge is determining the appropriate number of clusters in advance, as in K-Means, where results can vary depending on initialization.
Clustering results are also highly sensitive to the choice of distance metric and scaling of features, which can significantly alter outcomes.
Furthermore, clustering often struggles with high-dimensional data due to the “curse of dimensionality”, where distance measures lose their discriminative power, making it harder to identify meaningful groups.
Given these challenges, especially around data sensitivity and interpretability, it is often crucial to preprocess data with another form of unsupervised learning before clustering: <strong>Dimensionality Reduction</strong>.</p>
<p>Where clustering seeks to group samples, dimensionality reduction focuses on simplifying the feature space itself. By transforming a high-dimensional dataset into a lower-dimensional subspace while preserving its most critical relationships, dimension reduction can mitigate noise, reduce computational cost, and reveal the most discriminative features that define the data’s structure.
This process not only addresses clustering’s sensitivity to irrelevant features but also provides a powerful foundation for visualizing potential clusters in two or three dimensions, making the entire analytical process more robust and insightful.</p>
<p>Methods such as <strong>PCA</strong> (Principal Component Analysis), <strong>t-SNE</strong> (t-Distributed Stochastic Neighbor Embedding), and <strong>UMAP</strong> (Uniform Manifold Approximation and Projection) make data more manageable, reduce noise, and improve clustering performance.</p>
<p>Since the Penguins dataset includes multiple features (bill length, bill depth, flipper length, body mass, species labels, <em>etc.</em>), plotting it directly in two dimensions can make it difficult to capture all the relationships and structures hidden in the dataset. That is why we parepare the pairplots between all pair of features to achieve a clear visualization of their correlations.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-penguins-pairplot.png"><img alt="_images/4-penguins-pairplot.png" src="_images/4-penguins-pairplot.png" style="width: 100%;" />
</a>
</figure>
<p>In this episode, we will explore dimensionality reduction techniques and apply them to the Penguins dataset. Our goal is to take the dataset’s multiple features, and project them into a simpler, lower-dimensional space for a better visualization.
This process not only helps us better understand the data but also prepares it for downstream tasks, such as clustering or classification, by reducing noise and highlighting the most informative aspects of the dataset.</p>
</section>
<section id="data-preparation">
<h3>Data Preparation<a class="headerlink" href="#data-preparation" title="Link to this heading"></a></h3>
<p>Following the procedures used in previous episodes, we apply the same preprocessing steps to the Penguins dataset, including handling missing values and detecting outliers. For the clustering task, categorical features are not required, so encoding them is unnecessary.</p>
</section>
<section id="data-processing">
<h3>Data Processing<a class="headerlink" href="#data-processing" title="Link to this heading"></a></h3>
<p>The data processing is straightforward for the clustering task: we simply extract the numerical variables and apply standardization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;penguins&#39;</span><span class="p">)</span>
<span class="n">penguins_dimR</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">penguins_dimR</span><span class="o">.</span><span class="n">duplicated</span><span class="p">()</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>

<span class="n">species</span> <span class="o">=</span> <span class="n">penguins_dimR</span><span class="p">[</span><span class="s2">&quot;species&quot;</span><span class="p">]</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">penguins_dimR</span><span class="p">[[</span><span class="s1">&#39;bill_length_mm&#39;</span><span class="p">,</span> <span class="s1">&#39;bill_depth_mm&#39;</span><span class="p">,</span> <span class="s1">&#39;flipper_length_mm&#39;</span><span class="p">,</span> <span class="s1">&#39;body_mass_g&#39;</span><span class="p">]]</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-model-evaluating-model-performance">
<h3>Training Model &amp; Evaluating Model Performance<a class="headerlink" href="#training-model-evaluating-model-performance" title="Link to this heading"></a></h3>
<section id="principal-component-analysis">
<h4>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Link to this heading"></a></h4>
<p>We start with Principal Component Analysis (PCA), a powerful statistical technique used to reduce the dimensionality of data while preserving as much variability as possible.
This is achieved by transforming the original variables into a new set of uncorrelated variables called <strong>principal components</strong>.
Each principal component is a linear combination of the original variables, and they are ordered such that the first few retain most of the variation present in the original variables.</p>
<p><strong>1. A simplified view: PCA with two components</strong></p>
<p>We begin with a simple subspace consisting of two principal components. That is, we will transform the Penguins dataset, which originally contains four numerical features (bill length, bill depth, flipper length, and body mass), into a reduced subdataset represented by just two composite variables.
These two new variables, called principal components, form a simplified version of the Penguins dataset and can essentially capture the most significant variance in the Penguins dataset while reducing complexity.
This allows us to visualize the structure of the data in a two-dimensional space and better understand the relationships among the penguins.</p>
<p>We build a model using the <code class="docutils literal notranslate"><span class="pre">PCA</span></code> class from <code class="docutils literal notranslate"><span class="pre">sklearn.decomposition</span></code>, specifying that the <code class="docutils literal notranslate"><span class="pre">X_scaled</span></code> data should be reduced to two principal components.”</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>

<span class="c1"># construct a PCA model with 2 PCs</span>
<span class="n">pca_2</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_pca_2</span> <span class="o">=</span> <span class="n">pca_2</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
<span class="n">explained_var_2</span> <span class="o">=</span> <span class="n">pca_2</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;The explained variance of PC1    is </span><span class="si">{</span><span class="n">explained_var_2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.2%</span><span class="si">}</span>
<span class="s1">The explained variance of PC2    is </span><span class="si">{</span><span class="n">explained_var_2</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.2%</span><span class="si">}</span>
<span class="s1">The explained variance (PC1+PC2) is </span><span class="si">{</span><span class="n">explained_var_2</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s1">.2%</span><span class="si">}</span><span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The term <code class="docutils literal notranslate"><span class="pre">explained</span> <span class="pre">variance</span> <span class="pre">ratio</span></code> tells us how much of the total variability in the original Penguins dataset is captured by each principal component. The first principal component has an explained variance ratio of 0.72, it means 72% of the variability in the dataset can be represented by that single component.</p>
<p>For each penguine in the dataset, the contributions of its four original featuers to the new components are available in the <code class="docutils literal notranslate"><span class="pre">X_pca_2</span></code> dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_pca_2_species</span> <span class="o">=</span> <span class="n">penguins_dimR</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_pca_2</span><span class="p">,</span>
	<span class="n">index</span> <span class="o">=</span> <span class="n">penguins_dimR</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>
	<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PC_1&#39;</span><span class="p">,</span> <span class="s1">&#39;PC_2&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<div class="admonition-what-does-the-data-in-two-principal-components-represent discussion important admonition" id="discussion-0">
<p class="admonition-title">What does the data in two principal components represent?</p>
<ul class="simple">
<li><p>When we have two components,</p>
<ul>
<li><p>PC1 is the direction in feature space along which the data varies the most,</p></li>
<li><p>PC2 is the direction orthogonal to PC1 that captures the next largest variance.</p></li>
</ul>
</li>
<li><p>When we have three components, PC3 captures the next largest source of variance, orthogonal to both PC1 and PC2.</p>
<ul>
<li><p>In such a situation, we have to use 3D visualization to visualize the three components.</p></li>
</ul>
</li>
<li><p>When we have four componnents, PC4 captures the next largest source of variance, orthogonal to both PC1, PC2, and PC3.</p>
<ul>
<li><p>It id difficult for us to visualize the four dimensional space.</p></li>
</ul>
</li>
</ul>
<p>A short summary</p>
<ul class="simple">
<li><p>fewer components → lower-dimensional representation → some information is lost, but main patterns remain.</p></li>
<li><p>more components → higher-dimensional representation → more information retained.</p></li>
</ul>
</div>
<p>After reducing the dimensionality of the Penguins dataset to two principal components, we can now visualize the transformed data. Each penguin, originally described by four numerical features (bill length, bill depth, flipper length, and body mass), is now represented by just two composite variables.</p>
<p>By plotting the two components, we can examine how penguins cluster in this reduced space.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/8-pca2-penguins-distributions-species-sex-island.png"><img alt="_images/8-pca2-penguins-distributions-species-sex-island.png" src="_images/8-pca2-penguins-distributions-species-sex-island.png" style="width: 100%;" />
</a>
</figure>
<p><strong>2. Preserving full variance: PCA with four components</strong></p>
<p>The two-component model does a good job in capturing the main structure of the original features in the Penguins dataset, but it naturally raises an important question: <strong>how many principal components are truly necessary to represent the dataset effectively?</strong>
Choosing the optimal number of components requires balancing simplicity against information retention — fewer components make visualization and interpretation easier, while more components preserve a greater share of the original variance.
By examining metrics such as the explained variance ratio, we can make an informed choice about the number of components needed to capture the essential patterns in the Penguins dataset.</p>
<p>Here, we consider another case in which the new dataset has four principal components, preserving all of the original features in the Penguins dataset. By setting the parameter <code class="docutils literal notranslate"><span class="pre">n_components=4</span></code> and running the code example, we obtain a full representation of the Penguins data in the new component space. This can be verified by examining the <code class="docutils literal notranslate"><span class="pre">explained_variance_ratio</span></code>, which shows how much variance each component contributes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_scaled_temp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

<span class="c1"># build a new PCA model with 4 PCs</span>
<span class="n">pca_4</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">X_pca_4</span> <span class="o">=</span> <span class="n">pca_4</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled_temp</span><span class="p">)</span>
<span class="n">explained_var_4</span> <span class="o">=</span> <span class="n">pca_4</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;The explained variance of PC1 is </span><span class="si">{</span><span class="n">explained_var_4</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.2%</span><span class="si">}</span>
<span class="s1">The explained variance of PC2 is </span><span class="si">{</span><span class="n">explained_var_4</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.2%</span><span class="si">}</span>
<span class="s1">The explained variance of PC3 is </span><span class="si">{</span><span class="n">explained_var_4</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">:</span><span class="s1">.2%</span><span class="si">}</span>
<span class="s1">The explained variance of PC4 is </span><span class="si">{</span><span class="n">explained_var_4</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="si">:</span><span class="s1">.2%</span><span class="si">}</span>
<span class="s1">The explained variance of ALL is </span><span class="si">{</span><span class="n">explained_var_4</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">:</span><span class="s1">.2%</span><span class="si">}</span><span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>In PCA, each original variable (feature) is essentially correlated with each principal component (PC), which can be described by <code class="docutils literal notranslate"><span class="pre">corr_var_comp</span></code>, which qualifies how strongly each original variable contributes to a component. The <code class="docutils literal notranslate"><span class="pre">corr_var_comp</span></code> ranges from -1 to 1:</p>
<ul class="simple">
<li><p>+1 means a perfect positive correlation between the variable and the component.</p></li>
<li><p>0 indicates no linear correlation, and the component does not explain that variable at all.</p></li>
<li><p>-1 suggests a perfect negative correlation.</p></li>
</ul>
<p>In practical applications, the square of the correlation between a variable and a component is often denoted cos² (cosine squared). The cosine of the angle between the variable vector and the component axis tells you how much of the variable’s variance is explained by that component. Squaring it gives the proportion of variance of that variable explained by the component, hence cos².</p>
<p>We first calculate <code class="docutils literal notranslate"><span class="pre">corr_var_comp</span></code> and square it to get the proportion of each variable’s variance explained by the component, as shown in the figure below.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/8-pca4-correlation-variable-component.png"><img alt="_images/8-pca4-correlation-variable-component.png" src="_images/8-pca4-correlation-variable-component.png" style="width: 80%;" />
</a>
</figure>
<p>This allows us to examine and visualize in detail how each original penguine feature contributes to the four principal components.
As discussed in the previous subsection, each principal component is a linear combination of the original features and can be expressed mathematically as follows:</p>
<div class="math notranslate nohighlight">
\[PC1_i = w_1 * bill\_length_i + w_2 * bill\_depth_i + w_3 * flipper\_length_i + w_4 * body\_mass_i\]</div>
<p>By analyzing the PCA loadings, we can see that the first two principal components explain approximately 90% of the total variance in the Penguins dataset. This indicates that most of the important information in the original four features is already captured by these two components.
Consequently, we can conclude that reducing the dataset to two principal components is sufficient for visualization and analysis, striking a balance between simplicity and information retention while effectively summarizing the underlying structure of the data.</p>
<p><strong>3. Correlation circle</strong></p>
<p>To gain deeper insight into how the original variables relate to the principal components, we can use a <strong>correlation circle</strong>, also known as a <strong>variable factor map</strong>. This graphical tool provides a visual representation of the contribution and correlation of each original variable with the components.</p>
<p>In a 2D PCA plot (PC1 <em>vs.</em> PC2 in the left subplot, and PC3 <em>vs.</em> PC4 in the right subplot), each original feature is represented as a vector (arrow) pointing from the origin.</p>
<ul class="simple">
<li><p>The direction of the arrow indicates whether the variable (feature) is positively or negatively correlated with the principal components.</p></li>
<li><p>The length of the arrow indicates the strength of correlation — longer arrows mean the variable (feature) contributes strongly to the components.</p></li>
<li><p>The circle itself (radius = 1) represents the maximum possible correlation between a variable (feature) and the components, since PCA projects standardized variables (features) .</p></li>
</ul>
<p>In addition, the correlations between variables (features) can also be specified.</p>
<ul class="simple">
<li><p>Variables (features) pointing in similar directions are positively correlated (body mass and flipper length), that is why we performed regression task yesterday using these two features.</p></li>
<li><p>Variables (features) pointing in opposite directions are negatively correlated (for specific pairs of components).</p></li>
<li><p>Variables (features) at roughly 90° to each other are nearly uncorrelated.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/8-pca4-correlation-circle.png"><img alt="_images/8-pca4-correlation-circle.png" src="_images/8-pca4-correlation-circle.png" style="width: 100%;" />
</a>
</figure>
</section>
<section id="t-sne">
<h4>t-SNE<a class="headerlink" href="#t-sne" title="Link to this heading"></a></h4>
<p>Since PCA is based on linear combinations of all features, it has some inherent limitations. In particular, it may fail to capture complex, non-linear relationships in the data, and it primarily focuses on maximizing global variance, which can overlook subtle local structures, intricate clusters, or hierarchical patterns – that are quite common in real-world datasets.</p>
<p>To address these challenges, we move beyond linear methods and explore advanced non-linear dimensionality reduction techniques. Algorithms such as <strong>t-SNE</strong> (t-Distributed Stochastic Neighbor Embedding) and <strong>UMAP</strong> (Uniform Manifold Approximation and Projection) are particularly effective at capturing the non-linear structure of the data.
These methods are designed to preserve local neighborhood relationships, revealing intricate patterns that PCA might miss and providing a more detailed view of the underlying structure of the penguin population.</p>
<p>In this subsection, we apply the t-SNE algorithm to visualize local structures and potential clusters, and we will explore UMAP for complementary insights in the next subsection.</p>
<p>t-SNE, is a powerful dimensionality reduction technique primarily used for visualizing high-dimensional data in two or three dimensions. Unlike linear methods such as PCA, t-SNE is non-linear and focuses on preserving the local structure of the data, meaning that points that are close in the high-dimensional space remain close in the lower-dimensional embedding.
This is the main idea of t-SNE and it models the pairwise similarities between data points in both the high-dimensional and low-dimensional spaces.</p>
<p>We build a t-SNE model with 2 principle components using the <code class="docutils literal notranslate"><span class="pre">TSNE</span></code> class in the <code class="docutils literal notranslate"><span class="pre">sklearn.manifold</span></code> before fitting the model.
The hyperparameter <code class="docutils literal notranslate"><span class="pre">perplexity</span></code> controls the number of effective neighbors each point considers when learning the embedding. Higher values emphasize global structure, lower values emphasize local relationships.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.manifold</span><span class="w"> </span><span class="kn">import</span> <span class="n">TSNE</span>

<span class="c1"># build a t-SNE model having 2 PCs</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">perplexity</span> <span class="o">=</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">X_tsne</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</pre></div>
</div>
<p>After fitting the t-SNE model, we can visualize the Penguin dataset in two dimensions, allowing us to explore the relationships between individual data points, identify potential clusters, and observe how different species are distributed across the embedding.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/8-tsne-penguins-distributions-species-sex-island.png"><img alt="_images/8-tsne-penguins-distributions-species-sex-island.png" src="_images/8-tsne-penguins-distributions-species-sex-island.png" style="width: 100%;" />
</a>
</figure>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>It is noted that <strong>t-SNE is primarily a visualization tool rather than a feature extraction method</strong>, and the <strong>resulting embedding should not be used directly for downstream tasks such as classification</strong>. Each time you run t-SNE, the coordinates can change slightly due to randomness, even with the same data. As such, the features produced by t-SNE may not be stable or meaningful for predictive modeling.</p>
</div>
<div class="admonition-exercise exercise important admonition" id="exercise-0">
<p class="admonition-title">Exercise</p>
<p>Here, we will perform a clustering task using K-Means on a dataset reduced to two components obtained from the t-SNE model (code examples are availalbe in the <a class="reference internal" href="#document-jupyter-notebooks/8-ML-Dimension-Reduction"><span class="std std-doc">Jupyter Notebook</span></a>.</p>
<ul class="simple">
<li><p>Group the data into 9 clusters <code class="docutils literal notranslate"><span class="pre">kmeans</span> <span class="pre">=</span> <span class="pre">KMeans(n_clusters=9)</span></code>.</p></li>
<li><p>Repeat the clustering several times to observe how the cluster assignments change.</p></li>
<li><p>Change the number of clusters (<em>e.g.</em>, 5 or 11) to see how it affects the result.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/8-tsne-kmeans-clustering.png"><img alt="_images/8-tsne-kmeans-clustering.png" src="_images/8-tsne-kmeans-clustering.png" style="width: 60%;" />
</a>
</figure>
</div>
</section>
<section id="umap">
<h4>UMAP<a class="headerlink" href="#umap" title="Link to this heading"></a></h4>
<p>UMAP (Uniform Manifold Approximation and Projection) is a non-linear dimensionality reduction technique similar in purpose to t-SNE, but with some key differences:</p>
<ul class="simple">
<li><p>Preserves both local and some global structure of the data.</p></li>
<li><p>Generally faster and more scalable than t-SNE, especially for large datasets.</p></li>
<li><p>Can be used for visualization (2D/3D) or as a preprocessing step for downstream tasks like clustering or classification.</p></li>
</ul>
<p>UMAP is based on <strong>manifold learning</strong> and <strong>graph theory</strong>.</p>
<ul class="simple">
<li><p>Manifold learning means it assumes that even though our data might have many dimensions, the real structure of the data lies on a lower-dimensional surface (like a curve or sheet) inside that high-dimensional space. UMAP tries to find and keep that structure.</p></li>
<li><p>Graph theory means UMAP represents the data as a graph: each point is a node, and connections (edges) show how close points are to each other. Then it uses math to squeeze this graph down into 2D or 3D while keeping the structure as much as possible.</p></li>
</ul>
<p>UMAP it constructs a high-dimensional graph representation of the data and then projects it onto a low-dimensional space, by applying a series of optimization steps. This results in a visual representation of the data that is both accurate and interpretable. In comparison to PCA and t-SNE, UMAP offers a good balance of accuracy, efficiency, and scalability, making it a popular choice for dimensionality reduction in machine learning and data analysis.</p>
<p>Using the same procedure as in the t-SNE subsection, we build and fit the UMAP model, and visualize the Penguin dataset in two dimensions by species, island, and sex.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">umap</span>

<span class="n">umap_model</span> <span class="o">=</span> <span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">X_umap</span> <span class="o">=</span> <span class="n">umap_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

<span class="n">X_umap_species</span> <span class="o">=</span> <span class="n">penguins_dimR</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_umap</span><span class="p">,</span>
									<span class="n">index</span> <span class="o">=</span> <span class="n">penguins_dimR</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>
									<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PC_1&#39;</span><span class="p">,</span> <span class="s1">&#39;PC_2&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/8-umap-penguins-distributions-species-sex-island.png"><img alt="_images/8-umap-penguins-distributions-species-sex-island.png" src="_images/8-umap-penguins-distributions-species-sex-island.png" style="width: 100%;" />
</a>
</figure>
<div class="admonition-exercise exercise important admonition" id="exercise-1">
<p class="admonition-title">Exercise</p>
<p>Here we perform the same clustering task using K-Means on a dataset reduced to two components obtained from the UMAP model (code examples are availalbe in the <a class="reference internal" href="#document-jupyter-notebooks/8-ML-Dimension-Reduction"><span class="std std-doc">Jupyter Notebook</span></a>.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/8-umap-kmeans-clustering.png"><img alt="_images/8-umap-kmeans-clustering.png" src="_images/8-umap-kmeans-clustering.png" style="width: 100%;" />
</a>
</figure>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Representative method include PCA, t-SNE, and UMAP</p></li>
<li><p>PCA is a method to reduce dimensions via creating new variables called principal components (PCs), which are linear combinations of the original features.</p></li>
<li><p>Perform PCA task and then decide optmal number of component.</p></li>
<li><p>T-SNE and UMAP are both nonlinear dimensionality reduction methods mainly used to visualize high-dimensional data in 2D or 3D.</p></li>
<li><p>T-SNE focuses on keeping similar points close and dissimilar points apart in lower-dimensional space, but not applicable for feature reduction or predictive modeling.</p></li>
<li><p>UMAP preserves both local relationships (nearby points stay close) and some global structure, scales well to large datasets, and is mainly used for exploration and visualization.</p></li>
</ul>
</div>
</section>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-quick-reference"></span><section id="quick-reference">
<h2>Quick Reference<a class="headerlink" href="#quick-reference" title="Link to this heading"></a></h2>
</section>
<span id="document-guide"></span><section id="instructor-s-guide">
<h2>Instructor’s guide<a class="headerlink" href="#instructor-s-guide" title="Link to this heading"></a></h2>
<section id="why-we-teach-this-lesson">
<h3>Why we teach this lesson<a class="headerlink" href="#why-we-teach-this-lesson" title="Link to this heading"></a></h3>
</section>
<section id="intended-learning-outcomes">
<h3>Intended learning outcomes<a class="headerlink" href="#intended-learning-outcomes" title="Link to this heading"></a></h3>
</section>
<section id="timing">
<h3>Timing<a class="headerlink" href="#timing" title="Link to this heading"></a></h3>
</section>
<section id="preparing-exercises">
<h3>Preparing exercises<a class="headerlink" href="#preparing-exercises" title="Link to this heading"></a></h3>
<p>e.g. what to do the day before to set up common repositories.</p>
</section>
<section id="other-practical-aspects">
<h3>Other practical aspects<a class="headerlink" href="#other-practical-aspects" title="Link to this heading"></a></h3>
</section>
<section id="interesting-questions-you-might-get">
<h3>Interesting questions you might get<a class="headerlink" href="#interesting-questions-you-might-get" title="Link to this heading"></a></h3>
</section>
<section id="typical-pitfalls">
<h3>Typical pitfalls<a class="headerlink" href="#typical-pitfalls" title="Link to this heading"></a></h3>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
</div>
<section id="who-is-the-course-for">
<h2>Who is the course for?<a class="headerlink" href="#who-is-the-course-for" title="Link to this heading"></a></h2>
</section>
<section id="about-the-course">
<h2>About the course<a class="headerlink" href="#about-the-course" title="Link to this heading"></a></h2>
</section>
<section id="see-also">
<h2>See also<a class="headerlink" href="#see-also" title="Link to this heading"></a></h2>
</section>
<section id="credits">
<h2>Credits<a class="headerlink" href="#credits" title="Link to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, The contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>