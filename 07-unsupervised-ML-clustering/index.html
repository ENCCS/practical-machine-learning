

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Unsupervised Learning (I): Clustering &mdash; Practical Machine Learning  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_lesson.css?v=e9df6548" />
      <link rel="stylesheet" type="text/css" href="../_static/term_role_formatting.css?v=4194e21c" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_rtd_theme_ext_color_contrast.css?v=8e8ea19f" />
      <link rel="stylesheet" type="text/css" href="../_static/overrides.css?v=c88db32d" />

  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=187304be"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=35a8b989"></script>
      <script src="../_static/minipres.js?v=a0d29692"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="next" title="Unsupervised Learning (II): Dimensionality Reduction" href="../08-unsupervised-ML-dimensionality-reduction/" />
    <link rel="prev" title="Supervised Learning (II): Regression" href="../06-supervised-ML-regression/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../" class="icon icon-home">
            Practical Machine Learning
              <img src="../_static/ENCCS.jpg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Software setup</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../00-software-setup/">Setting Up Programming Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Lesson episodes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../01-intro-to-ML/">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02-fundamentals-of-ML/">Fundamentals of Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03-scientific-data-for-ML/">Scientific Data for Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04-data-preparation-for-ML/">Data Preparation for Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05-supervised-ML-classification/">Supervised Learning (I): Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../06-supervised-ML-regression/">Supervised Learning (II): Regression</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Unsupervised Learning (I): Clustering</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#unsupervised-learning">Unsupervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#clustering">Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-preparation">Data Preparation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-processing">Data Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training-model-evaluating-model-performance">Training Model &amp; Evaluating Model Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-means-clustering">K-Means Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hierarchical-clustering">Hierarchical Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dbscan">DBSCAN</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gaussian-mixture-models">Gaussian Mixture Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spectral-clustering">Spectral Clustering</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#comparison-of-clustering-models">Comparison of Clustering Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../08-unsupervised-ML-dimensionality-reduction/">Unsupervised Learning (II): Dimensionality Reduction</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick-reference/">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide/">Instructor’s guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://enccs.se/lessons/">All lessons</a></li>
<li class="toctree-l1"><a class="reference external" href="https://enccs.se/">ENCCS</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../">Practical Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Unsupervised Learning (I): Clustering</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/practical-machine-learning/blob/main/content/07-unsupervised-ML-clustering.md" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="unsupervised-learning-i-clustering">
<h1>Unsupervised Learning (I): Clustering<a class="headerlink" href="#unsupervised-learning-i-clustering" title="Link to this heading"></a></h1>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Explain what unsupervised learning is and how clustering fits into it.</p></li>
<li><p>Understand main ideas behind centroid-based (K-Means), hierarchical, density-based (DBSCAN), model-based (GMM), and graph-based (Spectral Clustering) methods.</p></li>
<li><p>Apply representative clustering algorithms in practice.</p></li>
<li><p>Understand how to evaluate clustering quality using confusion matrices, silhouette scores, or visualizations.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>40 min teaching/demonstration</p></li>
<li><p>40 min exercises</p></li>
</ul>
</div>
<section id="unsupervised-learning">
<h2>Unsupervised Learning<a class="headerlink" href="#unsupervised-learning" title="Link to this heading"></a></h2>
<p>In <a class="reference internal" href="../05-supervised-ML-classification/"><span class="std std-doc">Episode 5</span></a> and <a class="reference internal" href="../06-supervised-ML-regression/"><span class="std std-doc">Episode 6</span></a>, we have explored supervised learning, where each training example includes both input features and the corresponding output.
This setup enables the models to learn a direct mapping from inputs to targets. For the Penguins dataset, both classification and regression models, such as logistic/linear regression, decision trees, and neural networks, were applied to either classify penguin species or predict body mass based on flipper length.</p>
<p>It is important to emphasize that <strong>supervised learning</strong> depends heavily on labeled data, which may not always be available in real-world scenarios. Collecting labeled data can be expensive, time-consuming, or even impossible.
In such cases, we turn to <strong>unsupervised learning</strong> to uncover patterns and structure in the data.</p>
<p>In unsupervised learning, the dataset contains only the input features without associated labels. The goal is to discover hidden patterns, structures within the data, and derive insights without explicit supervision.
Unsupervised learning is essential for analyzing the vast amounts of raw data generated in real-world applications, from scientific research to business intelligence. Its significance can be seen across several key areas:</p>
<ul class="simple">
<li><p><strong>Exploratory Data Analysis</strong> (EDA): Techniques such as clustering and dimensionality reduction are fundamental for understanding the structure of complex datasets. They can reveal natural groupings, trends, and correlations that might otherwise remain hidden, providing a crucial first step in any data-driven investigation.</p></li>
<li><p><strong>Anomaly Detection</strong>: Unsupervised learning is vital for maintaining security and operational integrity. By modeling “normal” behavior, algorithms can identify unusual patterns, such as fraudulent financial transactions, network intrusions, or rare mechanical failures, without needing labeled examples of every type of anomaly.</p></li>
<li><p><strong>Feature Engineering</strong> and <strong>Representation Learning</strong>: Methods like <strong>Principal Component Analysis</strong> (PCA) can compress data into its most informative components, reducing noise and improving the efficiency and performance of downstream supervised models.</p></li>
</ul>
<p>In this and the following episodes, we will apply <strong>Clustering</strong> and <strong>Dimensionality Reduction</strong> methods on the Penguins dataset to explore its underlying structure and uncover hidden patterns without the guidance of pre-existing labels.
By employing clustering methods like K-means, we aim to identify species-specific clusters or other biologically meaningful subgroups among Adelie, Gentoo, and Chinstrap penguins. Additionally, dimensionality reduction techniques like PCA will simplify the dataset’s feature space, enabling visualization of complex relationships and enhancing subsequent analyses.
These approaches will deepen our understanding of penguin characteristics, reveal outliers, and complement supervised methods by providing a robust framework for exploratory data analysis.</p>
</section>
<section id="clustering">
<h2>Clustering<a class="headerlink" href="#clustering" title="Link to this heading"></a></h2>
<p>Clustering is one of the most widely used techniques in unsupervised learning, where the goal is to group similar data points together without using predefined labels. For example, if we cluster penguins based on their physical characteristics such as flipper length, body mass, bill length, and bill depth, we may be able to separate them into natural groups that correspond to their species – even without explicitly providing species labels.</p>
<p>Clustering, however, presents several fundamental challenges. One major issue is determining the optimal number of clusters (<em>k</em>), which is often non-trivial. Many algorithms, such as k-means, require specifying the number of clusters in advance, which may not be immediately obvious from the data.</p>
<p>As illustrated in the figure below, the data could be grouped into two, three, or four clusters, depending on the level of granularity chosen. Selecting too few clusters may oversimplify the structure and miss important patterns, while choosing too many clusters can lead to overfitting, where random noise is mistakenly treated as meaningful groups.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7-rawdata-into-clusters.png"><img alt="../_images/7-rawdata-into-clusters.png" src="../_images/7-rawdata-into-clusters.png" style="width: 80%;" />
</a>
</figure>
<p>In addition, the quality and scale of features also have a significant impact on clustering results. Features with larger scales can dominate distance computations, making preprocessing steps such as standardization essential.</p>
<p>It should be emphasized that the interpretation of clustering results is subjective. While an algorithm can identify groups, it is up to the analyst to determine whether those groups are meaningful or merely artifacts of the algorithm.</p>
<p>Clustering outcomes are also highly sensitive to the choice of algorithm and distance metric. For instance, K-Means tends to find spherical clusters, whereas DBSCAN (Density-Based Spatial Clustering of Applications with Noise) can detect arbitrarily shaped clusters and identify outliers, may leading to very different conclusions using the same dataset.</p>
<p>In this episode, we will apply multiple clustering algorithms to evaluate their performance on the Penguins dataset.</p>
</section>
<section id="data-preparation">
<h2>Data Preparation<a class="headerlink" href="#data-preparation" title="Link to this heading"></a></h2>
<p>Following the procedures used in previous episodes, we apply the same preprocessing steps to the Penguins dataset, including handling missing values and detecting outliers. For the clustering task, categorical features are not required, so encoding them is unnecessary.</p>
</section>
<section id="data-processing">
<h2>Data Processing<a class="headerlink" href="#data-processing" title="Link to this heading"></a></h2>
<p>The data processing is straightforward for the clustering task: we simply extract the numerical variables and apply standardization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;penguins&#39;</span><span class="p">)</span>
<span class="n">penguins_clustering</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">penguins_clustering</span><span class="p">[[</span><span class="s1">&#39;bill_length_mm&#39;</span><span class="p">,</span> <span class="s1">&#39;bill_depth_mm&#39;</span><span class="p">,</span> <span class="s1">&#39;flipper_length_mm&#39;</span><span class="p">,</span> <span class="s1">&#39;body_mass_g&#39;</span><span class="p">]]</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-model-evaluating-model-performance">
<h2>Training Model &amp; Evaluating Model Performance<a class="headerlink" href="#training-model-evaluating-model-performance" title="Link to this heading"></a></h2>
<section id="k-means-clustering">
<h3>K-Means Clustering<a class="headerlink" href="#k-means-clustering" title="Link to this heading"></a></h3>
<p>K-Means clustering, a centroid-based partitioning method, is a widely used unsupervised learning algorithm that divides data into k distinct, non-overlapping clusters. K-Means operates on a simple yet powerful principle: each cluster is represented by its centroid, which is the mean position of all points within the cluster.</p>
<p>The algorithm begins by randomly initializing k centroids in the feature space and then iteratively refines their positions through a two-step <strong>expectation</strong>-<strong>maximization</strong> process:</p>
<ul class="simple">
<li><p>In the expectation step, each data point is assigned to its nearest centroid based on Euclidean distance, forming preliminary clusters.</p></li>
<li><p>In the maximization step, the centroids are recalculated as the mean of all points assigned to each cluster. This process repeats until convergence, typically when centroid positions stabilize or cluster assignments no longer change significantly.</p></li>
</ul>
<figure class="align-default" id="id1">
<img alt="../_images/7-kmeans-description-expectation-maximization.png" src="../_images/7-kmeans-description-expectation-maximization.png" />
<figcaption>
<p><span class="caption-text">align: center
width: 100%</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>We build a <code class="docutils literal notranslate"><span class="pre">kmeans</span></code> model using the <code class="docutils literal notranslate"><span class="pre">KMeans</span></code> class from <code class="docutils literal notranslate"><span class="pre">sklearn.cluster</span></code> with specified parameters. By fitting the constructed <code class="docutils literal notranslate"><span class="pre">kmeans</span></code> model, we can obtain the cluster ID to which each point belongs.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_clusters=k</span></code>, number of clusters to find from the dataset</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_init=10</span></code>, number of times KMeans runs with different centroid seeds (default 10 or more)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_state</span></code>, ensures reproducibility</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>

<span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># we know there are 3 species</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</pre></div>
</div>
<p>Evaluating clustering efficiency can be tricky because, unlike supervised learning, we don’t always have “true labels”. Depending on whether we have ground truth or not, there are two main evaluation approaches:</p>
<ul class="simple">
<li><p>If we don’t know the actual labels, we can measure how well each data point fits within its assigned cluster compared to other clusters, such as the <strong>Silhouette Score</strong>.</p></li>
<li><p>If we know the actual labels (<em>e.g.</em>, penguin species), we can measure how well clustering recovers them, such as the <strong>Adjusted Rand Index</strong> (ARI).</p></li>
</ul>
<p>Here we adopte these two matrics to evaluate model performance.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins_cluster</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">clusters</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">silhouette_score</span><span class="p">,</span> <span class="n">adjusted_rand_score</span>

<span class="n">sil_score</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">clusters</span><span class="p">)</span>
<span class="n">ari_score</span> <span class="o">=</span> <span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">],</span> <span class="n">clusters</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Silhouette Score: </span><span class="si">{</span><span class="n">sil_score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adjusted Rand Index (vs true species): </span><span class="si">{</span><span class="n">ari_score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Higher ARI values indicate that the clustering results align closely with the true groupings. An ARI of 1.0 represents perfect agreement, and of 0 corresponds to random clustering. Negative ARI values suggest clustering performance worse than random chance.</p>
<p>The Silhouette Score ranges between -1 to +1.</p>
<ul class="simple">
<li><p>A score of +1 indicates that samples in the dataset are well-matched to their own cluster and clearly separated from other clusters.</p></li>
<li><p>A score of 0 suggests that samples lie on the boundary between clusters.</p></li>
<li><p>A score of -1 implies that samples may have been incorrectly assigned to clusters.</p></li>
</ul>
<p>We take a further step to visualize the distributions of penguins by comparing their true labels with the clusters determined by the <code class="docutils literal notranslate"><span class="pre">kmeans</span></code> model.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7-kmeans-penguins-label-cluster.png"><img alt="../_images/7-kmeans-penguins-label-cluster.png" src="../_images/7-kmeans-penguins-label-cluster.png" style="width: 100%;" />
</a>
</figure>
<p>We have 333 penguins, and from the plots above it is difficult to determine how many penguins belong to each cluster and what their species are. This can be clarified by examining the distribution of penguin species across clusters and computing a cross-tabulation of two categorical variables using the <code class="docutils literal notranslate"><span class="pre">.crosstab()</span></code> method in Pandas.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cross_tab</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">],</span> <span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span>

<span class="n">cross_tab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">stacked</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7-kmeans-penguins-in-clusters.png"><img alt="../_images/7-kmeans-penguins-in-clusters.png" src="../_images/7-kmeans-penguins-in-clusters.png" style="width: 80%;" />
</a>
</figure>
<p><strong>Determination of optimal number of clusters</strong></p>
<p>At the beginning of this section, we set <code class="docutils literal notranslate"><span class="pre">n_clusters</span> <span class="pre">=</span> <span class="pre">3</span></code>, because we already knew that the Penguins dataset contains three species. However, in many real-world applications, the true number of groups or clusters is not known in advance. In such cases, it becomes essential to estimate the appropriate number of clusters before performing the actual clustering task.</p>
<p>To address this, we employ two widely-used heuristic methods, the <strong>Elbow Method</strong> and the <strong>Silhouette Score analysis</strong>, to determine the optimal cluster number <em>k</em>.</p>
<ul class="simple">
<li><p>The Elbow Method quantifies the quality of the clustering using <strong>Within-Cluster Sum of Squares</strong> (WCSS), which measures how tightly the data points in each cluster are grouped around their centroid.</p>
<ul>
<li><p>Intuitively, we want clusters that are tight and cohesive, which corresponds to a low WCSS.</p></li>
<li><p>As we increase the number of clusters <em>k</em>, the WCSS will always decrease because the clusters become smaller and tighter. Beyond a certain point, the improvement of k becomes marginal contribution to WCSS.</p></li>
<li><p>By plotting the WCSS against the number of clusters, we look for the <strong>elbow</strong> point in the curve, which represents a good balance between model complexity and cluster compactness.</p></li>
</ul>
</li>
<li><p>The Silhouette Score Method evaluates the quality of clustering by measuring how similar each data point is to its own cluster compared to other clusters.</p>
<ul>
<li><p>For a single data point, the silhouette coefficient compares the average distance to points in its own cluster (cohesion) to the average distance to points in the nearest neighboring cluster (separation).</p></li>
</ul>
</li>
</ul>
<p>We rerun the computation using the K-Means algorithm across a range of cluster values. For each tested number of clusters, we compute both the WCSS and the Silhouette Score. By plotting these metrics against the number of clusters <em>k</em>, we can visually assess the trade-offs and identify the most suitable cluster count. The code example and corresponding output are shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_clusters</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">wcss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">silhouette_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_clusters</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

    <span class="n">wcss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>

    <span class="n">silhouette_avg</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
    <span class="n">silhouette_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">silhouette_avg</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Clusters: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">, WCSS: </span><span class="si">{</span><span class="n">kmeans</span><span class="o">.</span><span class="n">inertia_</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, Silhouette: </span><span class="si">{</span><span class="n">silhouette_avg</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="../_images/7-kmeans-optimal-parameter.png" src="../_images/7-kmeans-optimal-parameter.png" />
</figure>
<figure class="align-default">
<img alt="../_images/7-kmeans-234-clusters.png" src="../_images/7-kmeans-234-clusters.png" />
</figure>
<div class="admonition-why-does-k-means-suggest-grouping-the-penguins-into-2-clusters discussion important admonition" id="discussion-0">
<p class="admonition-title">Why does K-Means suggest grouping the penguins into 2 clusters?</p>
<p>K-Means suggesting 2 clusters instead of 3 in the Penguins dataset is actually a common outcome, and it happens for several reasons:</p>
<ul class="simple">
<li><p>feature overlap between species:</p>
<ul>
<li><p>Some penguin species, like Adelie and Chinstrap, have very similar measurements for features such as bill length, bill depth, flipper length, and body mass.</p></li>
<li><p>K-Means uses Euclidean distance, so if two species’ points are close in feature space, the algorithm may group them into a single cluster.</p></li>
</ul>
</li>
<li><p>data scaling or feature selection:</p>
<ul>
<li><p>Features with larger scales or high correlation can dominate the distance calculation.</p></li>
<li><p>If preprocessing is not optimal, K-Means may prioritize grouping based on dominant features rather than species distinctions.</p></li>
</ul>
</li>
<li><p>K-Means assumes spherical clusters:</p>
<ul>
<li><p>K-Means works best when clusters are roughly spherical and equally sized.</p></li>
<li><p>If clusters have different shapes, densities, or overlap, K-Means may merge two clusters to minimize WCSS, resulting in fewer clusters than the actual number of species.</p></li>
</ul>
</li>
<li><p>Elbow or Silhouette methods suggest 2:</p>
<ul>
<li><p>When using the elbow method, the WCSS curve may show a clear “elbow” at k=2, indicating that adding a third cluster doesn’t significantly reduce WCSS.</p></li>
<li><p>Similarly, the average Silhouette Score might be highest for k=2, because splitting the overlapping species into separate clusters reduces cohesion.</p></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="hierarchical-clustering">
<h3>Hierarchical Clustering<a class="headerlink" href="#hierarchical-clustering" title="Link to this heading"></a></h3>
<p><strong>Hierarchical clustering</strong> is an unsupervised learning method that builds a hierarchy of clusters by either divisive (top-down) or agglomerative (bottom-up) strategies. In the agglomerative approach, each data point starts as its own cluster, and the algorithm iteratively merges the closest clusters based on a distance metric. This continues until all points are merged into a single cluster. The result can be visualized as a <strong>dendrogram</strong>, a tree-like diagram showing the nested structure of clusters at different levels of granularity.</p>
<div class="admonition-hierarchical-clustering-vs-decision-tree callout admonition" id="callout-0">
<p class="admonition-title">Hierarchical Clustering <em>vs.</em> Decision Tree</p>
<p>Hierarchical clustering is conceptually similar to a decision tree in some ways, but it is not the same as a decision tree or random forest.</p>
<ul class="simple">
<li><p>Similarity is that both build a tree-like structure</p></li>
<li><p>Key differences</p>
<ul>
<li><p>Purpose of the tree</p>
<ul>
<li><p>In hierarchical clustering, the tree (dendrogram) represents nested clusters and shows the order in which points/clusters are merged or split.</p></li>
<li><p>In decision trees, the tree represents decision rules to predict a target variable.</p></li>
</ul>
</li>
<li><p>Supervised <em>vs.</em> unsupervised algorithms</p></li>
<li><p>With and without ensemble concept</p>
<ul>
<li><p>Random forest is an ensemble of decision trees and focuses on improving prediction accuracy and reducing overfitting.</p></li>
<li><p>Hierarchical clustering has no ensemble concept or predictive objective; it is purely descriptive.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>In short, <strong>Hierarchical Clustering is structurally similar to a tree (like a dendrogram)</strong>, but <strong>it is unsupervised and descriptive, unlike Decision Trees or Random Forests, which are supervised and predictive</strong>.</p>
</div>
<p>We first use SciPy and then Scikit-Learn packages for the clustering task, for the purpose of comparison.
In SciPy, hierarchical clustering involves two steps: <strong>computing the linkage matrix</strong> (<code class="docutils literal notranslate"><span class="pre">linkage()</span></code>), and then <strong>extracting clusters from it</strong> (<code class="docutils literal notranslate"><span class="pre">fcluster()</span></code>). In the code listed below,</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">linkage()</span></code> computes the full hierarchical clustering tree (linkage matrix), storing all merges, distances, and cluster sizes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fcluster()</span></code>, cuts the dendrogram at a specified threshold to produce a flat clustering, here forming 3 clusters (<code class="docutils literal notranslate"><span class="pre">t=3</span></code>, criterion=’maxclust’).</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.cluster.hierarchy</span><span class="w"> </span><span class="kn">import</span> <span class="n">dendrogram</span><span class="p">,</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">fcluster</span>

<span class="c1"># compute linkage matrix</span>
<span class="n">linked</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;ward&#39;</span><span class="p">)</span>

<span class="c1"># assign 3 clusters based on dendrogram cut</span>
<span class="n">labels_scipy</span> <span class="o">=</span> <span class="n">fcluster</span><span class="p">(</span><span class="n">linked</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;maxclust&#39;</span><span class="p">)</span>

<span class="n">penguins_cluster</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;hier_cluster_scipy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels_scipy</span>
</pre></div>
</div>
<p>Next we plot the dendrogram to visualize clustering structure.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7-hierarcical-dendrogram.png"><img alt="../_images/7-hierarcical-dendrogram.png" src="../_images/7-hierarcical-dendrogram.png" style="width: 100%;" />
</a>
</figure>
<p>Here, we move to the Scikit-learn package and employ <code class="docutils literal notranslate"><span class="pre">AgglomerativeClustering</span></code> to construct the clustering model with hyperparameters.</p>
<ul class="simple">
<li><p>The parameter <code class="docutils literal notranslate"><span class="pre">linkage</span></code> determines how the distance between clusters is calculated. There are several options for this parameters:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">ward</span></code>,  minimizes the variance of merged clusters (only works with Euclidean distance).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">complete</span></code>, maximum distance between points in clusters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">average</span></code>, average distance between points in clusters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">single</span></code>, minimum distance between points in clusters.</p></li>
</ul>
</li>
<li><p>The parameter <code class="docutils literal notranslate"><span class="pre">metric</span></code> is the distance metric used to compute the distance between points.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">euclidean</span></code> is the standard straight-line distance in feature space.</p></li>
<li><p>There are also other options like <code class="docutils literal notranslate"><span class="pre">manhattan</span></code>, <code class="docutils literal notranslate"><span class="pre">cosine</span></code>, <em>etc.</em></p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">AgglomerativeClustering</span>

<span class="n">hc</span> <span class="o">=</span> <span class="n">AgglomerativeClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">,</span> <span class="n">linkage</span><span class="o">=</span><span class="s1">&#39;ward&#39;</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">hc</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

<span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;hier_cluster_aggl&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
</pre></div>
</div>
<p>We can examine the number of penguins in each species within the clusters determined by the two models, and visualize their distributions using a confusion matrix.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7-hierarchical-clusters-from-scipy-scikit-learn.png"><img alt="../_images/7-hierarchical-clusters-from-scipy-scikit-learn.png" src="../_images/7-hierarchical-clusters-from-scipy-scikit-learn.png" style="width: 100%;" />
</a>
</figure>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7-hierarchical-confusion-matrix.png"><img alt="../_images/7-hierarchical-confusion-matrix.png" src="../_images/7-hierarchical-confusion-matrix.png" style="width: 100%;" />
</a>
</figure>
</section>
<section id="dbscan">
<h3>DBSCAN<a class="headerlink" href="#dbscan" title="Link to this heading"></a></h3>
<p><strong>DBSCAN</strong> (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together points that are closely packed while marking points in low-density regions as outliers. Unlike K-Means, DBSCAN does not require specifying the number of clusters in advance, making it particularly useful when the number of natural clusters is unknown. It is also capable of identifying clusters of arbitrary shapes, unlike centroid-based methods that assume roughly spherical clusters. This makes DBSCAN robust to clusters with irregular shapes or varying sizes.</p>
<p>We build a model using the <code class="docutils literal notranslate"><span class="pre">DBSCAN</span></code> class from <code class="docutils literal notranslate"><span class="pre">sklearn.cluster</span></code> with specified parameters. DBSCAN relies on two key parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">eps</span></code>, the radius that defines the neighborhood around a point.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">min_samples</span></code>, the minimum number of points required within a point’s eps neighborhood for it to be considered a core point.</p></li>
<li><p>Below we use <code class="docutils literal notranslate"><span class="pre">eps=0.55</span></code> and <code class="docutils literal notranslate"><span class="pre">min_samples=5</span></code> in the code.</p>
<ul>
<li><p>You can experiment with other <code class="docutils literal notranslate"><span class="pre">eps</span></code> values (<em>e.g.</em>, 0.50 and 0.80) while keeping <code class="docutils literal notranslate"><span class="pre">min_samples=5</span></code> to observe how the clustering results change.</p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">DBSCAN</span>

<span class="n">dbscan</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.55</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">dbscan</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

<span class="c1"># evaluate clustering (only if at least 2 clusters found)</span>
<span class="n">n_clusters</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">in</span> <span class="n">labels</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DBSCAN found </span><span class="si">{</span><span class="n">n_clusters</span><span class="si">}</span><span class="s2"> clusters (and </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">labels</span><span class="o">==-</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2"> noise points).&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">n_clusters</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">sil_score</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>  <span class="c1"># FIXED</span>
    <span class="n">ari_score</span> <span class="o">=</span> <span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">],</span> <span class="n">labels</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Silhouette Score: </span><span class="si">{</span><span class="n">sil_score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adjusted Rand Index (vs true species): </span><span class="si">{</span><span class="n">ari_score</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DBSCAN classifies samples into three types:</p>
<ul class="simple">
<li><p><strong>core points</strong>: points with at least <code class="docutils literal notranslate"><span class="pre">min_samples</span></code> neighbors within <code class="docutils literal notranslate"><span class="pre">eps</span></code>.</p></li>
<li><p><strong>border points</strong>: points within <code class="docutils literal notranslate"><span class="pre">eps</span></code> of a core point but with fewer than <code class="docutils literal notranslate"><span class="pre">min_samples</span></code> neighbors themselves.</p></li>
<li><p><strong>noise points</strong> (outliers): points that are neither core nor border points.</p></li>
</ul>
</div>
<p>Next, we visualize the distributions of penguins in each cluster, including any points identified as noise.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7-dbscan-point-types.png"><img alt="../_images/7-dbscan-point-types.png" src="../_images/7-dbscan-point-types.png" style="width: 80%;" />
</a>
</figure>
<p>We further examine the distribution of penguin species across clusters using the cross-tabulation (<code class="docutils literal notranslate"><span class="pre">.crosstab()</span></code>) method in Pandas.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cross_tab</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;dbscan_cluster&#39;</span><span class="p">],</span> <span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span>

<span class="n">cross_tab</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">stacked</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7-dbscan-penguins-in-clusters.png"><img alt="../_images/7-dbscan-penguins-in-clusters.png" src="../_images/7-dbscan-penguins-in-clusters.png" style="width: 80%;" />
</a>
</figure>
<div class="admonition-exercise exercise important admonition" id="exercise-0">
<p class="admonition-title">Exercise</p>
<p>In this exercise (code examples are availalbe in the <a class="reference internal" href="../jupyter-notebooks/7-ML-Clustering/"><span class="std std-doc">Jupyter Notebook</span></a>, we will</p>
<ul class="simple">
<li><p>Experiment with <code class="docutils literal notranslate"><span class="pre">eps</span></code> values (0.50 and 0.80) while keeping <code class="docutils literal notranslate"><span class="pre">min_samples=5</span></code> to observe how the clustering results change.</p></li>
<li><p>Computations with more combinations of <code class="docutils literal notranslate"><span class="pre">eps</span></code> and <code class="docutils literal notranslate"><span class="pre">min_samples</span></code>.</p></li>
<li><p>Explore methods to find optimal hyperparameters (using grid search and cross-validation).</p></li>
</ul>
</div>
</section>
<section id="gaussian-mixture-models">
<h3>Gaussian Mixture Models<a class="headerlink" href="#gaussian-mixture-models" title="Link to this heading"></a></h3>
<p>After exploring centroid-based methods like K-Means, hierarchical clustering models, and density-based approaches such as DBSCAN, we now turn our attention to model-based clustering algorithms.
Unlike the previous methods that rely primarily on distance metrics or density thresholds, model-based clustering assumes that the data come from a mixture of underlying probability distributions. Each distribution corresponds to a cluster, and the algorithm tries to estimate both the parameters of the distributions and the clusters.</p>
<p>Since model-based clustering assumes that a dataset is generated from a mixture of underlying probability distributions, a variety of algorithms have been developed to handle different types of distributions. The choice of model depends on the nature of the data.</p>
<ul class="simple">
<li><p>When dealing with continuous numerical features that approximately follow a bell-shaped distribution, <strong>Gaussian Mixture Models</strong> (GMMs) are the most common choice.</p></li>
<li><p>If the data consists of count values, mixture models based on Poisson distributions can be used.</p></li>
<li><p>For categorical data, methods like Latent Class Analysis (LCA), which treats clusters as latent categorical variables, are often applied.</p></li>
<li><p>In more flexible Bayesian frameworks, Dirichlet Process Mixture Models allow the number of clusters to be inferred directly from the data, avoiding the need to predefine it.</p></li>
</ul>
<p>The GMM assumes that data points are generated from a mixture of Gaussian distributions, each representing a cluster. Instead of assigning points strictly to one cluster (like K-Means), GMM assigns each point a probability of belonging to each cluster, making it a soft clustering method.</p>
<p>In the following example, we construct the GMM model with the specified hyperparameters.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_components=3</span></code> means the number of Gaussian distributions (clusters) to fit</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">covariance_type</span></code> controls the form of the covariance matrix for each Gaussian distribution</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">full</span></code> means that each cluster has its own general covariance matrix (most flexible, allows ellipsoidal shapes).</p></li>
<li><p>other options like <code class="docutils literal notranslate"><span class="pre">tied</span></code>, <code class="docutils literal notranslate"><span class="pre">diag</span></code>, and <code class="docutils literal notranslate"><span class="pre">spherical</span></code> corresponding to clusters with different shapes</p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.mixture</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianMixture</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">adjusted_rand_score</span><span class="p">,</span> <span class="n">silhouette_score</span>

<span class="c1"># build GMM model with 3 components (clusters)</span>
<span class="n">gmm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
<span class="n">labels_gmm</span> <span class="o">=</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</pre></div>
</div>
<p>Following the steps in the <a class="reference internal" href="../jupyter-notebooks/7-ML-Clustering/"><span class="std std-doc">Jupyter Notebook</span></a>, we can 1) examine the distribution of penguin species across clusters using the cross-tabulation method, 2) visualize the distribution of penguins within each cluster, and 3) illustrate their distributions with a confusion matrix.</p>
<p>Here, we specifically highlight the distributions penguins data points in clusters and the shapes clusters obtained from the KMeans and GMM models.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7-gmm-elliptical-clusters.png"><img alt="../_images/7-gmm-elliptical-clusters.png" src="../_images/7-gmm-elliptical-clusters.png" style="width: 100%;" />
</a>
</figure>
</section>
<section id="spectral-clustering">
<h3>Spectral Clustering<a class="headerlink" href="#spectral-clustering" title="Link to this heading"></a></h3>
<p>Following centroid-based, density-based, and model-based methods, we now turn our attention to <strong>Spectral Clustering</strong> algorithms.</p>
<p>Spectral Clustering represents a fundamentally different approach: rather than relying purely on distances between points or density, it leverages graph theory and the eigenstructure of similarity matrices to uncover clusters. That is, the main idea of this method is to represent the dataset as a graph where each node is a data point and edges encode the similarity between points (<em>e.g.</em>, using a Gaussian kernel). By computing the eigenvectors of the graph Laplacian, the algorithm transforms the original data into a lower-dimensional space where clusters become more distinguishable. Once in this space, standard clustering techniques, such as K-Means, are applied to assign cluster labels.</p>
<p>This method is especially powerful for datasets with complex, <strong>non-convex</strong> cluster shapes, where traditional algorithms like K-Means or Hierarchical Clustering may fail to capture the true underlying structure.</p>
<p>We adopted similar procedures to build the model, examine the distribution of penguin species across clusters using the cross-tabulation method, and visualize the distribution of penguins within each cluster.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">SpectralClustering</span>

<span class="c1"># build model using Spectral Clustering (graph-based)</span>
<span class="n">spectral</span> <span class="o">=</span> <span class="n">SpectralClustering</span><span class="p">(</span>
    <span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">affinity</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span>   <span class="c1"># Gaussian kernel</span>
    <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>        <span class="c1"># controls width of the Gaussian</span>
    <span class="n">assign_labels</span><span class="o">=</span><span class="s1">&#39;kmeans&#39;</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">spectral</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
<span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;spectral_cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>

<span class="c1"># evaluate clustering</span>
<span class="n">ari</span> <span class="o">=</span> <span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">penguins_cluster</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">],</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">sil</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adjusted Rand Index (vs species): </span><span class="si">{</span><span class="n">ari</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Silhouette Score: </span><span class="si">{</span><span class="n">sil</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7-spectral-confusion-matrix-kmeans-gmm.png"><img alt="../_images/7-spectral-confusion-matrix-kmeans-gmm.png" src="../_images/7-spectral-confusion-matrix-kmeans-gmm.png" style="width: 100%;" />
</a>
</figure>
<div class="dropdown callout admonition" id="callout-1">
<p class="admonition-title">Spectral Clustering <em>vs.</em> Gaussian Mixture Models <em>vs.</em> K-Means</p>
<p>From the confusion matrix shown above, it seems that Spectral Clustering and K-Means models are less effective than the GMM model on the Penguins dataset. Main reasons may attribute to:</p>
<ul class="simple">
<li><p>Small dataset size:</p>
<ul>
<li><p>The Penguins dataset has only 333 samples.</p></li>
<li><p>Spectral clustering computes the eigenvectors of the similarity matrix, which can be less stable with small datasets, leading to variability in cluster assignments.</p></li>
</ul>
</li>
<li><p>Small number of features/low dimensionality</p>
<ul>
<li><p>The Penguins dataset typically uses only 4 numerical features. In such low-dimensional, fairly well-separated data, simpler methods like K-Means or Gaussian Mixture Models often perform just as well or better.</p></li>
<li><p>Spectral clustering shines when clusters are non-convex or complexly shaped in high-dimensional spaces.</p></li>
</ul>
</li>
</ul>
</div>
<div class="admonition-exercise exercise important admonition" id="exercise-1">
<p class="admonition-title">Exercise</p>
<p>Here, we apply these two models to the classic <strong>two-moon dataset</strong>, a well-known synthetic dataset with non-linearly separable clusters. This allows us to visually and quantitatively evaluate how each algorithm performs in capturing complex, non-convex structures and to compare their strengths and limitations in a controlled setting.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7-spectral-kmeans-two-moon-dataset.png"><img alt="../_images/7-spectral-kmeans-two-moon-dataset.png" src="../_images/7-spectral-kmeans-two-moon-dataset.png" style="width: 80%;" />
</a>
</figure>
<p><strong>Spectral Clustering excels for datasets with complex, non-convex cluster shapes, where traditional algorithms like K-Meansmay fail to capture the true underlying structure</strong>.</p>
</div>
</section>
</section>
<section id="comparison-of-clustering-models">
<h2>Comparison of Clustering Models<a class="headerlink" href="#comparison-of-clustering-models" title="Link to this heading"></a></h2>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/7-comparison-sil-ari-scores.png"><img alt="../_images/7-comparison-sil-ari-scores.png" src="../_images/7-comparison-sil-ari-scores.png" style="width: 100%;" />
</a>
</figure>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Method</p></th>
<th class="head text-center"><p>Type/Approach</p></th>
<th class="head text-center"><p>Key Characteristics</p></th>
<th class="head text-center"><p>Pros</p></th>
<th class="head text-center"><p>Limitations</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>K-Means</p></td>
<td class="text-center"><p>Centroid-based</p></td>
<td class="text-center"><p>Partitions data into <br>k clusters by minimizing <br>within-cluster variance; <br>clusters represented by centroids</p></td>
<td class="text-center"><p>Simple, fast, widely used; <br>interpretable</p></td>
<td class="text-center"><p>Assumes spherical clusters; <br>sensitive to initialization and outliers; <br>requires specifying k</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Hierarchical Clustering <br>(SciPy)</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>Similar to Scikit-Learn, <br>uses linkage matrix <br>and fcluster to assign clusters</p></td>
<td class="text-center"><p>Flexible; supports different <br>distance metrics and linkage methods</p></td>
<td class="text-center"><p>Requires careful selection of <br>threshold to cut dendrogram; <br>can be slow for large data</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Hierarchical Clustering <br>(Scikit-Learn)</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>Builds a hierarchy of clusters <br>either bottom-up (agglomerative) <br>or top-down (divisive); <br>linkage criteria define <br>merge/split decisions</p></td>
<td class="text-center"><p>Dendrogram visualization; <br>no need to pre-specify number of clusters</p></td>
<td class="text-center"><p>Computationally expensive for large datasets; <br>choice of linkage affects results</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>DBSCAN</p></td>
<td class="text-center"><p>Density-based</p></td>
<td class="text-center"><p>Groups points <br>based on density; identifies core, border, <br>and noise points; no need to specify <br>number of clusters</p></td>
<td class="text-center"><p>Detects arbitrarily shaped clusters; <br>robust to outliers; <br>identifies noise</p></td>
<td class="text-center"><p>Sensitive to eps and min_samples; <br>struggles with varying densities</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>GMM</p></td>
<td class="text-center"><p>Model-based</p></td>
<td class="text-center"><p>Assumes data generated <br>from a mixture of Gaussian distributions; <br>each cluster has mean, covariance, and weight</p></td>
<td class="text-center"><p>Can model elliptical clusters; <br>provides probabilities for cluster membership</p></td>
<td class="text-center"><p>Sensitive to initialization; <br>may converge to local optima; <br>assumes Gaussian distribution</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Spectral Clustering</p></td>
<td class="text-center"><p>Graph-based</p></td>
<td class="text-center"><p>Uses graph Laplacian of similarity matrix; <br>clusters derived from eigenvectors; <br>can handle non-convex shapes</p></td>
<td class="text-center"><p>Captures complex structures; <br>good for connected or non-spherical clusters</p></td>
<td class="text-center"><p>Computationally expensive for large datasets; <br>sensitive to affinity and connectivity; <br>may fail on disconnected graphs</p></td>
</tr>
</tbody>
</table>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Clustering is about grouping data points that are similar to each other, without using labels.</p></li>
<li><p>Representative algorithms for clustering tasks: K-Means (centroid-based), Hierarchical, DBSCAN (density-based), Gaussian Mixture Models (model-based), and Spectral Clustering (graph-based).</p></li>
<li><p>Use tools like Silhouette score or visual plots to see how well clusters are separated.</p></li>
<li><p><strong>No single algorithm is “best”, and the right method depends on your data: size, type, shape, and what you want to achieve</strong>.</p></li>
</ul>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../06-supervised-ML-regression/" class="btn btn-neutral float-left" title="Supervised Learning (II): Regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../08-unsupervised-ML-dimensionality-reduction/" class="btn btn-neutral float-right" title="Unsupervised Learning (II): Dimensionality Reduction" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, The contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>